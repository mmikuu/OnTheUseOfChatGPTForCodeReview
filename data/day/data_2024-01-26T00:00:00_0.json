{
    "data": {
        "search": {
            "edges": [
                {
                    "node": {
                        "number": 400,
                        "title": "Upgrade to run against OpenAI >= 1.0",
                        "repository": {
                            "nameWithOwner": "simonw/llm",
                            "primaryLanguage": {
                                "name": "Python"
                            }
                        },
                        "createdAt": "2024-01-26T02:18:59Z",
                        "mergedAt": "2024-01-26T06:00:44Z",
                        "url": "https://github.com/simonw/llm/pull/400",
                        "state": "MERGED",
                        "author": {
                            "login": "simonw"
                        },
                        "editor": {
                            "login": "simonw"
                        },
                        "body": "Refs:\r\n- #325\r\n\r\nOriginal goal was \"Upgrade to run against both OpenAI < 1.0 and OpenAI >= 1.0\" but I eventually determined that the changes between the two were just too extensive for it to be worth trying to build a compatibility shim.",
                        "comments": {
                            "nodes": [
                                {
                                    "createdAt": "2024-01-26T02:24:42Z",
                                    "bodyText": "Looking at this diff generated by openai upgrade:\ndiff --git a/llm/default_plugins/openai_models.py b/llm/default_plugins/openai_models.py\nindex 611340d..d787c7a 100644\n--- a/llm/default_plugins/openai_models.py\n+++ b/llm/default_plugins/openai_models.py\n@@ -4,6 +4,9 @@ from llm.utils import dicts_to_table_string\n import click\n import datetime\n import openai\n+from openai import OpenAI\n+\n+client = OpenAI()\n import os\n \n try:\n@@ -22,7 +25,7 @@ if os.environ.get(\"LLM_OPENAI_SHOW_RESPONSES\"):\n         click.echo(response.text, err=True)\n         return response\n \n-    openai.requestssession = requests.Session()\n+    raise Exception(\"The 'openai.requestssession' option isn't read in the client API. You will need to pass it when you instantiate the client, e.g. 'OpenAI(requestssession=requests.Session())'\")\n     openai.requestssession.hooks[\"response\"].append(log_response)\n \n \n@@ -111,7 +114,7 @@ class OpenAIEmbeddingModel(EmbeddingModel):\n         }\n         if self.dimensions:\n             kwargs[\"dimensions\"] = self.dimensions\n-        results = openai.Embedding.create(**kwargs)[\"data\"]\n+        results = client.embeddings.create(**kwargs)[\"data\"]\n         return ([float(r) for r in result[\"embedding\"]] for result in results)\n \n \n@@ -305,12 +308,10 @@ class Chat(Model):\n         response._prompt_json = {\"messages\": messages}\n         kwargs = self.build_kwargs(prompt)\n         if stream:\n-            completion = openai.ChatCompletion.create(\n-                model=self.model_name or self.model_id,\n-                messages=messages,\n-                stream=True,\n-                **kwargs,\n-            )\n+            completion = client.chat.completions.create(model=self.model_name or self.model_id,\n+            messages=messages,\n+            stream=True,\n+            **kwargs)\n             chunks = []\n             for chunk in completion:\n                 chunks.append(chunk)\n@@ -319,12 +320,10 @@ class Chat(Model):\n                     yield content\n             response.response_json = combine_chunks(chunks)\n         else:\n-            completion = openai.ChatCompletion.create(\n-                model=self.model_name or self.model_id,\n-                messages=messages,\n-                stream=False,\n-                **kwargs,\n-            )\n+            completion = client.chat.completions.create(model=self.model_name or self.model_id,\n+            messages=messages,\n+            stream=False,\n+            **kwargs)\n             response.response_json = completion.to_dict_recursive()\n             yield completion.choices[0].message.content\n \n@@ -384,12 +383,10 @@ class Completion(Chat):\n         response._prompt_json = {\"messages\": messages}\n         kwargs = self.build_kwargs(prompt)\n         if stream:\n-            completion = openai.Completion.create(\n-                model=self.model_name or self.model_id,\n-                prompt=\"\\n\".join(messages),\n-                stream=True,\n-                **kwargs,\n-            )\n+            completion = client.completions.create(model=self.model_name or self.model_id,\n+            prompt=\"\\n\".join(messages),\n+            stream=True,\n+            **kwargs)\n             chunks = []\n             for chunk in completion:\n                 chunks.append(chunk)\n@@ -398,12 +395,10 @@ class Completion(Chat):\n                     yield content\n             response.response_json = combine_chunks(chunks)\n         else:\n-            completion = openai.Completion.create(\n-                model=self.model_name or self.model_id,\n-                prompt=\"\\n\".join(messages),\n-                stream=False,\n-                **kwargs,\n-            )\n+            completion = client.completions.create(model=self.model_name or self.model_id,\n+            prompt=\"\\n\".join(messages),\n+            stream=False,\n+            **kwargs)\n             response.response_json = completion.to_dict_recursive()\n             yield completion.choices[0][\"text\"]\nIt basically all comes down to these differences:\nopenai.Embedding.create(...)\nclient.embeddings.create(...)\n\nopenai.ChatCompletion.create(...)\nclient.chat.completions.create(...)\n\nopenai.Completion.create(...)\nclient.completions.create(...)\nSo... I could add a get_openai() helper function of some sort that papers over them.",
                                    "url": "https://github.com/simonw/llm/pull/400#issuecomment-1911309400",
                                    "author": {
                                        "login": "simonw"
                                    }
                                },
                                {
                                    "createdAt": "2024-01-26T02:26:25Z",
                                    "bodyText": "I'm going to do this:\nclient = get_openai_client(...)\nclient.ChatCompletion.create(...)\nclient.Completion.create(...)\nclient.Embedding.create(...)\nWhere my special client class knows to proxy to chat.completions.create(...) when necessary.",
                                    "url": "https://github.com/simonw/llm/pull/400#issuecomment-1911310827",
                                    "author": {
                                        "login": "simonw"
                                    }
                                },
                                {
                                    "createdAt": "2024-01-26T02:27:27Z",
                                    "bodyText": "Tests are now failing as intended - the 1.0 ones fail, the <1.0 ones pass:",
                                    "url": "https://github.com/simonw/llm/pull/400#issuecomment-1911311615",
                                    "author": {
                                        "login": "simonw"
                                    }
                                },
                                {
                                    "createdAt": "2024-01-26T02:30:10Z",
                                    "bodyText": "On the latest version:\n>>> openai.version.VERSION\n'1.10.0'\n>>> openai.version.VERSION.split('.')[0]\n'1'\nAnd on the pre-1.0 version:\n>>> import openai\n>>> openai.version.VERSION\n'0.28.1'\n>>> openai.version.VERSION.split('.')[0]\n'0'\nI can use that to detect the old version.",
                                    "url": "https://github.com/simonw/llm/pull/400#issuecomment-1911313435",
                                    "author": {
                                        "login": "simonw"
                                    }
                                },
                                {
                                    "createdAt": "2024-01-26T02:43:01Z",
                                    "bodyText": "Uh-oh, I need to convince mypy not to get upset:",
                                    "url": "https://github.com/simonw/llm/pull/400#issuecomment-1911321694",
                                    "author": {
                                        "login": "simonw"
                                    }
                                },
                                {
                                    "createdAt": "2024-01-26T02:48:42Z",
                                    "bodyText": "llm 'hello world'\n\nError: Completions.create() got an unexpected keyword argument 'api_key'\n\nI'm going to have to get my get_openai_client() method to take that API key.",
                                    "url": "https://github.com/simonw/llm/pull/400#issuecomment-1911325036",
                                    "author": {
                                        "login": "simonw"
                                    }
                                },
                                {
                                    "createdAt": "2024-01-26T02:51:46Z",
                                    "bodyText": "Oh this is annoying. I've been passing api_base and so on to the create() method:\n\n  \n    \n      llm/llm/default_plugins/openai_models.py\n    \n    \n        Lines 331 to 355\n      in\n      469644c\n    \n  \n  \n    \n\n        \n          \n           def build_kwargs(self, prompt): \n        \n\n        \n          \n               kwargs = dict(not_nulls(prompt.options)) \n        \n\n        \n          \n               json_object = kwargs.pop(\"json_object\", None) \n        \n\n        \n          \n               if \"max_tokens\" not in kwargs and self.default_max_tokens is not None: \n        \n\n        \n          \n                   kwargs[\"max_tokens\"] = self.default_max_tokens \n        \n\n        \n          \n               if self.api_base: \n        \n\n        \n          \n                   kwargs[\"api_base\"] = self.api_base \n        \n\n        \n          \n               if self.api_type: \n        \n\n        \n          \n                   kwargs[\"api_type\"] = self.api_type \n        \n\n        \n          \n               if self.api_version: \n        \n\n        \n          \n                   kwargs[\"api_version\"] = self.api_version \n        \n\n        \n          \n               if self.api_engine: \n        \n\n        \n          \n                   kwargs[\"engine\"] = self.api_engine \n        \n\n        \n          \n               if json_object: \n        \n\n        \n          \n                   kwargs[\"response_format\"] = {\"type\": \"json_object\"} \n        \n\n        \n          \n               if self.needs_key: \n        \n\n        \n          \n                   if self.key: \n        \n\n        \n          \n                       kwargs[\"api_key\"] = self.key \n        \n\n        \n          \n               else: \n        \n\n        \n          \n                   # OpenAI-compatible models don't need a key, but the \n        \n\n        \n          \n                   # openai client library requires one \n        \n\n        \n          \n                   kwargs[\"api_key\"] = \"DUMMY_KEY\" \n        \n\n        \n          \n               if self.headers: \n        \n\n        \n          \n                   kwargs[\"headers\"] = self.headers \n        \n\n        \n          \n               return kwargs \n        \n    \n  \n\n\nBut it looks like I need to pass those to that new constructor instead:\nclient=OpenAI(\n    api_key=\"<key>\",\n    base_url=\"<base_url>\",\n    http_client=httpx.Client(\n        headers={\n            \"header\": \"<key>\",\n\t\t\t...\n        } \n    )\n)\nThis is going to REALLY mess up my compatibility shim.",
                                    "url": "https://github.com/simonw/llm/pull/400#issuecomment-1911326830",
                                    "author": {
                                        "login": "simonw"
                                    }
                                },
                                {
                                    "createdAt": "2024-01-26T02:53:55Z",
                                    "bodyText": "I think I have two sensible options:\n\nGive up on compatibility with pre 1.0 - which is actually pretty reasonable, the chances that someone will have LLM installed in an environment alongside another library which itself depends on OpenAI <1.0 is pretty small\nRewrite my core code to call the new OpenAI() constructor and use client.chat.completions.create(...) and friends, then build my own compatibility shim for that instead\n\nI like option 2 a lot - it's much nicer for me to write code against the new API and have a compatibility shim, since I can then easily drop the shim later on if I change my mind.\nI'm going to spike on 2 for a bit and, if I can't get that working, switch to option 1 and drop <1.0 entirely.",
                                    "url": "https://github.com/simonw/llm/pull/400#issuecomment-1911328053",
                                    "author": {
                                        "login": "simonw"
                                    }
                                },
                                {
                                    "createdAt": "2024-01-26T03:00:45Z",
                                    "bodyText": "Urgh:\nllm 'hello world'                     \nError: 'ChatCompletionChunk' object is not subscriptable\n\nFrom the stack trace:\n  File \"/Users/simon/Dropbox/Development/llm/llm/cli.py\", line 277, in prompt\n    for chunk in response:\n  File \"/Users/simon/Dropbox/Development/llm/llm/models.py\", line 91, in __iter__\n    for chunk in self.model.execute(\n  File \"/Users/simon/Dropbox/Development/llm/llm/default_plugins/openai_models.py\", line 342, in execute\n    content = chunk[\"choices\"][0].get(\"delta\", {}).get(\"content\")\nTypeError: 'ChatCompletionChunk' object is not subscriptable",
                                    "url": "https://github.com/simonw/llm/pull/400#issuecomment-1911332280",
                                    "author": {
                                        "login": "simonw"
                                    }
                                },
                                {
                                    "createdAt": "2024-01-26T03:02:10Z",
                                    "bodyText": "Debugger exploration:\n-> content = chunk[\"choices\"][0].get(\"delta\", {}).get(\"content\")\n(Pdb) chunk\nChatCompletionChunk(id='chatcmpl-8l6l41lMUl3DiI3n5ujjGUF5nlOny', choices=[Choice(delta=ChoiceDelta(content='', function_call=None, role='assistant', tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1706238086, model='gpt-4-1106-preview', object='chat.completion.chunk', system_fingerprint='fp_f71eafccde')\n(Pdb) chunk[\"choices\"]\n*** TypeError: 'ChatCompletionChunk' object is not subscriptable\n(Pdb) chunk.choices\n[Choice(delta=ChoiceDelta(content='', function_call=None, role='assistant', tool_calls=None), finish_reason=None, index=0, logprobs=None)]\n(Pdb) chunk.choices[0]\nChoice(delta=ChoiceDelta(content='', function_call=None, role='assistant', tool_calls=None), finish_reason=None, index=0, logprobs=None)\n(Pdb) chunk.choices[0].get(\"delta\")\n*** AttributeError: 'Choice' object has no attribute 'get'\n(Pdb) chunk.choices[0].delta\nChoiceDelta(content='', function_call=None, role='assistant', tool_calls=None)\n(Pdb) chunk.choices[0].delta.content\n''",
                                    "url": "https://github.com/simonw/llm/pull/400#issuecomment-1911333180",
                                    "author": {
                                        "login": "simonw"
                                    }
                                },
                                {
                                    "createdAt": "2024-01-26T03:02:39Z",
                                    "bodyText": "I'm going to give up on the <1.0 dream at this point, the changes are just too deep for it to be worth trying to paper over them with a compatibility shim.",
                                    "url": "https://github.com/simonw/llm/pull/400#issuecomment-1911333464",
                                    "author": {
                                        "login": "simonw"
                                    }
                                },
                                {
                                    "createdAt": "2024-01-26T03:09:38Z",
                                    "bodyText": "Well here's a painful thing...\n(Pdb) completion.__class__\n<class 'openai.types.chat.chat_completion.ChatCompletion'>\n(Pdb) completion.__class__.__bases__\n(<class 'openai._models.BaseModel'>,)\n(Pdb) completion.__class__.__bases__[0]\n<class 'openai._models.BaseModel'>\n(Pdb) completion.__class__.__bases__[0].__bases__\n(<class 'pydantic.main.BaseModel'>,)\n\nIt turns out the new OpenAI library uses Pydantic. I've had huge pain keeping LLM compatible with Pydantic 1 and Pydantic 2 already!\nBut... https://github.com/openai/openai-python/blob/0c1e58d511bd60c4dd47ea8a8c0820dc2d013d1d/pyproject.toml#L12 says\ndependencies = [\n    \"httpx>=0.23.0, <1\",\n    \"pydantic>=1.9.0, <3\",\nSo maybe OpenAI is compatible with both Pydantic versions itself?\nYes, it turns out there are! Here's their own Pydantic 1 v. 2 compatibility shim: https://github.com/openai/openai-python/blob/0c1e58d511bd60c4dd47ea8a8c0820dc2d013d1d/src/openai/_compat.py#L20",
                                    "url": "https://github.com/simonw/llm/pull/400#issuecomment-1911337583",
                                    "author": {
                                        "login": "simonw"
                                    }
                                },
                                {
                                    "createdAt": "2024-01-26T03:12:07Z",
                                    "bodyText": "From openai/openai-python@v0.28.1...v1.0.0 I confirmed that pydantic was indeed one of the new things they added in 1.0.",
                                    "url": "https://github.com/simonw/llm/pull/400#issuecomment-1911339034",
                                    "author": {
                                        "login": "simonw"
                                    }
                                },
                                {
                                    "createdAt": "2024-01-26T03:28:08Z",
                                    "bodyText": "Now my tests are failing and I'm worried it might be because my mocks don't work any more:\n        result = runner.invoke(cli, [\"hello\", \"--no-stream\"], catch_exceptions=False)\n>       assert result.exit_code == 0\nE       assert 1 == 0\nE        +  where 1 = <Result SystemExit(1)>.exit_code\n\ntests/test_keys.py:68: AssertionError\n>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> entering PDB >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n\n>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> PDB post_mortem (IO-capturing turned off) >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n> /Users/simon/Dropbox/Development/llm/tests/test_keys.py(68)test_uses_correct_key()\n-> assert result.exit_code == 0\n(Pdb) result\n<Result SystemExit(1)>\n(Pdb) result.stdout\n\"Error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: from-key**file. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\\n\"\n\nDid 1.0 change the HTTP library they use? Yes - it looks like they switched from requests to httpx.",
                                    "url": "https://github.com/simonw/llm/pull/400#issuecomment-1911389226",
                                    "author": {
                                        "login": "simonw"
                                    }
                                },
                                {
                                    "createdAt": "2024-01-26T03:32:13Z",
                                    "bodyText": "openai/openai-python#742 confirms that openai.requestssession has been removed. So this code is obsolete:\n\n  \n    \n      llm/llm/default_plugins/openai_models.py\n    \n    \n        Lines 20 to 26\n      in\n      53c845e\n    \n  \n  \n    \n\n        \n          \n           def _log_response(response, *args, **kwargs): \n        \n\n        \n          \n               click.echo(response.text, err=True) \n        \n\n        \n          \n               return response \n        \n\n        \n          \n            \n        \n\n        \n          \n            \n        \n\n        \n          \n           _log_session = requests.Session() \n        \n\n        \n          \n           _log_session.hooks[\"response\"].append(_log_response) \n        \n    \n  \n\n\nLLM_OPENAI_SHOW_RESPONSES is a documented feature. If I remove it I need to update the documentation.",
                                    "url": "https://github.com/simonw/llm/pull/400#issuecomment-1911400134",
                                    "author": {
                                        "login": "simonw"
                                    }
                                },
                                {
                                    "createdAt": "2024-01-26T03:33:53Z",
                                    "bodyText": "Spotted in https://github.com/openai/openai-python/blob/0c1e58d511bd60c4dd47ea8a8c0820dc2d013d1d/src/openai/_utils/_logs.py#L5\ndef setup_logging() -> None:\n    env = os.environ.get(\"OPENAI_LOG\")\n    if env == \"debug\":\n        _basic_config()\n        logger.setLevel(logging.DEBUG)\n        httpx_logger.setLevel(logging.DEBUG)\n    elif env == \"info\":\n        _basic_config()\n        logger.setLevel(logging.INFO)\n        httpx_logger.setLevel(logging.INFO)\n\nSo maybe I can ditch LLM_OPENAI_SHOW_RESPONSES in favour of OPENAI_LOG instead?",
                                    "url": "https://github.com/simonw/llm/pull/400#issuecomment-1911404743",
                                    "author": {
                                        "login": "simonw"
                                    }
                                },
                                {
                                    "createdAt": "2024-01-26T03:35:30Z",
                                    "bodyText": "Here's what that does:\n$ OPENAI_LOG=debug llm 'hello'\n[2024-01-25 19:34:46 - httpx:79 - DEBUG] load_ssl_context verify=True cert=None trust_env=True http2=False\n[2024-01-25 19:34:46 - httpx:146 - DEBUG] load_verify_locations cafile='/Users/simon/.local/share/virtualenvs/llm-p4p8CDpq/lib/python3.10/site-packages/certifi/cacert.pem'\n[2024-01-25 19:34:46 - openai._base_client:439 - DEBUG] Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': 'hello'}], 'model': 'gpt-4-1106-preview', 'stream': True}}\n[2024-01-25 19:34:46 - httpx:1013 - INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n[2024-01-25 19:34:46 - openai._base_client:934 - DEBUG] HTTP Request: POST https://api.openai.com/v1/chat/completions \"200 OK\"\nHello! How can I assist you today?\n$ OPENAI_LOG=info llm 'hello'\n[2024-01-25 19:34:56 - httpx:1013 - INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\nHello! How can I assist you today?\n\nThat's good enough, I'll make that change.",
                                    "url": "https://github.com/simonw/llm/pull/400#issuecomment-1911407883",
                                    "author": {
                                        "login": "simonw"
                                    }
                                },
                                {
                                    "createdAt": "2024-01-26T03:36:56Z",
                                    "bodyText": "It's not as good - you don't get the full response, which is the thing that was most useful. But maybe I'll add that back in again in the future.",
                                    "url": "https://github.com/simonw/llm/pull/400#issuecomment-1911412678",
                                    "author": {
                                        "login": "simonw"
                                    }
                                },
                                {
                                    "createdAt": "2024-01-26T03:57:32Z",
                                    "bodyText": "Got this test failure:\n>       assert json.loads(row[\"response_json\"]) == {\n            \"model\": \"gpt-3.5-turbo\",\n            \"usage\": {},\n            \"choices\": [{\"message\": {\"content\": \"Bob, Alice, Eve\"}}],\n        }\n\nThe debugger shows:\n(Pdb) pprint(json.loads(row[\"response_json\"]))\n{'choices': [{'finish_reason': None,\n              'index': None,\n              'logprobs': None,\n              'message': {'content': 'Bob, Alice, Eve',\n                          'function_call': None,\n                          'role': None,\n                          'tool_calls': None}}],\n 'created': None,\n 'id': None,\n 'model': 'gpt-3.5-turbo',\n 'object': None,\n 'system_fingerprint': None,\n 'usage': {'completion_tokens': None,\n           'prompt_tokens': None,\n           'total_tokens': None}}",
                                    "url": "https://github.com/simonw/llm/pull/400#issuecomment-1911442101",
                                    "author": {
                                        "login": "simonw"
                                    }
                                },
                                {
                                    "createdAt": "2024-01-26T03:59:07Z",
                                    "bodyText": "The problem here is that I used to store the exact JSON that came back from the API - but the new Pydantic layer in OpenAI 1.0 reshapes that original information into something a lot more verbose, with a ton of None values.",
                                    "url": "https://github.com/simonw/llm/pull/400#issuecomment-1911443231",
                                    "author": {
                                        "login": "simonw"
                                    }
                                },
                                {
                                    "createdAt": "2024-01-26T04:02:05Z",
                                    "bodyText": "Pushing what I have so far.",
                                    "url": "https://github.com/simonw/llm/pull/400#issuecomment-1911445282",
                                    "author": {
                                        "login": "simonw"
                                    }
                                },
                                {
                                    "createdAt": "2024-01-26T05:21:07Z",
                                    "bodyText": "I'm going to clean that up with this:\ndef remove_dict_none_values(d: dict) -> dict:\n    \"\"\"\n    Recursively remove keys with value of None or value of a dict that is all values of None\n    \"\"\"\n    if not isinstance(d, dict):\n        return d\n    new_dict = {}\n    for key, value in d.items():\n        if value is not None:\n            if isinstance(value, dict):\n                nested = remove_dict_none_values(value)\n                if nested:\n                    new_dict[key] = nested\n            elif isinstance(value, list):\n                new_dict[key] = [remove_dict_none_values(v) for v in value]\n            else:\n                new_dict[key] = value\n    return new_dict\nWritten with help of: https://chat.openai.com/share/0dbf00c3-3feb-423c-98aa-7a4cca89023c",
                                    "url": "https://github.com/simonw/llm/pull/400#issuecomment-1911501277",
                                    "author": {
                                        "login": "simonw"
                                    }
                                },
                                {
                                    "createdAt": "2024-01-26T05:46:26Z",
                                    "bodyText": "One last error:\n(Pdb) result.output\n\"Error: OpenAI.__init__() got an unexpected keyword argument 'api_base'\\n\"\n\nFrom this test: \n  \n    \n      llm/tests/test_llm.py\n    \n    \n        Lines 460 to 496\n      in\n      469644c\n    \n  \n  \n    \n\n        \n          \n           EXTRA_MODELS_YAML = \"\"\" \n        \n\n        \n          \n           - model_id: orca \n        \n\n        \n          \n             model_name: orca-mini-3b \n        \n\n        \n          \n             api_base: \"http://localai.localhost\" \n        \n\n        \n          \n           - model_id: completion-babbage \n        \n\n        \n          \n             model_name: babbage \n        \n\n        \n          \n             api_base: \"http://localai.localhost\" \n        \n\n        \n          \n             completion: 1 \n        \n\n        \n          \n           \"\"\" \n        \n\n        \n          \n            \n        \n\n        \n          \n            \n        \n\n        \n          \n           def test_openai_localai_configuration(mocked_localai, user_path): \n        \n\n        \n          \n               log_path = user_path / \"logs.db\" \n        \n\n        \n          \n               sqlite_utils.Database(str(log_path)) \n        \n\n        \n          \n               # Write the configuration file \n        \n\n        \n          \n               config_path = user_path / \"extra-openai-models.yaml\" \n        \n\n        \n          \n               config_path.write_text(EXTRA_MODELS_YAML, \"utf-8\") \n        \n\n        \n          \n               # Run the prompt \n        \n\n        \n          \n               runner = CliRunner() \n        \n\n        \n          \n               prompt = \"three names \\nfor a pet pelican\" \n        \n\n        \n          \n               result = runner.invoke(cli, [\"--no-stream\", \"--model\", \"orca\", prompt]) \n        \n\n        \n          \n               assert result.exit_code == 0 \n        \n\n        \n          \n               assert result.output == \"Bob, Alice, Eve\\n\" \n        \n\n        \n          \n               assert json.loads(mocked_localai.last_request.text) == { \n        \n\n        \n          \n                   \"model\": \"orca-mini-3b\", \n        \n\n        \n          \n                   \"messages\": [{\"role\": \"user\", \"content\": \"three names \\nfor a pet pelican\"}], \n        \n\n        \n          \n                   \"stream\": False, \n        \n\n        \n          \n               } \n        \n\n        \n          \n               # And check the completion model too \n        \n\n        \n          \n               result2 = runner.invoke(cli, [\"--no-stream\", \"--model\", \"completion-babbage\", \"hi\"]) \n        \n\n        \n          \n               assert result2.exit_code == 0 \n        \n\n        \n          \n               assert result2.output == \"Hello\\n\" \n        \n\n        \n          \n               assert json.loads(mocked_localai.last_request.text) == { \n        \n\n        \n          \n                   \"model\": \"babbage\", \n        \n\n        \n          \n                   \"prompt\": \"hi\", \n        \n\n        \n          \n                   \"stream\": False, \n        \n\n        \n          \n               } \n        \n    \n  \n\n\nopenai/openai-python#742 says:\n\nopenai.api_base -> openai.base_url",
                                    "url": "https://github.com/simonw/llm/pull/400#issuecomment-1911520581",
                                    "author": {
                                        "login": "simonw"
                                    }
                                },
                                {
                                    "createdAt": "2024-01-26T05:50:10Z",
                                    "bodyText": "Well they passed on my laptop! Failing here because I forgot to run linters.",
                                    "url": "https://github.com/simonw/llm/pull/400#issuecomment-1911522870",
                                    "author": {
                                        "login": "simonw"
                                    }
                                },
                                {
                                    "createdAt": "2024-01-26T05:56:14Z",
                                    "bodyText": "I used ChatGPT to help port the requests mock tests to pytest: https://chat.openai.com/share/1ea97304-1ceb-4e4c-9213-bae9949cd261",
                                    "url": "https://github.com/simonw/llm/pull/400#issuecomment-1911526463",
                                    "author": {
                                        "login": "simonw"
                                    }
                                },
                                {
                                    "createdAt": "2024-01-26T05:57:02Z",
                                    "bodyText": "Turns out Black was failing because I needed to upgrade Black in my dev environment (as of only 45 minutes ago: https://github.com/psf/black/releases/tag/24.1.0)\nInstalling collected packages: black\n  Attempting uninstall: black\n    Found existing installation: black 23.7.0\n    Uninstalling black-23.7.0:\n      Successfully uninstalled black-23.7.0\nSuccessfully installed black-24.1.0\n\ndiff --git a/llm/cli.py b/llm/cli.py\nindex 3fa2ecc..03a6b09 100644\n--- a/llm/cli.py\n+++ b/llm/cli.py\n@@ -746,12 +746,16 @@ def logs_list(\n             click.echo(\n                 \"# {}{}\\n{}\".format(\n                     row[\"datetime_utc\"].split(\".\")[0],\n-                    \"    conversation: {}\".format(row[\"conversation_id\"])\n-                    if should_show_conversation\n-                    else \"\",\n-                    \"\\nModel: **{}**\\n\".format(row[\"model\"])\n-                    if should_show_conversation\n-                    else \"\",\n+                    (\n+                        \"    conversation: {}\".format(row[\"conversation_id\"])\n+                        if should_show_conversation\n+                        else \"\"\n+                    ),\n+                    (\n+                        \"\\nModel: **{}**\\n\".format(row[\"model\"])\n+                        if should_show_conversation\n+                        else \"\"\n+                    ),\n                 )\n             )\n             # In conversation log mode only show it for the first one\ndiff --git a/llm/embeddings.py b/llm/embeddings.py\nindex 8c5333f..5efeda0 100644\n--- a/llm/embeddings.py\n+++ b/llm/embeddings.py\n@@ -220,12 +220,12 @@ class Collection:\n                             \"collection_id\": collection_id,\n                             \"id\": id,\n                             \"embedding\": llm.encode(embedding),\n-                            \"content\": value\n-                            if (store and isinstance(value, str))\n-                            else None,\n-                            \"content_blob\": value\n-                            if (store and isinstance(value, bytes))\n-                            else None,\n+                            \"content\": (\n+                                value if (store and isinstance(value, str)) else None\n+                            ),\n+                            \"content_blob\": (\n+                                value if (store and isinstance(value, bytes)) else None\n+                            ),\n                             \"content_hash\": self.content_hash(value),\n                             \"metadata\": json.dumps(metadata) if metadata else None,\n                             \"updated\": int(time.time()),\nThat's a nice cosmetic improvement.",
                                    "url": "https://github.com/simonw/llm/pull/400#issuecomment-1911526897",
                                    "author": {
                                        "login": "simonw"
                                    }
                                },
                                {
                                    "createdAt": "2024-01-28T22:32:41Z",
                                    "bodyText": "Heya @simonw, I'm one of the maintainers of the openai package. Another maintainer, @RobertCraigie, sent me this thread after reading through it. Tremendous thanks for the detailed stream-of-consciousness! Very useful.\nFew follow-ups:\n\n[OPENAI_LOG is] not as good - you don't get the full response, which is the thing that was most useful. But maybe I'll add that back in again in the future.\n\nThis is good feedback, we may add that in the future. In the meantime, you can use httpx's event_hooks to achieve this if you like.\n\nthe new Pydantic layer in OpenAI 1.0 reshapes that original information into something a lot more verbose, with a ton of None values\n\nThis is good feedback \u2013\u00a0we've been torn between sticking with Pydantic defaults (what you see here, lots of Nones) and something more minimally JSONic (what you expected). Do you have any advice or suggestions here?\nNote that I believe that your remove_dict_none_values could be replaced with foo.model_dump_json(indent=2, exclude_unset=True), as documented here. We're considering adding a .to_json() helper method which would do this for you more conveniently.\nEDIT: I should ask, do you have any other feedback, whether overall / high-level, or nitty-gritty, about how the python SDK could be better or how the experience porting from the v0 could be better?",
                                    "url": "https://github.com/simonw/llm/pull/400#issuecomment-1913743486",
                                    "author": {
                                        "login": "rattrayalex"
                                    }
                                }
                            ]
                        },
                        "reviews": {
                            "edges": []
                        }
                    },
                    "textMatches": [
                        {
                            "property": "comments.body"
                        }
                    ]
                }
            ],
            "pageInfo": {
                "endCursor": "Y3Vyc29yOjE=",
                "hasNextPage": false,
                "hasPreviousPage": false,
                "startCursor": "Y3Vyc29yOjE="
            },
            "issueCount": 1
        }
    }
}