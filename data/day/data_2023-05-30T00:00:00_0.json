{
    "data": {
        "search": {
            "edges": [
                {
                    "node": {
                        "number": 199,
                        "title": "Wrap ActiveRecord::Base with ActiveSupport.on_load",
                        "repository": {
                            "nameWithOwner": "citusdata/activerecord-multi-tenant",
                            "primaryLanguage": {
                                "name": "Ruby"
                            }
                        },
                        "createdAt": "2023-05-30T12:48:25Z",
                        "mergedAt": "2023-06-05T05:34:35Z",
                        "url": "https://github.com/citusdata/activerecord-multi-tenant/pull/199",
                        "state": "MERGED",
                        "author": {
                            "login": "nipe0324"
                        },
                        "editor": {
                            "login": "nipe0324"
                        },
                        "body": "This PR try to fix #198\r\n\r\nCause\r\n---\r\n\r\n### Before: Rails initialization order is NOT good.\r\n\r\nFirst. Set `ActiveRecord::Base.filter_attributes` which is empty array.\r\n\r\n```rb\r\n# https://github.com/rails/rails/blob/v7.0.5/activerecord/lib/active_record/railtie.rb#L318-L322\r\ninitializer \"active_record.set_filter_attributes\" do\r\n  ActiveSupport.on_load(:active_record) do\r\n    self.filter_attributes += Rails.application.config.filter_parameters\r\n  end\r\nend\r\n```\r\n\r\nSecond. Initialize `Rails.application.config.filter_parameters`\r\n\r\n```rb\r\n# My Rails Application\r\n# config/initializers/filter_parameter_logging.rb\r\nRails.application.config.filter_parameters += [\r\n  :passw, :secret, :token, :_key, :crypt, :salt, :certificate, :otp, :ssn\r\n]\r\n````\r\n### After: Rails initialization order is good.\r\n\r\nFirst. Initialize `Rails.application.config.filter_parameters`\r\n\r\n```rb\r\n# My Rails Application\r\n# config/initializers/filter_parameter_logging.rb\r\nRails.application.config.filter_parameters += [\r\n  :passw, :secret, :token, :_key, :crypt, :salt, :certificate, :otp, :ssn\r\n]\r\n````\r\n\r\nSecond. Set `ActiveRecord::Base.filter_attributes` which is NOT empty array.\r\n\r\n```rb\r\n# https://github.com/rails/rails/blob/v7.0.5/activerecord/lib/active_record/railtie.rb#L318-L322\r\ninitializer \"active_record.set_filter_attributes\" do\r\n  ActiveSupport.on_load(:active_record) do\r\n    self.filter_attributes += Rails.application.config.filter_parameters\r\n  end\r\nend\r\n```\r\n\r\nTest\r\n---\r\n\r\nUse fixed gem\r\n\r\n```rb\r\n# Gemfile\r\ngem 'activerecord-multi-tenant', github: 'nipe0324/activerecord-multi-tenant', branch: 'wrap_active_support_on_load'\r\n```\r\n\r\nRun `bundle install`. And, `bin/rails c` and `filter_parameters` works!!\r\n\r\n```rb\r\nLoading development environment (Rails 7.0.5)\r\nirb(main):001:0> Rails.application.config.filter_parameters\r\n=> [:passw, :secret, :token, :_key, :crypt, :salt, :certificate, :otp, :ssn]\r\nirb(main):002:0> Customer.inspection_filter\r\n=> \r\n#<ActiveSupport::ParameterFilter:0x0000000111df2188\r\n @filters=[:passw, :secret, :token, :_key, :crypt, :salt, :certificate, :otp, :ssn],\r\n @mask=\"[FILTERED]\">\r\nirb(main):003:0> Customer.new(name: 'name', token: 'some-token')\r\n=> #<Customer:0x0000000111e9b878 id: nil, name: \"name\", token: \"[FILTERED]\", created_at: nil, updated_at: nil>\r\n```",
                        "comments": {
                            "nodes": [
                                {
                                    "createdAt": "2023-05-30T15:59:10Z",
                                    "bodyText": "Codecov Report\n\nMerging #199 (c89b21d) into master (076db75) will not change coverage.\nThe diff coverage is 100.00%.\n\n@@           Coverage Diff           @@\n##           master     #199   +/-   ##\n=======================================\n  Coverage   82.61%   82.61%           \n=======================================\n  Files          14       14           \n  Lines         719      719           \n=======================================\n  Hits          594      594           \n  Misses        125      125           \n\n\n\nImpacted Files\nCoverage \u0394\n\n\n\n\n\nlib/activerecord-multi-tenant/query_rewriter.rb\n92.30% <100.00%> (\u00f8)\n\n\n\n\n\ud83d\udce3 We\u2019re building smart automated test selection to slash your CI/CD build times. Learn more",
                                    "url": "https://github.com/citusdata/activerecord-multi-tenant/pull/199#issuecomment-1568689607",
                                    "author": {
                                        "login": "codecov"
                                    }
                                },
                                {
                                    "createdAt": "2023-06-02T15:51:56Z",
                                    "bodyText": "@nipe0324 thanks for your contribution.Could you add a test that covers this scenario?\nThanks",
                                    "url": "https://github.com/citusdata/activerecord-multi-tenant/pull/199#issuecomment-1573956564",
                                    "author": {
                                        "login": "gurkanindibay"
                                    }
                                },
                                {
                                    "createdAt": "2023-06-03T23:23:20Z",
                                    "bodyText": "OK. Please give me a few days to add a test that covers this scenario. Thank you.",
                                    "url": "https://github.com/citusdata/activerecord-multi-tenant/pull/199#issuecomment-1575258565",
                                    "author": {
                                        "login": "nipe0324"
                                    }
                                },
                                {
                                    "createdAt": "2023-06-04T01:54:27Z",
                                    "bodyText": "@microsoft-github-policy-service agree",
                                    "url": "https://github.com/citusdata/activerecord-multi-tenant/pull/199#issuecomment-1575323488",
                                    "author": {
                                        "login": "nipe0324"
                                    }
                                },
                                {
                                    "createdAt": "2023-06-04T12:45:23Z",
                                    "bodyText": "Hi @nipe0324\nI think you need a test for filters since this fix is about filters. By adding tests for filters\n\nWe will have a live documentation for filter support\nWe will make sure that our code base will support filter in the future\nWe can make sure that current PR fixes the issue\n\nThanks for your contribution",
                                    "url": "https://github.com/citusdata/activerecord-multi-tenant/pull/199#issuecomment-1575556080",
                                    "author": {
                                        "login": "gurkanindibay"
                                    }
                                },
                                {
                                    "createdAt": "2023-06-04T12:51:05Z",
                                    "bodyText": "@nipe0324 Just giving a starting point, I prepared a prompt in ChatGPT. I didn't test it but I think test should be like below\nhttps://chat.openai.com/share/ad170578-c628-4182-bc14-b4d46a2b1648",
                                    "url": "https://github.com/citusdata/activerecord-multi-tenant/pull/199#issuecomment-1575558178",
                                    "author": {
                                        "login": "gurkanindibay"
                                    }
                                },
                                {
                                    "createdAt": "2023-06-04T14:08:56Z",
                                    "bodyText": "@gurkanindibay Thanks a lot. I try to add filter test. Could you please confirm?",
                                    "url": "https://github.com/citusdata/activerecord-multi-tenant/pull/199#issuecomment-1575583947",
                                    "author": {
                                        "login": "nipe0324"
                                    }
                                }
                            ]
                        },
                        "reviews": {
                            "edges": [
                                {
                                    "node": {
                                        "state": "APPROVED",
                                        "bodyText": "",
                                        "comments": {
                                            "edges": []
                                        }
                                    }
                                },
                                {
                                    "node": {
                                        "state": "CHANGES_REQUESTED",
                                        "bodyText": "Unit tests are required",
                                        "comments": {
                                            "edges": []
                                        }
                                    }
                                },
                                {
                                    "node": {
                                        "state": "COMMENTED",
                                        "bodyText": "",
                                        "comments": {
                                            "edges": [
                                                {
                                                    "node": {
                                                        "bodyText": "This code should execute before ActiveRecord::Base called. So, I moved to here.",
                                                        "author": {
                                                            "login": "nipe0324"
                                                        },
                                                        "url": "https://github.com/citusdata/activerecord-multi-tenant/pull/199#discussion_r1216728197",
                                                        "originalCommit": {
                                                            "abbreviatedOid": "98809c7",
                                                            "authoredDate": "2023-06-04T01:47:36Z"
                                                        }
                                                    }
                                                }
                                            ]
                                        }
                                    }
                                },
                                {
                                    "node": {
                                        "state": "CHANGES_REQUESTED",
                                        "bodyText": "",
                                        "comments": {
                                            "edges": [
                                                {
                                                    "node": {
                                                        "bodyText": "Could you add a comment explaining this necessity? Could you refer the test in the comment as well?",
                                                        "author": {
                                                            "login": "gurkanindibay"
                                                        },
                                                        "url": "https://github.com/citusdata/activerecord-multi-tenant/pull/199#discussion_r1217019028",
                                                        "originalCommit": {
                                                            "abbreviatedOid": "98809c7",
                                                            "authoredDate": "2023-06-04T01:47:36Z"
                                                        }
                                                    }
                                                }
                                            ]
                                        }
                                    }
                                },
                                {
                                    "node": {
                                        "state": "COMMENTED",
                                        "bodyText": "",
                                        "comments": {
                                            "edges": [
                                                {
                                                    "node": {
                                                        "bodyText": "This 2 lines are not used. So I removed.",
                                                        "author": {
                                                            "login": "nipe0324"
                                                        },
                                                        "url": "https://github.com/citusdata/activerecord-multi-tenant/pull/199#discussion_r1217450003",
                                                        "originalCommit": {
                                                            "abbreviatedOid": "c89b21d",
                                                            "authoredDate": "2023-06-05T04:13:55Z"
                                                        }
                                                    }
                                                }
                                            ]
                                        }
                                    }
                                },
                                {
                                    "node": {
                                        "state": "COMMENTED",
                                        "bodyText": "",
                                        "comments": {
                                            "edges": [
                                                {
                                                    "node": {
                                                        "bodyText": "Ok, I commented.",
                                                        "author": {
                                                            "login": "nipe0324"
                                                        },
                                                        "url": "https://github.com/citusdata/activerecord-multi-tenant/pull/199#discussion_r1217450254",
                                                        "originalCommit": {
                                                            "abbreviatedOid": "98809c7",
                                                            "authoredDate": "2023-06-04T01:47:36Z"
                                                        }
                                                    }
                                                }
                                            ]
                                        }
                                    }
                                },
                                {
                                    "node": {
                                        "state": "COMMENTED",
                                        "bodyText": "",
                                        "comments": {
                                            "edges": [
                                                {
                                                    "node": {
                                                        "bodyText": "Thanks @nipe0324 for your contribution :)",
                                                        "author": {
                                                            "login": "gurkanindibay"
                                                        },
                                                        "url": "https://github.com/citusdata/activerecord-multi-tenant/pull/199#discussion_r1217511138",
                                                        "originalCommit": {
                                                            "abbreviatedOid": "c89b21d",
                                                            "authoredDate": "2023-06-05T04:13:55Z"
                                                        }
                                                    }
                                                }
                                            ]
                                        }
                                    }
                                },
                                {
                                    "node": {
                                        "state": "APPROVED",
                                        "bodyText": "",
                                        "comments": {
                                            "edges": []
                                        }
                                    }
                                }
                            ]
                        }
                    },
                    "textMatches": [
                        {
                            "property": "comments.body"
                        },
                        {
                            "property": "body"
                        }
                    ]
                },
                {
                    "node": {
                        "number": 1205,
                        "title": "[Feat] Add PARSeq model TF and PT",
                        "repository": {
                            "nameWithOwner": "mindee/doctr",
                            "primaryLanguage": {
                                "name": "Python"
                            }
                        },
                        "createdAt": "2023-05-30T08:07:05Z",
                        "mergedAt": "2023-06-15T10:53:56Z",
                        "url": "https://github.com/mindee/doctr/pull/1205",
                        "state": "MERGED",
                        "author": {
                            "login": "nikokks"
                        },
                        "editor": {
                            "login": "felixdittrich92"
                        },
                        "body": "Hi I am going to add PARSeq model to the list of doctr models.\r\n\r\nThis PR:\r\n\r\n- adds PARSeq tensorflow implementation\r\n- adds PARSeq pytorch implementation\r\n- adds corresponding tests\r\n\r\nAny feedback is welcome :)",
                        "comments": {
                            "nodes": [
                                {
                                    "createdAt": "2023-05-30T10:40:57Z",
                                    "bodyText": "@nikokks\ntemplate with todos for parseq/pytorch.py:  (maybe more helpful to explain what you need to do for the implementation)\n# Copyright (C) 2021-2023, Mindee.\n\n# This program is licensed under the Apache License 2.0.\n# See LICENSE or go to <https://opensource.org/licenses/Apache-2.0> for full license details.\n\nfrom copy import deepcopy\nfrom typing import Any, Callable, Dict, List, Optional, Tuple\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torchvision.models._utils import IntermediateLayerGetter\n\nfrom doctr.datasets import VOCABS\n\nfrom ...classification import vit_s\nfrom ...utils.pytorch import load_pretrained_params\nfrom .base import _PARSeq, _PARSeqPostProcessor\n\n__all__ = [\"PARSeq\", \"parseq\"]\n\ndefault_cfgs: Dict[str, Dict[str, Any]] = {\n    \"parseq\": {\n        \"mean\": (0.694, 0.695, 0.693),\n        \"std\": (0.299, 0.296, 0.301),\n        \"input_shape\": (3, 32, 128),\n        \"vocab\": VOCABS[\"french\"],\n        \"url\": None,\n    },\n}\n\nclass PARSeqDecoder(nn.Module):\n    \"\"\"Implements decoder module of the PARSeq model\n\n    Args:\n        TODO\n\n    \"\"\"\n    # TODO\n\nclass PARSeq(_PARSeq, nn.Module):\n    \"\"\"Implements a PARSeq architecture as described in `\"Scene Text Recognition\n    with Permuted Autoregressive Sequence Models\" <https://arxiv.org/pdf/2207.06966>`_.\n\n    Args:\n        feature_extractor: the backbone serving as feature extractor\n        vocab: vocabulary used for encoding\n        embedding_units: number of embedding units\n        max_length: maximum word length handled by the model\n        dropout_prob: dropout probability of the encoder LSTM\n        input_shape: input shape of the image\n        exportable: onnx exportable returns only logits\n        cfg: dictionary containing information about the model\n    \"\"\"\n\n    def __init__(\n        self,\n        feature_extractor,\n        vocab: str,\n        embedding_units: int,\n        max_length: int = 25,\n        input_shape: Tuple[int, int, int] = (3, 32, 128),\n        exportable: bool = False,\n        cfg: Optional[Dict[str, Any]] = None,\n    ) -> None:\n        super().__init__()\n        self.vocab = vocab\n        self.exportable = exportable\n        self.cfg = cfg\n        self.max_length = max_length + 3  # Add 1 step for EOS, 1 for SOS, 1 for PAD\n\n        self.feat_extractor = feature_extractor\n        self.decoder = PARSeqDecoder() # TODO\n        self.head = nn.Linear(embedding_units, len(self.vocab) + 3)\n\n        self.postprocessor = PARSeqPostProcessor(vocab=self.vocab)\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        target: Optional[List[str]] = None,\n        return_model_output: bool = False,\n        return_preds: bool = False,\n    ) -> Dict[str, Any]:\n        features = self.feat_extractor(x)[\"features\"]  # (batch_size, patches_seqlen, d_model)\n\n        if target is not None:\n            _gt, _seq_len = self.build_target(target)\n            gt, seq_len = torch.from_numpy(_gt).to(dtype=torch.long), torch.tensor(_seq_len)\n            gt, seq_len = gt.to(x.device), seq_len.to(x.device)\n\n        if self.training and target is None:\n            raise ValueError(\"Need to provide labels during training\")\n\n        # TODO\n\n        out: Dict[str, Any] = {}\n        if self.exportable:\n            out[\"logits\"] = decoded_features\n            return out\n\n        if return_model_output:\n            out[\"out_map\"] = decoded_features\n\n        if target is None or return_preds:\n            # Post-process boxes\n            out[\"preds\"] = self.postprocessor(decoded_features)\n\n        if target is not None:\n            out[\"loss\"] = self.compute_loss(decoded_features, gt, seq_len)\n\n        return out\n\n    @staticmethod\n    def compute_loss(\n        model_output: torch.Tensor,\n        gt: torch.Tensor,\n        seq_len: torch.Tensor,\n    ) -> torch.Tensor:\n        \"\"\"Compute categorical cross-entropy loss for the model.\n        Sequences are masked after the EOS character.\n\n        Args:\n            model_output: predicted logits of the model\n            gt: the encoded tensor with gt labels\n            seq_len: lengths of each gt word inside the batch\n\n        Returns:\n            The loss of the model on the batch\n        \"\"\"\n        # TODO\n\n\nclass PARSeqPostProcessor(_PARSeqPostProcessor):\n    \"\"\"Post processor for PARSeq architecture\n\n    Args:\n        vocab: string containing the ordered sequence of supported characters\n    \"\"\"\n\n    def __call__(\n        self,\n        logits: torch.Tensor,\n    ) -> List[Tuple[str, float]]:\n        # TODO\n\n\ndef _parseq(\n    arch: str,\n    pretrained: bool,\n    backbone_fn: Callable[[bool], nn.Module],\n    layer: str,\n    pretrained_backbone: bool = True,\n    ignore_keys: Optional[List[str]] = None,\n    **kwargs: Any,\n) -> PARSeq:\n    pretrained_backbone = pretrained_backbone and not pretrained\n\n    # Patch the config\n    _cfg = deepcopy(default_cfgs[arch])\n    _cfg[\"vocab\"] = kwargs.get(\"vocab\", _cfg[\"vocab\"])\n    _cfg[\"input_shape\"] = kwargs.get(\"input_shape\", _cfg[\"input_shape\"])\n\n    kwargs[\"vocab\"] = _cfg[\"vocab\"]\n    kwargs[\"input_shape\"] = _cfg[\"input_shape\"]\n\n    # Feature extractor\n    feat_extractor = IntermediateLayerGetter(\n        backbone_fn(pretrained_backbone, input_shape=_cfg[\"input_shape\"]),  # type: ignore[call-arg]\n        {layer: \"features\"},\n    )\n\n    # Build the model\n    model = PARSeq(feat_extractor, cfg=_cfg, **kwargs)\n    # Load pretrained parameters\n    if pretrained:\n        # The number of classes is not the same as the number of classes in the pretrained model =>\n        # remove the last layer weights\n        _ignore_keys = ignore_keys if _cfg[\"vocab\"] != default_cfgs[arch][\"vocab\"] else None\n        load_pretrained_params(model, default_cfgs[arch][\"url\"], ignore_keys=_ignore_keys)\n\n    return model\n\n\ndef parseq(pretrained: bool = False, **kwargs: Any) -> PARSeq:\n    \"\"\"PARSeq architecture from\n    `\"Scene Text Recognition with Permuted Autoregressive Sequence Models\" <https://arxiv.org/pdf/2207.06966>`_.\n\n    >>> import torch\n    >>> from doctr.models import parseq\n    >>> model = parseq(pretrained=False)\n    >>> input_tensor = torch.rand((1, 3, 32, 128))\n    >>> out = model(input_tensor)\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on our text recognition dataset\n\n    Returns:\n        text recognition architecture\n    \"\"\"\n\n    return _parseq(\n        \"parseq\",\n        pretrained,\n        vit_s,\n        \"1\",\n        embedding_units=384,\n        ignore_keys=[\"head.weight\", \"head.bias\"],\n        **kwargs,\n    )",
                                    "url": "https://github.com/mindee/doctr/pull/1205#issuecomment-1568204797",
                                    "author": {
                                        "login": "felixdittrich92"
                                    }
                                },
                                {
                                    "createdAt": "2023-05-30T11:26:11Z",
                                    "bodyText": "Hi again;\nwhere do I put the config model ?\ndefault_cfgs: Dict[str, Dict[str, Any]] = {\n    \"parseq\": {\n        \"mean\": (0.694, 0.695, 0.693),\n        \"std\": (0.299, 0.296, 0.301),\n        \"charset_train\": \"0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\\\"#$%&'()*+,-./:;<=>?@[\\\\]^_`{|}~\",\n        \"charset_test\": \"0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\\\"#$%&'()*+,-./:;<=>?@[\\\\]^_`{|}~\" ,\n        \"max_label_length\": 25 ,\n        \"batch_size\": 384,\n        \"lr\": 7e-4,\n        \"warmup_pct\": 0.075,\n        \"weight_decay\": 0.0,\n        \"img_size\": [ 32, 128 ],\n        \"patch_size\": [ 4, 8 ] ,\n        \"embed_dim\": 384 ,\n        \"enc_num_heads\": 6,\n        \"enc_mlp_ratio\": 4,\n        \"enc_depth\": 12,\n        \"dec_num_heads\": 12,\n        \"dec_mlp_ratio\": 4 ,\n        \"dec_depth\": 1,\n        \"perm_num\": 6 ,\n        \"perm_forward\": True ,\n        \"perm_mirrored\": True ,\n        \"decode_ar\": True,\n        \"refine_iters\": 1,\n        \"dropout\": 0.1,\n        \"vocab\": VOCABS[\"french\"],\n        \"input_shape\": (3, 32, 128),\n        \"classes\": list(VOCABS[\"french\"]),\n        \"url\": \"/home/nikkokks/Desktop/github/parseq-bb5792a6.pt\",\n        }\n}```",
                                    "url": "https://github.com/mindee/doctr/pull/1205#issuecomment-1568265905",
                                    "author": {
                                        "login": "nikokks"
                                    }
                                },
                                {
                                    "createdAt": "2023-05-30T12:04:19Z",
                                    "bodyText": "Hi again; where do I put the config model ?\ndefault_cfgs: Dict[str, Dict[str, Any]] = {\n    \"parseq\": {\n        \"mean\": (0.694, 0.695, 0.693),\n        \"std\": (0.299, 0.296, 0.301),\n        \"charset_train\": \"0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\\\"#$%&'()*+,-./:;<=>?@[\\\\]^_`{|}~\",\n        \"charset_test\": \"0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\\\"#$%&'()*+,-./:;<=>?@[\\\\]^_`{|}~\" ,\n        \"max_label_length\": 25 ,\n        \"batch_size\": 384,\n        \"lr\": 7e-4,\n        \"warmup_pct\": 0.075,\n        \"weight_decay\": 0.0,\n        \"img_size\": [ 32, 128 ],\n        \"patch_size\": [ 4, 8 ] ,\n        \"embed_dim\": 384 ,\n        \"enc_num_heads\": 6,\n        \"enc_mlp_ratio\": 4,\n        \"enc_depth\": 12,\n        \"dec_num_heads\": 12,\n        \"dec_mlp_ratio\": 4 ,\n        \"dec_depth\": 1,\n        \"perm_num\": 6 ,\n        \"perm_forward\": True ,\n        \"perm_mirrored\": True ,\n        \"decode_ar\": True,\n        \"refine_iters\": 1,\n        \"dropout\": 0.1,\n        \"vocab\": VOCABS[\"french\"],\n        \"input_shape\": (3, 32, 128),\n        \"classes\": list(VOCABS[\"french\"]),\n        \"url\": \"/home/nikkokks/Desktop/github/parseq-bb5792a6.pt\",\n        }\n}```\n\n\nNo need to modify the default_cfgs :)\nThe only values you need are the onces for the decoder part:\n\"dec_num_heads\": 12,\n\"dec_mlp_ratio\": 4 ,\n\"dec_depth\": 1,\n\nyou can init the PARSeqDecoder with this values by default and same for the decoding:\n\"perm_num\": 6 ,\n\"perm_forward\": True ,\n\"perm_mirrored\": True ,\n\"decode_ar\": True,\n\"refine_iters\": 1,\n\nThen update\nclass PARSeq(_PARSeq, nn.Module):\n    \n    def __init__(\n        self,\n        feature_extractor,\n        vocab: str,\n        embedding_units: int,\n        max_length: int = 25,\n        input_shape: Tuple[int, int, int] = (3, 32, 128),\n        exportable: bool = False,\n        perm_num: int = 6,\n        perm_forward: bool = True,\n        ....\n\nso we can update the model by passing this config as kwargs :)",
                                    "url": "https://github.com/mindee/doctr/pull/1205#issuecomment-1568315407",
                                    "author": {
                                        "login": "felixdittrich92"
                                    }
                                },
                                {
                                    "createdAt": "2023-05-30T12:22:48Z",
                                    "bodyText": "I have modified the file recognition/parseq/pytorch.py\nI did not implement the decoder because it is in classification/parseq\nto end with recognition/parseq/pytorch.py, what do you think about it?\nI do not use the vocab file in postprocessor but the tokenizer delivered with the model. Is it ok ?",
                                    "url": "https://github.com/mindee/doctr/pull/1205#issuecomment-1568340246",
                                    "author": {
                                        "login": "nikokks"
                                    }
                                },
                                {
                                    "createdAt": "2023-05-30T12:32:45Z",
                                    "bodyText": "I have modified the file recognition/parseq/pytorch.py I did not implement the decoder because it is in classification/parseq to end with recognition/parseq/pytorch.py, what do you think about it? I do not use the vocab file in postprocessor but the tokenizer delivered with the model. Is it ok ?\n\nWe should keep our tokenization at the end you can copy paste it from the ViTSTR implementation in doctr :)\nWe should not touch the classification folder (that's the place for the backbone / encoder models) but in the template i have already added the backbone loading (for parseq which is vit_s)\nSo what you need to implement is the decoder in parseq/pytorch.py and the forward + compute loss function\nI would say copy the relevant stuff to parseq/pytorch.py and clean up all this classification additions\nif you need some help to integrate the decoder / forward i am happy to help :)",
                                    "url": "https://github.com/mindee/doctr/pull/1205#issuecomment-1568353931",
                                    "author": {
                                        "login": "felixdittrich92"
                                    }
                                },
                                {
                                    "createdAt": "2023-05-30T13:57:56Z",
                                    "bodyText": "I have done some changes like removing the parseq tokenizer for your vocab.\nDo you have others advices for recognition/parseq ?",
                                    "url": "https://github.com/mindee/doctr/pull/1205#issuecomment-1568484769",
                                    "author": {
                                        "login": "nikokks"
                                    }
                                },
                                {
                                    "createdAt": "2023-05-30T14:13:18Z",
                                    "bodyText": "Do you think I should implement the decoder in recognition/parseq or in classification/parseq ?\nAnd for the forward and compute loss of PARSeq , do I put it in recognition/parseq or just in classification/parseq ?",
                                    "url": "https://github.com/mindee/doctr/pull/1205#issuecomment-1568511297",
                                    "author": {
                                        "login": "nikokks"
                                    }
                                },
                                {
                                    "createdAt": "2023-05-30T14:20:59Z",
                                    "bodyText": "Do you think I should implement the decoder in recognition/parseq or in classification/parseq ? And for the forward and compute loss of PARSeq , do I put it in recognition/parseq or just in classification/parseq ?\n\nAt the end all should be inside recognition/parseq/pytorch.py\nFor the model addition you don't need to change anything in the classification folder :)\nAbove i have marked the files with a \"remove\" comment :)",
                                    "url": "https://github.com/mindee/doctr/pull/1205#issuecomment-1568524260",
                                    "author": {
                                        "login": "felixdittrich92"
                                    }
                                },
                                {
                                    "createdAt": "2023-05-31T09:57:08Z",
                                    "bodyText": "Hi\nto test the temporary implementation I try to load the model parseq, but the keys of state_dict are not the same now.\nthis is what I get\n\tMissing key(s) in state_dict: \"encoder.0.cls_token\", \"encoder.0.positions\", \"encoder.0.proj.weight\", \"encoder.0.proj.bias\", \"encoder.1.layer_norm_input.weight\", \"encoder.1.layer_norm_input.bias\", \"encoder.1.layer_norm_attention.weight\", \"encoder.1.layer_norm_attention.bias\", \"encoder.1.layer_norm_output.weight\", \"encoder.1.layer_norm_output.bias\", \"encoder.1.attention.0.linear_layers.0.weight\", \"encoder.1.attention.0.linear_layers.0.bias\", \"encoder.1.attention.0.linear_layers.1.weight\", \"encoder.1.attention.0.linear_layers.1.bias\", \"encoder.1.attention.0.linear_layers.2.weight\", \"encoder.1.attention.0.linear_layers.2.bias\", \"encoder.1.attention.0.output_linear.weight\", \"encoder.1.attention.0.output_linear.bias\", \"encoder.1.attention.1.linear_layers.0.weight\", \"encoder.1.attention.1.linear_layers.0.bias\", \"encoder.1.attention.1.linear_layers.1.weight\", \"encoder.1.attention.1.linear_layers.1.bias\", \"encoder.1.attention.1.linear_layers.2.weight\", \"encoder.1.attention.1.linear_layers.2.bias\", \"encoder.1.attention.1.output_linear.weight\", \"encoder.1.attention.1.output_linear.bias\", \"encoder.1.attention.2.linear_layers.0.weight\", \"encoder.1.attention.2.linear_layers.0.bias\", \"encoder.1.attention.2.linear_layers.1.weight\", \"encoder.1.attention.2.linear_layers.1.bias\", \"encoder.1.attention.2.linear_layers.2.weight\", \"encoder.1.attention.2.linear_layers.2.bias\", \"encoder.1.attention.2.output_linear.weight\", \"encoder.1.attention.2.output_linear.bias\", \"encoder.1.attention.3.linear_layers.0.weight\", \"encoder.1.attention.3.linear_layers.0.bias\", \"encoder.1.attention.3.linear_layers.1.weight\", \"encoder.1.attention.3.linear_layers.1.bias\", \"encoder.1.attention.3.linear_layers.2.weight\", \"encoder.1.attention.3.linear_layers.2.bias\", \"encoder.1.attention.3.output_linear.weight\", \"encoder.1.attention.3.output_linear.bias\", \"encoder.1.attention.4.linear_layers.0.weight\", \"encoder.1.attention.4.linear_layers.0.bias\", \"encoder.1.attention.4.linear_layers.1.weight\", \"encoder.1.attention.4.linear_layers.1.bias\", \"encoder.1.attention.4.linear_layers.2.weight\", \"encoder.1.attention.4.linear_layers.2.bias\", \"encoder.1.attention.4.output_linear.weight\", \"encoder.1.attention.4.output_linear.bias\", \"encoder.1.attention.5.linear_layers.0.weight\", \"encoder.1.attention.5.linear_layers.0.bias\", \"encoder.1.attention.5.linear_layers.1.weight\", \"encoder.1.attention.5.linear_layers.1.bias\", \"encoder.1.attention.5.linear_layers.2.weight\", \"encoder.1.attention.5.linear_layers.2.bias\", \"encoder.1.attention.5.output_linear.weight\", \"encoder.1.attention.5.output_linear.bias\", \"encoder.1.attention.6.linear_layers.0.weight\", \"encoder.1.attention.6.linear_layers.0.bias\", \"encoder.1.attention.6.linear_layers.1.weight\", \"encoder.1.attention.6.linear_layers.1.bias\", \"encoder.1.attention.6.linear_layers.2.weight\", \"encoder.1.attention.6.linear_layers.2.bias\", \"encoder.1.attention.6.output_linear.weight\", \"encoder.1.attention.6.output_linear.bias\", \"encoder.1.attention.7.linear_layers.0.weight\", \"encoder.1.attention.7.linear_layers.0.bias\", \"encoder.1.attention.7.linear_layers.1.weight\", \"encoder.1.attention.7.linear_layers.1.bias\", \"encoder.1.attention.7.linear_layers.2.weight\", \"encoder.1.attention.7.linear_layers.2.bias\", \"encoder.1.attention.7.output_linear.weight\", \"encoder.1.attention.7.output_linear.bias\", \"encoder.1.attention.8.linear_layers.0.weight\", \"encoder.1.attention.8.linear_layers.0.bias\", \"encoder.1.attention.8.linear_layers.1.weight\", \"encoder.1.attention.8.linear_layers.1.bias\", \"encoder.1.attention.8.linear_layers.2.weight\", \"encoder.1.attention.8.linear_layers.2.bias\", \"encoder.1.attention.8.output_linear.weight\", \"encoder.1.attention.8.output_linear.bias\", \"encoder.1.attention.9.linear_layers.0.weight\", \"encoder.1.attention.9.linear_layers.0.bias\", \"encoder.1.attention.9.linear_layers.1.weight\", \"encoder.1.attention.9.linear_layers.1.bias\", \"encoder.1.attention.9.linear_layers.2.weight\", \"encoder.1.attention.9.linear_layers.2.bias\", \"encoder.1.attention.9.output_linear.weight\", \"encoder.1.attention.9.output_linear.bias\", \"encoder.1.attention.10.linear_layers.0.weight\", \"encoder.1.attention.10.linear_layers.0.bias\", \"encoder.1.attention.10.linear_layers.1.weight\", \"encoder.1.attention.10.linear_layers.1.bias\", \"encoder.1.attention.10.linear_layers.2.weight\", \"encoder.1.attention.10.linear_layers.2.bias\", \"encoder.1.attention.10.output_linear.weight\", \"encoder.1.attention.10.output_linear.bias\", \"encoder.1.attention.11.linear_layers.0.weight\", \"encoder.1.attention.11.linear_layers.0.bias\", \"encoder.1.attention.11.linear_layers.1.weight\", \"encoder.1.attention.11.linear_layers.1.bias\", \"encoder.1.attention.11.linear_layers.2.weight\", \"encoder.1.attention.11.linear_layers.2.bias\", \"encoder.1.attention.11.output_linear.weight\", \"encoder.1.attention.11.output_linear.bias\", \"encoder.1.position_feed_forward.0.0.weight\", \"encoder.1.position_feed_forward.0.0.bias\", \"encoder.1.position_feed_forward.0.3.weight\", \"encoder.1.position_feed_forward.0.3.bias\", \"encoder.1.position_feed_forward.1.0.weight\", \"encoder.1.position_feed_forward.1.0.bias\", \"encoder.1.position_feed_forward.1.3.weight\", \"encoder.1.position_feed_forward.1.3.bias\", \"encoder.1.position_feed_forward.2.0.weight\", \"encoder.1.position_feed_forward.2.0.bias\", \"encoder.1.position_feed_forward.2.3.weight\", \"encoder.1.position_feed_forward.2.3.bias\", \"encoder.1.position_feed_forward.3.0.weight\", \"encoder.1.position_feed_forward.3.0.bias\", \"encoder.1.position_feed_forward.3.3.weight\", \"encoder.1.position_feed_forward.3.3.bias\", \"encoder.1.position_feed_forward.4.0.weight\", \"encoder.1.position_feed_forward.4.0.bias\", \"encoder.1.position_feed_forward.4.3.weight\", \"encoder.1.position_feed_forward.4.3.bias\", \"encoder.1.position_feed_forward.5.0.weight\", \"encoder.1.position_feed_forward.5.0.bias\", \"encoder.1.position_feed_forward.5.3.weight\", \"encoder.1.position_feed_forward.5.3.bias\", \"encoder.1.position_feed_forward.6.0.weight\", \"encoder.1.position_feed_forward.6.0.bias\", \"encoder.1.position_feed_forward.6.3.weight\", \"encoder.1.position_feed_forward.6.3.bias\", \"encoder.1.position_feed_forward.7.0.weight\", \"encoder.1.position_feed_forward.7.0.bias\", \"encoder.1.position_feed_forward.7.3.weight\", \"encoder.1.position_feed_forward.7.3.bias\", \"encoder.1.position_feed_forward.8.0.weight\", \"encoder.1.position_feed_forward.8.0.bias\", \"encoder.1.position_feed_forward.8.3.weight\", \"encoder.1.position_feed_forward.8.3.bias\", \"encoder.1.position_feed_forward.9.0.weight\", \"encoder.1.position_feed_forward.9.0.bias\", \"encoder.1.position_feed_forward.9.3.weight\", \"encoder.1.position_feed_forward.9.3.bias\", \"encoder.1.position_feed_forward.10.0.weight\", \"encoder.1.position_feed_forward.10.0.bias\", \"encoder.1.position_feed_forward.10.3.weight\", \"encoder.1.position_feed_forward.10.3.bias\", \"encoder.1.position_feed_forward.11.0.weight\", \"encoder.1.position_feed_forward.11.0.bias\", \"encoder.1.position_feed_forward.11.3.weight\", \"encoder.1.position_feed_forward.11.3.bias\".\n\t\n\t\n\t \n\tUnexpected key(s) in state_dict: \"encoder.pos_embed\", \"encoder.patch_embed.proj.weight\", \"encoder.patch_embed.proj.bias\", \"encoder.blocks.0.norm1.weight\", \"encoder.blocks.0.norm1.bias\", \"encoder.blocks.0.attn.qkv.weight\", \"encoder.blocks.0.attn.qkv.bias\", \"encoder.blocks.0.attn.proj.weight\", \"encoder.blocks.0.attn.proj.bias\", \"encoder.blocks.0.norm2.weight\", \"encoder.blocks.0.norm2.bias\", \"encoder.blocks.0.mlp.fc1.weight\", \"encoder.blocks.0.mlp.fc1.bias\", \"encoder.blocks.0.mlp.fc2.weight\", \"encoder.blocks.0.mlp.fc2.bias\", \"encoder.blocks.1.norm1.weight\", \"encoder.blocks.1.norm1.bias\", \"encoder.blocks.1.attn.qkv.weight\", \"encoder.blocks.1.attn.qkv.bias\", \"encoder.blocks.1.attn.proj.weight\", \"encoder.blocks.1.attn.proj.bias\", \"encoder.blocks.1.norm2.weight\", \"encoder.blocks.1.norm2.bias\", \"encoder.blocks.1.mlp.fc1.weight\", \"encoder.blocks.1.mlp.fc1.bias\", \"encoder.blocks.1.mlp.fc2.weight\", \"encoder.blocks.1.mlp.fc2.bias\", \"encoder.blocks.2.norm1.weight\", \"encoder.blocks.2.norm1.bias\", \"encoder.blocks.2.attn.qkv.weight\", \"encoder.blocks.2.attn.qkv.bias\", \"encoder.blocks.2.attn.proj.weight\", \"encoder.blocks.2.attn.proj.bias\", \"encoder.blocks.2.norm2.weight\", \"encoder.blocks.2.norm2.bias\", \"encoder.blocks.2.mlp.fc1.weight\", \"encoder.blocks.2.mlp.fc1.bias\", \"encoder.blocks.2.mlp.fc2.weight\", \"encoder.blocks.2.mlp.fc2.bias\", \"encoder.blocks.3.norm1.weight\", \"encoder.blocks.3.norm1.bias\", \"encoder.blocks.3.attn.qkv.weight\", \"encoder.blocks.3.attn.qkv.bias\", \"encoder.blocks.3.attn.proj.weight\", \"encoder.blocks.3.attn.proj.bias\", \"encoder.blocks.3.norm2.weight\", \"encoder.blocks.3.norm2.bias\", \"encoder.blocks.3.mlp.fc1.weight\", \"encoder.blocks.3.mlp.fc1.bias\", \"encoder.blocks.3.mlp.fc2.weight\", \"encoder.blocks.3.mlp.fc2.bias\", \"encoder.blocks.4.norm1.weight\", \"encoder.blocks.4.norm1.bias\", \"encoder.blocks.4.attn.qkv.weight\", \"encoder.blocks.4.attn.qkv.bias\", \"encoder.blocks.4.attn.proj.weight\", \"encoder.blocks.4.attn.proj.bias\", \"encoder.blocks.4.norm2.weight\", \"encoder.blocks.4.norm2.bias\", \"encoder.blocks.4.mlp.fc1.weight\", \"encoder.blocks.4.mlp.fc1.bias\", \"encoder.blocks.4.mlp.fc2.weight\", \"encoder.blocks.4.mlp.fc2.bias\", \"encoder.blocks.5.norm1.weight\", \"encoder.blocks.5.norm1.bias\", \"encoder.blocks.5.attn.qkv.weight\", \"encoder.blocks.5.attn.qkv.bias\", \"encoder.blocks.5.attn.proj.weight\", \"encoder.blocks.5.attn.proj.bias\", \"encoder.blocks.5.norm2.weight\", \"encoder.blocks.5.norm2.bias\", \"encoder.blocks.5.mlp.fc1.weight\", \"encoder.blocks.5.mlp.fc1.bias\", \"encoder.blocks.5.mlp.fc2.weight\", \"encoder.blocks.5.mlp.fc2.bias\", \"encoder.blocks.6.norm1.weight\", \"encoder.blocks.6.norm1.bias\", \"encoder.blocks.6.attn.qkv.weight\", \"encoder.blocks.6.attn.qkv.bias\", \"encoder.blocks.6.attn.proj.weight\", \"encoder.blocks.6.attn.proj.bias\", \"encoder.blocks.6.norm2.weight\", \"encoder.blocks.6.norm2.bias\", \"encoder.blocks.6.mlp.fc1.weight\", \"encoder.blocks.6.mlp.fc1.bias\", \"encoder.blocks.6.mlp.fc2.weight\", \"encoder.blocks.6.mlp.fc2.bias\", \"encoder.blocks.7.norm1.weight\", \"encoder.blocks.7.norm1.bias\", \"encoder.blocks.7.attn.qkv.weight\", \"encoder.blocks.7.attn.qkv.bias\", \"encoder.blocks.7.attn.proj.weight\", \"encoder.blocks.7.attn.proj.bias\", \"encoder.blocks.7.norm2.weight\", \"encoder.blocks.7.norm2.bias\", \"encoder.blocks.7.mlp.fc1.weight\", \"encoder.blocks.7.mlp.fc1.bias\", \"encoder.blocks.7.mlp.fc2.weight\", \"encoder.blocks.7.mlp.fc2.bias\", \"encoder.blocks.8.norm1.weight\", \"encoder.blocks.8.norm1.bias\", \"encoder.blocks.8.attn.qkv.weight\", \"encoder.blocks.8.attn.qkv.bias\", \"encoder.blocks.8.attn.proj.weight\", \"encoder.blocks.8.attn.proj.bias\", \"encoder.blocks.8.norm2.weight\", \"encoder.blocks.8.norm2.bias\", \"encoder.blocks.8.mlp.fc1.weight\", \"encoder.blocks.8.mlp.fc1.bias\", \"encoder.blocks.8.mlp.fc2.weight\", \"encoder.blocks.8.mlp.fc2.bias\", \"encoder.blocks.9.norm1.weight\", \"encoder.blocks.9.norm1.bias\", \"encoder.blocks.9.attn.qkv.weight\", \"encoder.blocks.9.attn.qkv.bias\", \"encoder.blocks.9.attn.proj.weight\", \"encoder.blocks.9.attn.proj.bias\", \"encoder.blocks.9.norm2.weight\", \"encoder.blocks.9.norm2.bias\", \"encoder.blocks.9.mlp.fc1.weight\", \"encoder.blocks.9.mlp.fc1.bias\", \"encoder.blocks.9.mlp.fc2.weight\", \"encoder.blocks.9.mlp.fc2.bias\", \"encoder.blocks.10.norm1.weight\", \"encoder.blocks.10.norm1.bias\", \"encoder.blocks.10.attn.qkv.weight\", \"encoder.blocks.10.attn.qkv.bias\", \"encoder.blocks.10.attn.proj.weight\", \"encoder.blocks.10.attn.proj.bias\", \"encoder.blocks.10.norm2.weight\", \"encoder.blocks.10.norm2.bias\", \"encoder.blocks.10.mlp.fc1.weight\", \"encoder.blocks.10.mlp.fc1.bias\", \"encoder.blocks.10.mlp.fc2.weight\", \"encoder.blocks.10.mlp.fc2.bias\", \"encoder.blocks.11.norm1.weight\", \"encoder.blocks.11.norm1.bias\", \"encoder.blocks.11.attn.qkv.weight\", \"encoder.blocks.11.attn.qkv.bias\", \"encoder.blocks.11.attn.proj.weight\", \"encoder.blocks.11.attn.proj.bias\", \"encoder.blocks.11.norm2.weight\", \"encoder.blocks.11.norm2.bias\", \"encoder.blocks.11.mlp.fc1.weight\", \"encoder.blocks.11.mlp.fc1.bias\", \"encoder.blocks.11.mlp.fc2.weight\", \"encoder.blocks.11.mlp.fc2.bias\", \"encoder.norm.weight\", \"encoder.norm.bias\".\n\t\n\t\n\t\n\n\ndo you know how to do ?\nThis is due to the difference between the parseq implementation and and the doctr VitSTR Encoder",
                                    "url": "https://github.com/mindee/doctr/pull/1205#issuecomment-1569876032",
                                    "author": {
                                        "login": "nikokks"
                                    }
                                },
                                {
                                    "createdAt": "2023-05-31T10:04:55Z",
                                    "bodyText": "Hi to test the temporary implementation I try to load the model parseq, but the keys of state_dict are not the same now.\nthis is what I get\n\tMissing key(s) in state_dict: \"encoder.0.cls_token\", \"encoder.0.positions\", \"encoder.0.proj.weight\", \"encoder.0.proj.bias\", \"encoder.1.layer_norm_input.weight\", \"encoder.1.layer_norm_input.bias\", \"encoder.1.layer_norm_attention.weight\", \"encoder.1.layer_norm_attention.bias\", \"encoder.1.layer_norm_output.weight\", \"encoder.1.layer_norm_output.bias\", \"encoder.1.attention.0.linear_layers.0.weight\", \"encoder.1.attention.0.linear_layers.0.bias\", \"encoder.1.attention.0.linear_layers.1.weight\", \"encoder.1.attention.0.linear_layers.1.bias\", \"encoder.1.attention.0.linear_layers.2.weight\", \"encoder.1.attention.0.linear_layers.2.bias\", \"encoder.1.attention.0.output_linear.weight\", \"encoder.1.attention.0.output_linear.bias\", \"encoder.1.attention.1.linear_layers.0.weight\", \"encoder.1.attention.1.linear_layers.0.bias\", \"encoder.1.attention.1.linear_layers.1.weight\", \"encoder.1.attention.1.linear_layers.1.bias\", \"encoder.1.attention.1.linear_layers.2.weight\", \"encoder.1.attention.1.linear_layers.2.bias\", \"encoder.1.attention.1.output_linear.weight\", \"encoder.1.attention.1.output_linear.bias\", \"encoder.1.attention.2.linear_layers.0.weight\", \"encoder.1.attention.2.linear_layers.0.bias\", \"encoder.1.attention.2.linear_layers.1.weight\", \"encoder.1.attention.2.linear_layers.1.bias\", \"encoder.1.attention.2.linear_layers.2.weight\", \"encoder.1.attention.2.linear_layers.2.bias\", \"encoder.1.attention.2.output_linear.weight\", \"encoder.1.attention.2.output_linear.bias\", \"encoder.1.attention.3.linear_layers.0.weight\", \"encoder.1.attention.3.linear_layers.0.bias\", \"encoder.1.attention.3.linear_layers.1.weight\", \"encoder.1.attention.3.linear_layers.1.bias\", \"encoder.1.attention.3.linear_layers.2.weight\", \"encoder.1.attention.3.linear_layers.2.bias\", \"encoder.1.attention.3.output_linear.weight\", \"encoder.1.attention.3.output_linear.bias\", \"encoder.1.attention.4.linear_layers.0.weight\", \"encoder.1.attention.4.linear_layers.0.bias\", \"encoder.1.attention.4.linear_layers.1.weight\", \"encoder.1.attention.4.linear_layers.1.bias\", \"encoder.1.attention.4.linear_layers.2.weight\", \"encoder.1.attention.4.linear_layers.2.bias\", \"encoder.1.attention.4.output_linear.weight\", \"encoder.1.attention.4.output_linear.bias\", \"encoder.1.attention.5.linear_layers.0.weight\", \"encoder.1.attention.5.linear_layers.0.bias\", \"encoder.1.attention.5.linear_layers.1.weight\", \"encoder.1.attention.5.linear_layers.1.bias\", \"encoder.1.attention.5.linear_layers.2.weight\", \"encoder.1.attention.5.linear_layers.2.bias\", \"encoder.1.attention.5.output_linear.weight\", \"encoder.1.attention.5.output_linear.bias\", \"encoder.1.attention.6.linear_layers.0.weight\", \"encoder.1.attention.6.linear_layers.0.bias\", \"encoder.1.attention.6.linear_layers.1.weight\", \"encoder.1.attention.6.linear_layers.1.bias\", \"encoder.1.attention.6.linear_layers.2.weight\", \"encoder.1.attention.6.linear_layers.2.bias\", \"encoder.1.attention.6.output_linear.weight\", \"encoder.1.attention.6.output_linear.bias\", \"encoder.1.attention.7.linear_layers.0.weight\", \"encoder.1.attention.7.linear_layers.0.bias\", \"encoder.1.attention.7.linear_layers.1.weight\", \"encoder.1.attention.7.linear_layers.1.bias\", \"encoder.1.attention.7.linear_layers.2.weight\", \"encoder.1.attention.7.linear_layers.2.bias\", \"encoder.1.attention.7.output_linear.weight\", \"encoder.1.attention.7.output_linear.bias\", \"encoder.1.attention.8.linear_layers.0.weight\", \"encoder.1.attention.8.linear_layers.0.bias\", \"encoder.1.attention.8.linear_layers.1.weight\", \"encoder.1.attention.8.linear_layers.1.bias\", \"encoder.1.attention.8.linear_layers.2.weight\", \"encoder.1.attention.8.linear_layers.2.bias\", \"encoder.1.attention.8.output_linear.weight\", \"encoder.1.attention.8.output_linear.bias\", \"encoder.1.attention.9.linear_layers.0.weight\", \"encoder.1.attention.9.linear_layers.0.bias\", \"encoder.1.attention.9.linear_layers.1.weight\", \"encoder.1.attention.9.linear_layers.1.bias\", \"encoder.1.attention.9.linear_layers.2.weight\", \"encoder.1.attention.9.linear_layers.2.bias\", \"encoder.1.attention.9.output_linear.weight\", \"encoder.1.attention.9.output_linear.bias\", \"encoder.1.attention.10.linear_layers.0.weight\", \"encoder.1.attention.10.linear_layers.0.bias\", \"encoder.1.attention.10.linear_layers.1.weight\", \"encoder.1.attention.10.linear_layers.1.bias\", \"encoder.1.attention.10.linear_layers.2.weight\", \"encoder.1.attention.10.linear_layers.2.bias\", \"encoder.1.attention.10.output_linear.weight\", \"encoder.1.attention.10.output_linear.bias\", \"encoder.1.attention.11.linear_layers.0.weight\", \"encoder.1.attention.11.linear_layers.0.bias\", \"encoder.1.attention.11.linear_layers.1.weight\", \"encoder.1.attention.11.linear_layers.1.bias\", \"encoder.1.attention.11.linear_layers.2.weight\", \"encoder.1.attention.11.linear_layers.2.bias\", \"encoder.1.attention.11.output_linear.weight\", \"encoder.1.attention.11.output_linear.bias\", \"encoder.1.position_feed_forward.0.0.weight\", \"encoder.1.position_feed_forward.0.0.bias\", \"encoder.1.position_feed_forward.0.3.weight\", \"encoder.1.position_feed_forward.0.3.bias\", \"encoder.1.position_feed_forward.1.0.weight\", \"encoder.1.position_feed_forward.1.0.bias\", \"encoder.1.position_feed_forward.1.3.weight\", \"encoder.1.position_feed_forward.1.3.bias\", \"encoder.1.position_feed_forward.2.0.weight\", \"encoder.1.position_feed_forward.2.0.bias\", \"encoder.1.position_feed_forward.2.3.weight\", \"encoder.1.position_feed_forward.2.3.bias\", \"encoder.1.position_feed_forward.3.0.weight\", \"encoder.1.position_feed_forward.3.0.bias\", \"encoder.1.position_feed_forward.3.3.weight\", \"encoder.1.position_feed_forward.3.3.bias\", \"encoder.1.position_feed_forward.4.0.weight\", \"encoder.1.position_feed_forward.4.0.bias\", \"encoder.1.position_feed_forward.4.3.weight\", \"encoder.1.position_feed_forward.4.3.bias\", \"encoder.1.position_feed_forward.5.0.weight\", \"encoder.1.position_feed_forward.5.0.bias\", \"encoder.1.position_feed_forward.5.3.weight\", \"encoder.1.position_feed_forward.5.3.bias\", \"encoder.1.position_feed_forward.6.0.weight\", \"encoder.1.position_feed_forward.6.0.bias\", \"encoder.1.position_feed_forward.6.3.weight\", \"encoder.1.position_feed_forward.6.3.bias\", \"encoder.1.position_feed_forward.7.0.weight\", \"encoder.1.position_feed_forward.7.0.bias\", \"encoder.1.position_feed_forward.7.3.weight\", \"encoder.1.position_feed_forward.7.3.bias\", \"encoder.1.position_feed_forward.8.0.weight\", \"encoder.1.position_feed_forward.8.0.bias\", \"encoder.1.position_feed_forward.8.3.weight\", \"encoder.1.position_feed_forward.8.3.bias\", \"encoder.1.position_feed_forward.9.0.weight\", \"encoder.1.position_feed_forward.9.0.bias\", \"encoder.1.position_feed_forward.9.3.weight\", \"encoder.1.position_feed_forward.9.3.bias\", \"encoder.1.position_feed_forward.10.0.weight\", \"encoder.1.position_feed_forward.10.0.bias\", \"encoder.1.position_feed_forward.10.3.weight\", \"encoder.1.position_feed_forward.10.3.bias\", \"encoder.1.position_feed_forward.11.0.weight\", \"encoder.1.position_feed_forward.11.0.bias\", \"encoder.1.position_feed_forward.11.3.weight\", \"encoder.1.position_feed_forward.11.3.bias\".\n\t\n\t\n\t \n\tUnexpected key(s) in state_dict: \"encoder.pos_embed\", \"encoder.patch_embed.proj.weight\", \"encoder.patch_embed.proj.bias\", \"encoder.blocks.0.norm1.weight\", \"encoder.blocks.0.norm1.bias\", \"encoder.blocks.0.attn.qkv.weight\", \"encoder.blocks.0.attn.qkv.bias\", \"encoder.blocks.0.attn.proj.weight\", \"encoder.blocks.0.attn.proj.bias\", \"encoder.blocks.0.norm2.weight\", \"encoder.blocks.0.norm2.bias\", \"encoder.blocks.0.mlp.fc1.weight\", \"encoder.blocks.0.mlp.fc1.bias\", \"encoder.blocks.0.mlp.fc2.weight\", \"encoder.blocks.0.mlp.fc2.bias\", \"encoder.blocks.1.norm1.weight\", \"encoder.blocks.1.norm1.bias\", \"encoder.blocks.1.attn.qkv.weight\", \"encoder.blocks.1.attn.qkv.bias\", \"encoder.blocks.1.attn.proj.weight\", \"encoder.blocks.1.attn.proj.bias\", \"encoder.blocks.1.norm2.weight\", \"encoder.blocks.1.norm2.bias\", \"encoder.blocks.1.mlp.fc1.weight\", \"encoder.blocks.1.mlp.fc1.bias\", \"encoder.blocks.1.mlp.fc2.weight\", \"encoder.blocks.1.mlp.fc2.bias\", \"encoder.blocks.2.norm1.weight\", \"encoder.blocks.2.norm1.bias\", \"encoder.blocks.2.attn.qkv.weight\", \"encoder.blocks.2.attn.qkv.bias\", \"encoder.blocks.2.attn.proj.weight\", \"encoder.blocks.2.attn.proj.bias\", \"encoder.blocks.2.norm2.weight\", \"encoder.blocks.2.norm2.bias\", \"encoder.blocks.2.mlp.fc1.weight\", \"encoder.blocks.2.mlp.fc1.bias\", \"encoder.blocks.2.mlp.fc2.weight\", \"encoder.blocks.2.mlp.fc2.bias\", \"encoder.blocks.3.norm1.weight\", \"encoder.blocks.3.norm1.bias\", \"encoder.blocks.3.attn.qkv.weight\", \"encoder.blocks.3.attn.qkv.bias\", \"encoder.blocks.3.attn.proj.weight\", \"encoder.blocks.3.attn.proj.bias\", \"encoder.blocks.3.norm2.weight\", \"encoder.blocks.3.norm2.bias\", \"encoder.blocks.3.mlp.fc1.weight\", \"encoder.blocks.3.mlp.fc1.bias\", \"encoder.blocks.3.mlp.fc2.weight\", \"encoder.blocks.3.mlp.fc2.bias\", \"encoder.blocks.4.norm1.weight\", \"encoder.blocks.4.norm1.bias\", \"encoder.blocks.4.attn.qkv.weight\", \"encoder.blocks.4.attn.qkv.bias\", \"encoder.blocks.4.attn.proj.weight\", \"encoder.blocks.4.attn.proj.bias\", \"encoder.blocks.4.norm2.weight\", \"encoder.blocks.4.norm2.bias\", \"encoder.blocks.4.mlp.fc1.weight\", \"encoder.blocks.4.mlp.fc1.bias\", \"encoder.blocks.4.mlp.fc2.weight\", \"encoder.blocks.4.mlp.fc2.bias\", \"encoder.blocks.5.norm1.weight\", \"encoder.blocks.5.norm1.bias\", \"encoder.blocks.5.attn.qkv.weight\", \"encoder.blocks.5.attn.qkv.bias\", \"encoder.blocks.5.attn.proj.weight\", \"encoder.blocks.5.attn.proj.bias\", \"encoder.blocks.5.norm2.weight\", \"encoder.blocks.5.norm2.bias\", \"encoder.blocks.5.mlp.fc1.weight\", \"encoder.blocks.5.mlp.fc1.bias\", \"encoder.blocks.5.mlp.fc2.weight\", \"encoder.blocks.5.mlp.fc2.bias\", \"encoder.blocks.6.norm1.weight\", \"encoder.blocks.6.norm1.bias\", \"encoder.blocks.6.attn.qkv.weight\", \"encoder.blocks.6.attn.qkv.bias\", \"encoder.blocks.6.attn.proj.weight\", \"encoder.blocks.6.attn.proj.bias\", \"encoder.blocks.6.norm2.weight\", \"encoder.blocks.6.norm2.bias\", \"encoder.blocks.6.mlp.fc1.weight\", \"encoder.blocks.6.mlp.fc1.bias\", \"encoder.blocks.6.mlp.fc2.weight\", \"encoder.blocks.6.mlp.fc2.bias\", \"encoder.blocks.7.norm1.weight\", \"encoder.blocks.7.norm1.bias\", \"encoder.blocks.7.attn.qkv.weight\", \"encoder.blocks.7.attn.qkv.bias\", \"encoder.blocks.7.attn.proj.weight\", \"encoder.blocks.7.attn.proj.bias\", \"encoder.blocks.7.norm2.weight\", \"encoder.blocks.7.norm2.bias\", \"encoder.blocks.7.mlp.fc1.weight\", \"encoder.blocks.7.mlp.fc1.bias\", \"encoder.blocks.7.mlp.fc2.weight\", \"encoder.blocks.7.mlp.fc2.bias\", \"encoder.blocks.8.norm1.weight\", \"encoder.blocks.8.norm1.bias\", \"encoder.blocks.8.attn.qkv.weight\", \"encoder.blocks.8.attn.qkv.bias\", \"encoder.blocks.8.attn.proj.weight\", \"encoder.blocks.8.attn.proj.bias\", \"encoder.blocks.8.norm2.weight\", \"encoder.blocks.8.norm2.bias\", \"encoder.blocks.8.mlp.fc1.weight\", \"encoder.blocks.8.mlp.fc1.bias\", \"encoder.blocks.8.mlp.fc2.weight\", \"encoder.blocks.8.mlp.fc2.bias\", \"encoder.blocks.9.norm1.weight\", \"encoder.blocks.9.norm1.bias\", \"encoder.blocks.9.attn.qkv.weight\", \"encoder.blocks.9.attn.qkv.bias\", \"encoder.blocks.9.attn.proj.weight\", \"encoder.blocks.9.attn.proj.bias\", \"encoder.blocks.9.norm2.weight\", \"encoder.blocks.9.norm2.bias\", \"encoder.blocks.9.mlp.fc1.weight\", \"encoder.blocks.9.mlp.fc1.bias\", \"encoder.blocks.9.mlp.fc2.weight\", \"encoder.blocks.9.mlp.fc2.bias\", \"encoder.blocks.10.norm1.weight\", \"encoder.blocks.10.norm1.bias\", \"encoder.blocks.10.attn.qkv.weight\", \"encoder.blocks.10.attn.qkv.bias\", \"encoder.blocks.10.attn.proj.weight\", \"encoder.blocks.10.attn.proj.bias\", \"encoder.blocks.10.norm2.weight\", \"encoder.blocks.10.norm2.bias\", \"encoder.blocks.10.mlp.fc1.weight\", \"encoder.blocks.10.mlp.fc1.bias\", \"encoder.blocks.10.mlp.fc2.weight\", \"encoder.blocks.10.mlp.fc2.bias\", \"encoder.blocks.11.norm1.weight\", \"encoder.blocks.11.norm1.bias\", \"encoder.blocks.11.attn.qkv.weight\", \"encoder.blocks.11.attn.qkv.bias\", \"encoder.blocks.11.attn.proj.weight\", \"encoder.blocks.11.attn.proj.bias\", \"encoder.blocks.11.norm2.weight\", \"encoder.blocks.11.norm2.bias\", \"encoder.blocks.11.mlp.fc1.weight\", \"encoder.blocks.11.mlp.fc1.bias\", \"encoder.blocks.11.mlp.fc2.weight\", \"encoder.blocks.11.mlp.fc2.bias\", \"encoder.norm.weight\", \"encoder.norm.bias\".\n\t\n\t\n\t\n\ndo you know how to do ? This is due to the difference implementation and names of VitSTR Encoder\n\nFor debugging you can run:\npython references/recognition/train_pytorch.py parseq\nOr convert the actual state_dict but i would suggest the above way :)\nIf the implementation is done we can do a dummy run to verify that all works well",
                                    "url": "https://github.com/mindee/doctr/pull/1205#issuecomment-1569890955",
                                    "author": {
                                        "login": "felixT2K"
                                    }
                                },
                                {
                                    "createdAt": "2023-05-31T11:19:23Z",
                                    "bodyText": "@nikokks You can update your PR with this:\nmain...felixdittrich92:doctr:parseq-torch\nIt contains all you need to implement only the whole decoding should be left from https://github.com/baudm/parseq/blob/main/strhub/models/parseq/system.py :)\nAnother ref for the mask generation: \n  \n    \n      doctr/doctr/models/recognition/master/pytorch.py\n    \n    \n         Line 102\n      in\n      1bf12a3\n    \n  \n  \n    \n\n        \n          \n           def make_source_and_target_mask(",
                                    "url": "https://github.com/mindee/doctr/pull/1205#issuecomment-1570001898",
                                    "author": {
                                        "login": "felixdittrich92"
                                    }
                                },
                                {
                                    "createdAt": "2023-05-31T13:28:07Z",
                                    "bodyText": "Hi to test the temporary implementation I try to load the model parseq, but the keys of state_dict are not the same now.\nthis is what I get\n\tMissing key(s) in state_dict: \"encoder.0.cls_token\", \"encoder.0.positions\", \"encoder.0.proj.weight\", \"encoder.0.proj.bias\", \"encoder.1.layer_norm_input.weight\", \"encoder.1.layer_norm_input.bias\", \"encoder.1.layer_norm_attention.weight\", \"encoder.1.layer_norm_attention.bias\", \"encoder.1.layer_norm_output.weight\", \"encoder.1.layer_norm_output.bias\", \"encoder.1.attention.0.linear_layers.0.weight\", \"encoder.1.attention.0.linear_layers.0.bias\", \"encoder.1.attention.0.linear_layers.1.weight\", \"encoder.1.attention.0.linear_layers.1.bias\", \"encoder.1.attention.0.linear_layers.2.weight\", \"encoder.1.attention.0.linear_layers.2.bias\", \"encoder.1.attention.0.output_linear.weight\", \"encoder.1.attention.0.output_linear.bias\", \"encoder.1.attention.1.linear_layers.0.weight\", \"encoder.1.attention.1.linear_layers.0.bias\", \"encoder.1.attention.1.linear_layers.1.weight\", \"encoder.1.attention.1.linear_layers.1.bias\", \"encoder.1.attention.1.linear_layers.2.weight\", \"encoder.1.attention.1.linear_layers.2.bias\", \"encoder.1.attention.1.output_linear.weight\", \"encoder.1.attention.1.output_linear.bias\", \"encoder.1.attention.2.linear_layers.0.weight\", \"encoder.1.attention.2.linear_layers.0.bias\", \"encoder.1.attention.2.linear_layers.1.weight\", \"encoder.1.attention.2.linear_layers.1.bias\", \"encoder.1.attention.2.linear_layers.2.weight\", \"encoder.1.attention.2.linear_layers.2.bias\", \"encoder.1.attention.2.output_linear.weight\", \"encoder.1.attention.2.output_linear.bias\", \"encoder.1.attention.3.linear_layers.0.weight\", \"encoder.1.attention.3.linear_layers.0.bias\", \"encoder.1.attention.3.linear_layers.1.weight\", \"encoder.1.attention.3.linear_layers.1.bias\", \"encoder.1.attention.3.linear_layers.2.weight\", \"encoder.1.attention.3.linear_layers.2.bias\", \"encoder.1.attention.3.output_linear.weight\", \"encoder.1.attention.3.output_linear.bias\", \"encoder.1.attention.4.linear_layers.0.weight\", \"encoder.1.attention.4.linear_layers.0.bias\", \"encoder.1.attention.4.linear_layers.1.weight\", \"encoder.1.attention.4.linear_layers.1.bias\", \"encoder.1.attention.4.linear_layers.2.weight\", \"encoder.1.attention.4.linear_layers.2.bias\", \"encoder.1.attention.4.output_linear.weight\", \"encoder.1.attention.4.output_linear.bias\", \"encoder.1.attention.5.linear_layers.0.weight\", \"encoder.1.attention.5.linear_layers.0.bias\", \"encoder.1.attention.5.linear_layers.1.weight\", \"encoder.1.attention.5.linear_layers.1.bias\", \"encoder.1.attention.5.linear_layers.2.weight\", \"encoder.1.attention.5.linear_layers.2.bias\", \"encoder.1.attention.5.output_linear.weight\", \"encoder.1.attention.5.output_linear.bias\", \"encoder.1.attention.6.linear_layers.0.weight\", \"encoder.1.attention.6.linear_layers.0.bias\", \"encoder.1.attention.6.linear_layers.1.weight\", \"encoder.1.attention.6.linear_layers.1.bias\", \"encoder.1.attention.6.linear_layers.2.weight\", \"encoder.1.attention.6.linear_layers.2.bias\", \"encoder.1.attention.6.output_linear.weight\", \"encoder.1.attention.6.output_linear.bias\", \"encoder.1.attention.7.linear_layers.0.weight\", \"encoder.1.attention.7.linear_layers.0.bias\", \"encoder.1.attention.7.linear_layers.1.weight\", \"encoder.1.attention.7.linear_layers.1.bias\", \"encoder.1.attention.7.linear_layers.2.weight\", \"encoder.1.attention.7.linear_layers.2.bias\", \"encoder.1.attention.7.output_linear.weight\", \"encoder.1.attention.7.output_linear.bias\", \"encoder.1.attention.8.linear_layers.0.weight\", \"encoder.1.attention.8.linear_layers.0.bias\", \"encoder.1.attention.8.linear_layers.1.weight\", \"encoder.1.attention.8.linear_layers.1.bias\", \"encoder.1.attention.8.linear_layers.2.weight\", \"encoder.1.attention.8.linear_layers.2.bias\", \"encoder.1.attention.8.output_linear.weight\", \"encoder.1.attention.8.output_linear.bias\", \"encoder.1.attention.9.linear_layers.0.weight\", \"encoder.1.attention.9.linear_layers.0.bias\", \"encoder.1.attention.9.linear_layers.1.weight\", \"encoder.1.attention.9.linear_layers.1.bias\", \"encoder.1.attention.9.linear_layers.2.weight\", \"encoder.1.attention.9.linear_layers.2.bias\", \"encoder.1.attention.9.output_linear.weight\", \"encoder.1.attention.9.output_linear.bias\", \"encoder.1.attention.10.linear_layers.0.weight\", \"encoder.1.attention.10.linear_layers.0.bias\", \"encoder.1.attention.10.linear_layers.1.weight\", \"encoder.1.attention.10.linear_layers.1.bias\", \"encoder.1.attention.10.linear_layers.2.weight\", \"encoder.1.attention.10.linear_layers.2.bias\", \"encoder.1.attention.10.output_linear.weight\", \"encoder.1.attention.10.output_linear.bias\", \"encoder.1.attention.11.linear_layers.0.weight\", \"encoder.1.attention.11.linear_layers.0.bias\", \"encoder.1.attention.11.linear_layers.1.weight\", \"encoder.1.attention.11.linear_layers.1.bias\", \"encoder.1.attention.11.linear_layers.2.weight\", \"encoder.1.attention.11.linear_layers.2.bias\", \"encoder.1.attention.11.output_linear.weight\", \"encoder.1.attention.11.output_linear.bias\", \"encoder.1.position_feed_forward.0.0.weight\", \"encoder.1.position_feed_forward.0.0.bias\", \"encoder.1.position_feed_forward.0.3.weight\", \"encoder.1.position_feed_forward.0.3.bias\", \"encoder.1.position_feed_forward.1.0.weight\", \"encoder.1.position_feed_forward.1.0.bias\", \"encoder.1.position_feed_forward.1.3.weight\", \"encoder.1.position_feed_forward.1.3.bias\", \"encoder.1.position_feed_forward.2.0.weight\", \"encoder.1.position_feed_forward.2.0.bias\", \"encoder.1.position_feed_forward.2.3.weight\", \"encoder.1.position_feed_forward.2.3.bias\", \"encoder.1.position_feed_forward.3.0.weight\", \"encoder.1.position_feed_forward.3.0.bias\", \"encoder.1.position_feed_forward.3.3.weight\", \"encoder.1.position_feed_forward.3.3.bias\", \"encoder.1.position_feed_forward.4.0.weight\", \"encoder.1.position_feed_forward.4.0.bias\", \"encoder.1.position_feed_forward.4.3.weight\", \"encoder.1.position_feed_forward.4.3.bias\", \"encoder.1.position_feed_forward.5.0.weight\", \"encoder.1.position_feed_forward.5.0.bias\", \"encoder.1.position_feed_forward.5.3.weight\", \"encoder.1.position_feed_forward.5.3.bias\", \"encoder.1.position_feed_forward.6.0.weight\", \"encoder.1.position_feed_forward.6.0.bias\", \"encoder.1.position_feed_forward.6.3.weight\", \"encoder.1.position_feed_forward.6.3.bias\", \"encoder.1.position_feed_forward.7.0.weight\", \"encoder.1.position_feed_forward.7.0.bias\", \"encoder.1.position_feed_forward.7.3.weight\", \"encoder.1.position_feed_forward.7.3.bias\", \"encoder.1.position_feed_forward.8.0.weight\", \"encoder.1.position_feed_forward.8.0.bias\", \"encoder.1.position_feed_forward.8.3.weight\", \"encoder.1.position_feed_forward.8.3.bias\", \"encoder.1.position_feed_forward.9.0.weight\", \"encoder.1.position_feed_forward.9.0.bias\", \"encoder.1.position_feed_forward.9.3.weight\", \"encoder.1.position_feed_forward.9.3.bias\", \"encoder.1.position_feed_forward.10.0.weight\", \"encoder.1.position_feed_forward.10.0.bias\", \"encoder.1.position_feed_forward.10.3.weight\", \"encoder.1.position_feed_forward.10.3.bias\", \"encoder.1.position_feed_forward.11.0.weight\", \"encoder.1.position_feed_forward.11.0.bias\", \"encoder.1.position_feed_forward.11.3.weight\", \"encoder.1.position_feed_forward.11.3.bias\".\n\t\n\t\n\t \n\tUnexpected key(s) in state_dict: \"encoder.pos_embed\", \"encoder.patch_embed.proj.weight\", \"encoder.patch_embed.proj.bias\", \"encoder.blocks.0.norm1.weight\", \"encoder.blocks.0.norm1.bias\", \"encoder.blocks.0.attn.qkv.weight\", \"encoder.blocks.0.attn.qkv.bias\", \"encoder.blocks.0.attn.proj.weight\", \"encoder.blocks.0.attn.proj.bias\", \"encoder.blocks.0.norm2.weight\", \"encoder.blocks.0.norm2.bias\", \"encoder.blocks.0.mlp.fc1.weight\", \"encoder.blocks.0.mlp.fc1.bias\", \"encoder.blocks.0.mlp.fc2.weight\", \"encoder.blocks.0.mlp.fc2.bias\", \"encoder.blocks.1.norm1.weight\", \"encoder.blocks.1.norm1.bias\", \"encoder.blocks.1.attn.qkv.weight\", \"encoder.blocks.1.attn.qkv.bias\", \"encoder.blocks.1.attn.proj.weight\", \"encoder.blocks.1.attn.proj.bias\", \"encoder.blocks.1.norm2.weight\", \"encoder.blocks.1.norm2.bias\", \"encoder.blocks.1.mlp.fc1.weight\", \"encoder.blocks.1.mlp.fc1.bias\", \"encoder.blocks.1.mlp.fc2.weight\", \"encoder.blocks.1.mlp.fc2.bias\", \"encoder.blocks.2.norm1.weight\", \"encoder.blocks.2.norm1.bias\", \"encoder.blocks.2.attn.qkv.weight\", \"encoder.blocks.2.attn.qkv.bias\", \"encoder.blocks.2.attn.proj.weight\", \"encoder.blocks.2.attn.proj.bias\", \"encoder.blocks.2.norm2.weight\", \"encoder.blocks.2.norm2.bias\", \"encoder.blocks.2.mlp.fc1.weight\", \"encoder.blocks.2.mlp.fc1.bias\", \"encoder.blocks.2.mlp.fc2.weight\", \"encoder.blocks.2.mlp.fc2.bias\", \"encoder.blocks.3.norm1.weight\", \"encoder.blocks.3.norm1.bias\", \"encoder.blocks.3.attn.qkv.weight\", \"encoder.blocks.3.attn.qkv.bias\", \"encoder.blocks.3.attn.proj.weight\", \"encoder.blocks.3.attn.proj.bias\", \"encoder.blocks.3.norm2.weight\", \"encoder.blocks.3.norm2.bias\", \"encoder.blocks.3.mlp.fc1.weight\", \"encoder.blocks.3.mlp.fc1.bias\", \"encoder.blocks.3.mlp.fc2.weight\", \"encoder.blocks.3.mlp.fc2.bias\", \"encoder.blocks.4.norm1.weight\", \"encoder.blocks.4.norm1.bias\", \"encoder.blocks.4.attn.qkv.weight\", \"encoder.blocks.4.attn.qkv.bias\", \"encoder.blocks.4.attn.proj.weight\", \"encoder.blocks.4.attn.proj.bias\", \"encoder.blocks.4.norm2.weight\", \"encoder.blocks.4.norm2.bias\", \"encoder.blocks.4.mlp.fc1.weight\", \"encoder.blocks.4.mlp.fc1.bias\", \"encoder.blocks.4.mlp.fc2.weight\", \"encoder.blocks.4.mlp.fc2.bias\", \"encoder.blocks.5.norm1.weight\", \"encoder.blocks.5.norm1.bias\", \"encoder.blocks.5.attn.qkv.weight\", \"encoder.blocks.5.attn.qkv.bias\", \"encoder.blocks.5.attn.proj.weight\", \"encoder.blocks.5.attn.proj.bias\", \"encoder.blocks.5.norm2.weight\", \"encoder.blocks.5.norm2.bias\", \"encoder.blocks.5.mlp.fc1.weight\", \"encoder.blocks.5.mlp.fc1.bias\", \"encoder.blocks.5.mlp.fc2.weight\", \"encoder.blocks.5.mlp.fc2.bias\", \"encoder.blocks.6.norm1.weight\", \"encoder.blocks.6.norm1.bias\", \"encoder.blocks.6.attn.qkv.weight\", \"encoder.blocks.6.attn.qkv.bias\", \"encoder.blocks.6.attn.proj.weight\", \"encoder.blocks.6.attn.proj.bias\", \"encoder.blocks.6.norm2.weight\", \"encoder.blocks.6.norm2.bias\", \"encoder.blocks.6.mlp.fc1.weight\", \"encoder.blocks.6.mlp.fc1.bias\", \"encoder.blocks.6.mlp.fc2.weight\", \"encoder.blocks.6.mlp.fc2.bias\", \"encoder.blocks.7.norm1.weight\", \"encoder.blocks.7.norm1.bias\", \"encoder.blocks.7.attn.qkv.weight\", \"encoder.blocks.7.attn.qkv.bias\", \"encoder.blocks.7.attn.proj.weight\", \"encoder.blocks.7.attn.proj.bias\", \"encoder.blocks.7.norm2.weight\", \"encoder.blocks.7.norm2.bias\", \"encoder.blocks.7.mlp.fc1.weight\", \"encoder.blocks.7.mlp.fc1.bias\", \"encoder.blocks.7.mlp.fc2.weight\", \"encoder.blocks.7.mlp.fc2.bias\", \"encoder.blocks.8.norm1.weight\", \"encoder.blocks.8.norm1.bias\", \"encoder.blocks.8.attn.qkv.weight\", \"encoder.blocks.8.attn.qkv.bias\", \"encoder.blocks.8.attn.proj.weight\", \"encoder.blocks.8.attn.proj.bias\", \"encoder.blocks.8.norm2.weight\", \"encoder.blocks.8.norm2.bias\", \"encoder.blocks.8.mlp.fc1.weight\", \"encoder.blocks.8.mlp.fc1.bias\", \"encoder.blocks.8.mlp.fc2.weight\", \"encoder.blocks.8.mlp.fc2.bias\", \"encoder.blocks.9.norm1.weight\", \"encoder.blocks.9.norm1.bias\", \"encoder.blocks.9.attn.qkv.weight\", \"encoder.blocks.9.attn.qkv.bias\", \"encoder.blocks.9.attn.proj.weight\", \"encoder.blocks.9.attn.proj.bias\", \"encoder.blocks.9.norm2.weight\", \"encoder.blocks.9.norm2.bias\", \"encoder.blocks.9.mlp.fc1.weight\", \"encoder.blocks.9.mlp.fc1.bias\", \"encoder.blocks.9.mlp.fc2.weight\", \"encoder.blocks.9.mlp.fc2.bias\", \"encoder.blocks.10.norm1.weight\", \"encoder.blocks.10.norm1.bias\", \"encoder.blocks.10.attn.qkv.weight\", \"encoder.blocks.10.attn.qkv.bias\", \"encoder.blocks.10.attn.proj.weight\", \"encoder.blocks.10.attn.proj.bias\", \"encoder.blocks.10.norm2.weight\", \"encoder.blocks.10.norm2.bias\", \"encoder.blocks.10.mlp.fc1.weight\", \"encoder.blocks.10.mlp.fc1.bias\", \"encoder.blocks.10.mlp.fc2.weight\", \"encoder.blocks.10.mlp.fc2.bias\", \"encoder.blocks.11.norm1.weight\", \"encoder.blocks.11.norm1.bias\", \"encoder.blocks.11.attn.qkv.weight\", \"encoder.blocks.11.attn.qkv.bias\", \"encoder.blocks.11.attn.proj.weight\", \"encoder.blocks.11.attn.proj.bias\", \"encoder.blocks.11.norm2.weight\", \"encoder.blocks.11.norm2.bias\", \"encoder.blocks.11.mlp.fc1.weight\", \"encoder.blocks.11.mlp.fc1.bias\", \"encoder.blocks.11.mlp.fc2.weight\", \"encoder.blocks.11.mlp.fc2.bias\", \"encoder.norm.weight\", \"encoder.norm.bias\".\n\t\n\t\n\t\n\ndo you know how to do ? This is due to the difference implementation and names of VitSTR Encoder\n\nFor debugging you can run: python references/recognition/train_pytorch.py parseq Or convert the actual state_dict but i would suggest the above way :) If the implementation is done we can do a dummy run to verify that all works well\n\nDo you think retraining the model should be better ?",
                                    "url": "https://github.com/mindee/doctr/pull/1205#issuecomment-1570236810",
                                    "author": {
                                        "login": "nikokks"
                                    }
                                },
                                {
                                    "createdAt": "2023-05-31T13:31:53Z",
                                    "bodyText": "Hi to test the temporary implementation I try to load the model parseq, but the keys of state_dict are not the same now.\nthis is what I get\n\tMissing key(s) in state_dict: \"encoder.0.cls_token\", \"encoder.0.positions\", \"encoder.0.proj.weight\", \"encoder.0.proj.bias\", \"encoder.1.layer_norm_input.weight\", \"encoder.1.layer_norm_input.bias\", \"encoder.1.layer_norm_attention.weight\", \"encoder.1.layer_norm_attention.bias\", \"encoder.1.layer_norm_output.weight\", \"encoder.1.layer_norm_output.bias\", \"encoder.1.attention.0.linear_layers.0.weight\", \"encoder.1.attention.0.linear_layers.0.bias\", \"encoder.1.attention.0.linear_layers.1.weight\", \"encoder.1.attention.0.linear_layers.1.bias\", \"encoder.1.attention.0.linear_layers.2.weight\", \"encoder.1.attention.0.linear_layers.2.bias\", \"encoder.1.attention.0.output_linear.weight\", \"encoder.1.attention.0.output_linear.bias\", \"encoder.1.attention.1.linear_layers.0.weight\", \"encoder.1.attention.1.linear_layers.0.bias\", \"encoder.1.attention.1.linear_layers.1.weight\", \"encoder.1.attention.1.linear_layers.1.bias\", \"encoder.1.attention.1.linear_layers.2.weight\", \"encoder.1.attention.1.linear_layers.2.bias\", \"encoder.1.attention.1.output_linear.weight\", \"encoder.1.attention.1.output_linear.bias\", \"encoder.1.attention.2.linear_layers.0.weight\", \"encoder.1.attention.2.linear_layers.0.bias\", \"encoder.1.attention.2.linear_layers.1.weight\", \"encoder.1.attention.2.linear_layers.1.bias\", \"encoder.1.attention.2.linear_layers.2.weight\", \"encoder.1.attention.2.linear_layers.2.bias\", \"encoder.1.attention.2.output_linear.weight\", \"encoder.1.attention.2.output_linear.bias\", \"encoder.1.attention.3.linear_layers.0.weight\", \"encoder.1.attention.3.linear_layers.0.bias\", \"encoder.1.attention.3.linear_layers.1.weight\", \"encoder.1.attention.3.linear_layers.1.bias\", \"encoder.1.attention.3.linear_layers.2.weight\", \"encoder.1.attention.3.linear_layers.2.bias\", \"encoder.1.attention.3.output_linear.weight\", \"encoder.1.attention.3.output_linear.bias\", \"encoder.1.attention.4.linear_layers.0.weight\", \"encoder.1.attention.4.linear_layers.0.bias\", \"encoder.1.attention.4.linear_layers.1.weight\", \"encoder.1.attention.4.linear_layers.1.bias\", \"encoder.1.attention.4.linear_layers.2.weight\", \"encoder.1.attention.4.linear_layers.2.bias\", \"encoder.1.attention.4.output_linear.weight\", \"encoder.1.attention.4.output_linear.bias\", \"encoder.1.attention.5.linear_layers.0.weight\", \"encoder.1.attention.5.linear_layers.0.bias\", \"encoder.1.attention.5.linear_layers.1.weight\", \"encoder.1.attention.5.linear_layers.1.bias\", \"encoder.1.attention.5.linear_layers.2.weight\", \"encoder.1.attention.5.linear_layers.2.bias\", \"encoder.1.attention.5.output_linear.weight\", \"encoder.1.attention.5.output_linear.bias\", \"encoder.1.attention.6.linear_layers.0.weight\", \"encoder.1.attention.6.linear_layers.0.bias\", \"encoder.1.attention.6.linear_layers.1.weight\", \"encoder.1.attention.6.linear_layers.1.bias\", \"encoder.1.attention.6.linear_layers.2.weight\", \"encoder.1.attention.6.linear_layers.2.bias\", \"encoder.1.attention.6.output_linear.weight\", \"encoder.1.attention.6.output_linear.bias\", \"encoder.1.attention.7.linear_layers.0.weight\", \"encoder.1.attention.7.linear_layers.0.bias\", \"encoder.1.attention.7.linear_layers.1.weight\", \"encoder.1.attention.7.linear_layers.1.bias\", \"encoder.1.attention.7.linear_layers.2.weight\", \"encoder.1.attention.7.linear_layers.2.bias\", \"encoder.1.attention.7.output_linear.weight\", \"encoder.1.attention.7.output_linear.bias\", \"encoder.1.attention.8.linear_layers.0.weight\", \"encoder.1.attention.8.linear_layers.0.bias\", \"encoder.1.attention.8.linear_layers.1.weight\", \"encoder.1.attention.8.linear_layers.1.bias\", \"encoder.1.attention.8.linear_layers.2.weight\", \"encoder.1.attention.8.linear_layers.2.bias\", \"encoder.1.attention.8.output_linear.weight\", \"encoder.1.attention.8.output_linear.bias\", \"encoder.1.attention.9.linear_layers.0.weight\", \"encoder.1.attention.9.linear_layers.0.bias\", \"encoder.1.attention.9.linear_layers.1.weight\", \"encoder.1.attention.9.linear_layers.1.bias\", \"encoder.1.attention.9.linear_layers.2.weight\", \"encoder.1.attention.9.linear_layers.2.bias\", \"encoder.1.attention.9.output_linear.weight\", \"encoder.1.attention.9.output_linear.bias\", \"encoder.1.attention.10.linear_layers.0.weight\", \"encoder.1.attention.10.linear_layers.0.bias\", \"encoder.1.attention.10.linear_layers.1.weight\", \"encoder.1.attention.10.linear_layers.1.bias\", \"encoder.1.attention.10.linear_layers.2.weight\", \"encoder.1.attention.10.linear_layers.2.bias\", \"encoder.1.attention.10.output_linear.weight\", \"encoder.1.attention.10.output_linear.bias\", \"encoder.1.attention.11.linear_layers.0.weight\", \"encoder.1.attention.11.linear_layers.0.bias\", \"encoder.1.attention.11.linear_layers.1.weight\", \"encoder.1.attention.11.linear_layers.1.bias\", \"encoder.1.attention.11.linear_layers.2.weight\", \"encoder.1.attention.11.linear_layers.2.bias\", \"encoder.1.attention.11.output_linear.weight\", \"encoder.1.attention.11.output_linear.bias\", \"encoder.1.position_feed_forward.0.0.weight\", \"encoder.1.position_feed_forward.0.0.bias\", \"encoder.1.position_feed_forward.0.3.weight\", \"encoder.1.position_feed_forward.0.3.bias\", \"encoder.1.position_feed_forward.1.0.weight\", \"encoder.1.position_feed_forward.1.0.bias\", \"encoder.1.position_feed_forward.1.3.weight\", \"encoder.1.position_feed_forward.1.3.bias\", \"encoder.1.position_feed_forward.2.0.weight\", \"encoder.1.position_feed_forward.2.0.bias\", \"encoder.1.position_feed_forward.2.3.weight\", \"encoder.1.position_feed_forward.2.3.bias\", \"encoder.1.position_feed_forward.3.0.weight\", \"encoder.1.position_feed_forward.3.0.bias\", \"encoder.1.position_feed_forward.3.3.weight\", \"encoder.1.position_feed_forward.3.3.bias\", \"encoder.1.position_feed_forward.4.0.weight\", \"encoder.1.position_feed_forward.4.0.bias\", \"encoder.1.position_feed_forward.4.3.weight\", \"encoder.1.position_feed_forward.4.3.bias\", \"encoder.1.position_feed_forward.5.0.weight\", \"encoder.1.position_feed_forward.5.0.bias\", \"encoder.1.position_feed_forward.5.3.weight\", \"encoder.1.position_feed_forward.5.3.bias\", \"encoder.1.position_feed_forward.6.0.weight\", \"encoder.1.position_feed_forward.6.0.bias\", \"encoder.1.position_feed_forward.6.3.weight\", \"encoder.1.position_feed_forward.6.3.bias\", \"encoder.1.position_feed_forward.7.0.weight\", \"encoder.1.position_feed_forward.7.0.bias\", \"encoder.1.position_feed_forward.7.3.weight\", \"encoder.1.position_feed_forward.7.3.bias\", \"encoder.1.position_feed_forward.8.0.weight\", \"encoder.1.position_feed_forward.8.0.bias\", \"encoder.1.position_feed_forward.8.3.weight\", \"encoder.1.position_feed_forward.8.3.bias\", \"encoder.1.position_feed_forward.9.0.weight\", \"encoder.1.position_feed_forward.9.0.bias\", \"encoder.1.position_feed_forward.9.3.weight\", \"encoder.1.position_feed_forward.9.3.bias\", \"encoder.1.position_feed_forward.10.0.weight\", \"encoder.1.position_feed_forward.10.0.bias\", \"encoder.1.position_feed_forward.10.3.weight\", \"encoder.1.position_feed_forward.10.3.bias\", \"encoder.1.position_feed_forward.11.0.weight\", \"encoder.1.position_feed_forward.11.0.bias\", \"encoder.1.position_feed_forward.11.3.weight\", \"encoder.1.position_feed_forward.11.3.bias\".\n\t\n\t\n\t \n\tUnexpected key(s) in state_dict: \"encoder.pos_embed\", \"encoder.patch_embed.proj.weight\", \"encoder.patch_embed.proj.bias\", \"encoder.blocks.0.norm1.weight\", \"encoder.blocks.0.norm1.bias\", \"encoder.blocks.0.attn.qkv.weight\", \"encoder.blocks.0.attn.qkv.bias\", \"encoder.blocks.0.attn.proj.weight\", \"encoder.blocks.0.attn.proj.bias\", \"encoder.blocks.0.norm2.weight\", \"encoder.blocks.0.norm2.bias\", \"encoder.blocks.0.mlp.fc1.weight\", \"encoder.blocks.0.mlp.fc1.bias\", \"encoder.blocks.0.mlp.fc2.weight\", \"encoder.blocks.0.mlp.fc2.bias\", \"encoder.blocks.1.norm1.weight\", \"encoder.blocks.1.norm1.bias\", \"encoder.blocks.1.attn.qkv.weight\", \"encoder.blocks.1.attn.qkv.bias\", \"encoder.blocks.1.attn.proj.weight\", \"encoder.blocks.1.attn.proj.bias\", \"encoder.blocks.1.norm2.weight\", \"encoder.blocks.1.norm2.bias\", \"encoder.blocks.1.mlp.fc1.weight\", \"encoder.blocks.1.mlp.fc1.bias\", \"encoder.blocks.1.mlp.fc2.weight\", \"encoder.blocks.1.mlp.fc2.bias\", \"encoder.blocks.2.norm1.weight\", \"encoder.blocks.2.norm1.bias\", \"encoder.blocks.2.attn.qkv.weight\", \"encoder.blocks.2.attn.qkv.bias\", \"encoder.blocks.2.attn.proj.weight\", \"encoder.blocks.2.attn.proj.bias\", \"encoder.blocks.2.norm2.weight\", \"encoder.blocks.2.norm2.bias\", \"encoder.blocks.2.mlp.fc1.weight\", \"encoder.blocks.2.mlp.fc1.bias\", \"encoder.blocks.2.mlp.fc2.weight\", \"encoder.blocks.2.mlp.fc2.bias\", \"encoder.blocks.3.norm1.weight\", \"encoder.blocks.3.norm1.bias\", \"encoder.blocks.3.attn.qkv.weight\", \"encoder.blocks.3.attn.qkv.bias\", \"encoder.blocks.3.attn.proj.weight\", \"encoder.blocks.3.attn.proj.bias\", \"encoder.blocks.3.norm2.weight\", \"encoder.blocks.3.norm2.bias\", \"encoder.blocks.3.mlp.fc1.weight\", \"encoder.blocks.3.mlp.fc1.bias\", \"encoder.blocks.3.mlp.fc2.weight\", \"encoder.blocks.3.mlp.fc2.bias\", \"encoder.blocks.4.norm1.weight\", \"encoder.blocks.4.norm1.bias\", \"encoder.blocks.4.attn.qkv.weight\", \"encoder.blocks.4.attn.qkv.bias\", \"encoder.blocks.4.attn.proj.weight\", \"encoder.blocks.4.attn.proj.bias\", \"encoder.blocks.4.norm2.weight\", \"encoder.blocks.4.norm2.bias\", \"encoder.blocks.4.mlp.fc1.weight\", \"encoder.blocks.4.mlp.fc1.bias\", \"encoder.blocks.4.mlp.fc2.weight\", \"encoder.blocks.4.mlp.fc2.bias\", \"encoder.blocks.5.norm1.weight\", \"encoder.blocks.5.norm1.bias\", \"encoder.blocks.5.attn.qkv.weight\", \"encoder.blocks.5.attn.qkv.bias\", \"encoder.blocks.5.attn.proj.weight\", \"encoder.blocks.5.attn.proj.bias\", \"encoder.blocks.5.norm2.weight\", \"encoder.blocks.5.norm2.bias\", \"encoder.blocks.5.mlp.fc1.weight\", \"encoder.blocks.5.mlp.fc1.bias\", \"encoder.blocks.5.mlp.fc2.weight\", \"encoder.blocks.5.mlp.fc2.bias\", \"encoder.blocks.6.norm1.weight\", \"encoder.blocks.6.norm1.bias\", \"encoder.blocks.6.attn.qkv.weight\", \"encoder.blocks.6.attn.qkv.bias\", \"encoder.blocks.6.attn.proj.weight\", \"encoder.blocks.6.attn.proj.bias\", \"encoder.blocks.6.norm2.weight\", \"encoder.blocks.6.norm2.bias\", \"encoder.blocks.6.mlp.fc1.weight\", \"encoder.blocks.6.mlp.fc1.bias\", \"encoder.blocks.6.mlp.fc2.weight\", \"encoder.blocks.6.mlp.fc2.bias\", \"encoder.blocks.7.norm1.weight\", \"encoder.blocks.7.norm1.bias\", \"encoder.blocks.7.attn.qkv.weight\", \"encoder.blocks.7.attn.qkv.bias\", \"encoder.blocks.7.attn.proj.weight\", \"encoder.blocks.7.attn.proj.bias\", \"encoder.blocks.7.norm2.weight\", \"encoder.blocks.7.norm2.bias\", \"encoder.blocks.7.mlp.fc1.weight\", \"encoder.blocks.7.mlp.fc1.bias\", \"encoder.blocks.7.mlp.fc2.weight\", \"encoder.blocks.7.mlp.fc2.bias\", \"encoder.blocks.8.norm1.weight\", \"encoder.blocks.8.norm1.bias\", \"encoder.blocks.8.attn.qkv.weight\", \"encoder.blocks.8.attn.qkv.bias\", \"encoder.blocks.8.attn.proj.weight\", \"encoder.blocks.8.attn.proj.bias\", \"encoder.blocks.8.norm2.weight\", \"encoder.blocks.8.norm2.bias\", \"encoder.blocks.8.mlp.fc1.weight\", \"encoder.blocks.8.mlp.fc1.bias\", \"encoder.blocks.8.mlp.fc2.weight\", \"encoder.blocks.8.mlp.fc2.bias\", \"encoder.blocks.9.norm1.weight\", \"encoder.blocks.9.norm1.bias\", \"encoder.blocks.9.attn.qkv.weight\", \"encoder.blocks.9.attn.qkv.bias\", \"encoder.blocks.9.attn.proj.weight\", \"encoder.blocks.9.attn.proj.bias\", \"encoder.blocks.9.norm2.weight\", \"encoder.blocks.9.norm2.bias\", \"encoder.blocks.9.mlp.fc1.weight\", \"encoder.blocks.9.mlp.fc1.bias\", \"encoder.blocks.9.mlp.fc2.weight\", \"encoder.blocks.9.mlp.fc2.bias\", \"encoder.blocks.10.norm1.weight\", \"encoder.blocks.10.norm1.bias\", \"encoder.blocks.10.attn.qkv.weight\", \"encoder.blocks.10.attn.qkv.bias\", \"encoder.blocks.10.attn.proj.weight\", \"encoder.blocks.10.attn.proj.bias\", \"encoder.blocks.10.norm2.weight\", \"encoder.blocks.10.norm2.bias\", \"encoder.blocks.10.mlp.fc1.weight\", \"encoder.blocks.10.mlp.fc1.bias\", \"encoder.blocks.10.mlp.fc2.weight\", \"encoder.blocks.10.mlp.fc2.bias\", \"encoder.blocks.11.norm1.weight\", \"encoder.blocks.11.norm1.bias\", \"encoder.blocks.11.attn.qkv.weight\", \"encoder.blocks.11.attn.qkv.bias\", \"encoder.blocks.11.attn.proj.weight\", \"encoder.blocks.11.attn.proj.bias\", \"encoder.blocks.11.norm2.weight\", \"encoder.blocks.11.norm2.bias\", \"encoder.blocks.11.mlp.fc1.weight\", \"encoder.blocks.11.mlp.fc1.bias\", \"encoder.blocks.11.mlp.fc2.weight\", \"encoder.blocks.11.mlp.fc2.bias\", \"encoder.norm.weight\", \"encoder.norm.bias\".\n\t\n\t\n\t\n\ndo you know how to do ? This is due to the difference implementation and names of VitSTR Encoder\n\nFor debugging you can run: python references/recognition/train_pytorch.py parseq Or convert the actual state_dict but i would suggest the above way :) If the implementation is done we can do a dummy run to verify that all works well\n\nDo you think retraining the model should be better ?\n\nYep it will be trained on mindee's internal dataset which contains real data, so at the end we only have to verify that the model works well (i will do a toy run for verification after the implementation is done)",
                                    "url": "https://github.com/mindee/doctr/pull/1205#issuecomment-1570244039",
                                    "author": {
                                        "login": "felixdittrich92"
                                    }
                                },
                                {
                                    "createdAt": "2023-05-31T13:45:15Z",
                                    "bodyText": "do I delete the parseq folder in classification ?\nNow recognition/parseq works without it",
                                    "url": "https://github.com/mindee/doctr/pull/1205#issuecomment-1570267534",
                                    "author": {
                                        "login": "nikokks"
                                    }
                                },
                                {
                                    "createdAt": "2023-05-31T13:46:23Z",
                                    "bodyText": "do I delete the parseq folder in classification ? Now recognition/parseq works without it\n\nYes look into my branch: main...felixdittrich92:doctr:parseq-torch  that's the only files you need to change :)",
                                    "url": "https://github.com/mindee/doctr/pull/1205#issuecomment-1570270598",
                                    "author": {
                                        "login": "felixdittrich92"
                                    }
                                },
                                {
                                    "createdAt": "2023-05-31T13:55:10Z",
                                    "bodyText": "Do I delete the TokenEmbedding in parseq/pytorch.py ?",
                                    "url": "https://github.com/mindee/doctr/pull/1205#issuecomment-1570286645",
                                    "author": {
                                        "login": "nikokks"
                                    }
                                },
                                {
                                    "createdAt": "2023-05-31T13:57:38Z",
                                    "bodyText": "Do I delete the TokenEmbedding in parseq/pytorch.py ?\n\nWe need this in my branch if have only renamed it to CharEmbedding",
                                    "url": "https://github.com/mindee/doctr/pull/1205#issuecomment-1570291122",
                                    "author": {
                                        "login": "felixT2K"
                                    }
                                },
                                {
                                    "createdAt": "2023-05-31T14:52:20Z",
                                    "bodyText": "I do not understand your decoder\nyou have\nforward(self, query, content, memory, query_mask: Optional[torch.Tensor] = None):\n\nbut what I need is this to get in input\nself.decoder(tgt_in[:, :j], memory, tgt_mask[:j, :j], tgt_query=pos_queries[:, i:j],\n                                      tgt_query_mask=query_mask[i:j, :j])",
                                    "url": "https://github.com/mindee/doctr/pull/1205#issuecomment-1570389620",
                                    "author": {
                                        "login": "nikokks"
                                    }
                                },
                                {
                                    "createdAt": "2023-05-31T16:18:47Z",
                                    "bodyText": "I do not understand your decoder you have\nforward(self, query, content, memory, query_mask: Optional[torch.Tensor] = None):\n\nbut what I need is this to get in input\nself.decoder(tgt_in[:, :j], memory, tgt_mask[:j, :j], tgt_query=pos_queries[:, i:j],\n                                      tgt_query_mask=query_mask[i:j, :j])\n\n\nIt is ok if you try it in the first stage with the original implementation but we have had some issues with pytorch's MHA implementation so we should try to use our implementation. At the end the difference is, that pytorch's implemtation takes the single masks and we use a merged mask for source and target to avoid that the model can \"cheat\" :)\nOne interesting thing:\nAfter taking a deeper look it looks like decode_ar does the same like the stuff i have implemented in https://github.com/mindee/doctr/blob/main/doctr/models/recognition/master/pytorch.py",
                                    "url": "https://github.com/mindee/doctr/pull/1205#issuecomment-1570535706",
                                    "author": {
                                        "login": "felixdittrich92"
                                    }
                                },
                                {
                                    "createdAt": "2023-05-31T18:28:24Z",
                                    "bodyText": "not sure if its good now\nwhen I run\npython3.8 ./references/recognition/train_pytorch.py parseq \nIt gets this\nNamespace(amp=False, arch='parseq', batch_size=32, device=None, epochs=10, find_lr=False, font='FreeMono.ttf,FreeSans.ttf,FreeSerif.ttf', input_size=32, lr=0.001, max_chars=12, min_chars=1, name=None, pretrained=False, push_to_hub=False, resume=None, sched='cosine', show_samples=False, test_only=False, train_path=None, train_samples=1000, val_path=None, val_samples=20, vocab='french', wb=False, weight_decay=0, workers=None)\nValidation set loaded in 1.923s (2520 samples in 79 batches)\nTrain set loaded in 0.006754s (126000 samples in 3937 batches)\nTraceback (most recent call last):--------------------------------------------------------------------------------| 0.00% [0/3937 00:00<00:00]\n  File \"./references/recognition/train_pytorch.py\", line 471, in <module>\n    main(args)\n  File \"./references/recognition/train_pytorch.py\", line 382, in main\n    fit_one_epoch(model, train_loader, batch_transforms, optimizer, scheduler, mb, amp=args.amp)\n  File \"./references/recognition/train_pytorch.py\", line 137, in fit_one_epoch\n    train_loss.backward()\n  File \"/home/nikkokks/.local/lib/python3.8/site-packages/torch/_tensor.py\", line 487, in backward\n    torch.autograd.backward(\n  File \"/home/nikkokks/.local/lib/python3.8/site-packages/torch/autograd/__init__.py\", line 200, in backward\n    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.LongTensor [32, 67]] is at version 69; expected version 68 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).\n\nAny idea ?",
                                    "url": "https://github.com/mindee/doctr/pull/1205#issuecomment-1570712489",
                                    "author": {
                                        "login": "nikokks"
                                    }
                                },
                                {
                                    "createdAt": "2023-05-31T20:31:37Z",
                                    "bodyText": "@nikokks to override self._device of the nn.Module  isn't a good idea you can use memory.device instead :)",
                                    "url": "https://github.com/mindee/doctr/pull/1205#issuecomment-1570907041",
                                    "author": {
                                        "login": "felixdittrich92"
                                    }
                                },
                                {
                                    "createdAt": "2023-05-31T20:38:11Z",
                                    "bodyText": "The issue is in pos_queries this variable changes later and shouldn't do it if we train (backward compatible) ... Changes while inference (No backward) should be fine",
                                    "url": "https://github.com/mindee/doctr/pull/1205#issuecomment-1570915740",
                                    "author": {
                                        "login": "felixdittrich92"
                                    }
                                },
                                {
                                    "createdAt": "2023-05-31T20:51:07Z",
                                    "bodyText": "The issue is in pos_queries this variable changes later and shouldn't do it if we train (backward compatible) ... Changes while inference (No backward) should be fine\n\nI do not see in the code where to change pos_queries",
                                    "url": "https://github.com/mindee/doctr/pull/1205#issuecomment-1570935219",
                                    "author": {
                                        "login": "nikokks"
                                    }
                                },
                                {
                                    "createdAt": "2023-05-31T21:21:51Z",
                                    "bodyText": "I found that the problem is in the Parseqdecoder in the\n    def forward_stream(\n        self,\n        target: torch.Tensor,\n        normalized_target: torch.Tensor,\n        kv_target: torch.Tensor,\n        memory: torch.Tensor,\n        tgt_mask: Optional[torch.Tensor],\n    ):\n\n        target +=  self.attention_dropout(self.attention(normalized_target, kv_target, kv_target, mask=tgt_mask))\n        target +=  self.cross_attention_dropout(self.cross_attention(self.attention_norm(target), memory, memory))\n        target += self.feed_forward_dropout(self.position_feed_forward(self.cross_attention_norm(target)))\n        print('target',target)\n        return target\n\nit finally returns Nan\nand I get this error\nTraceback (most recent call last):\n  File \"<stdin>\", line 6, in <module>\n  File \"/home/nikkokks/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/nikkokks/.local/lib/python3.8/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n    return func(*args, **kwargs)\n  File \"/home/nikkokks/Desktop/github/doctr/doctr/models/predictor/pytorch.py\", line 117, in forward\n    word_preds = self.reco_predictor([crop for page_crops in crops for crop in page_crops], **kwargs)\n  File \"/home/nikkokks/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/nikkokks/.local/lib/python3.8/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n    return func(*args, **kwargs)\n  File \"/home/nikkokks/Desktop/github/doctr/doctr/models/recognition/predictor/pytorch.py\", line 72, in forward\n    raw = [\n  File \"/home/nikkokks/Desktop/github/doctr/doctr/models/recognition/predictor/pytorch.py\", line 73, in <listcomp>\n    self.model(batch.to(device=_device), return_preds=True, **kwargs)[\"preds\"] for batch in processed_batches\n  File \"/home/nikkokks/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/nikkokks/Desktop/github/doctr/doctr/models/recognition/parseq/pytorch.py\", line 216, in forward\n    tgt_out = self.decode(tgt_in[:, :j], memory, tgt_mask[:j, :j], tgt_query=pos_queries[:, i:j],\n  File \"/home/nikkokks/Desktop/github/doctr/doctr/models/recognition/parseq/pytorch.py\", line 300, in decode\n    return self.decoder(tgt_query, tgt_emb, memory, tgt_query_mask)\n  File \"/home/nikkokks/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/nikkokks/Desktop/github/doctr/doctr/models/recognition/parseq/pytorch.py\", line 128, in forward\n    query = self.forward_stream(query, query_norm, content_norm, memory, query_mask)\n  File \"/home/nikkokks/Desktop/github/doctr/doctr/models/recognition/parseq/pytorch.py\", line 119, in forward_stream\n    target +=  self.attention_dropout(self.attention(normalized_target, kv_target, kv_target, mask=tgt_mask))\nRuntimeError: unsupported operation: more than one element of the written-to tensor refers to a single memory location. Please clone() the tensor before performing the operation.\n\n\nAny idea ?",
                                    "url": "https://github.com/mindee/doctr/pull/1205#issuecomment-1570974595",
                                    "author": {
                                        "login": "nikokks"
                                    }
                                },
                                {
                                    "createdAt": "2023-06-01T04:29:27Z",
                                    "bodyText": "I found that the problem is in the Parseqdecoder in the\n    def forward_stream(\n        self,\n        target: torch.Tensor,\n        normalized_target: torch.Tensor,\n        kv_target: torch.Tensor,\n        memory: torch.Tensor,\n        tgt_mask: Optional[torch.Tensor],\n    ):\n\n        target +=  self.attention_dropout(self.attention(normalized_target, kv_target, kv_target, mask=tgt_mask))\n        target +=  self.cross_attention_dropout(self.cross_attention(self.attention_norm(target), memory, memory))\n        target += self.feed_forward_dropout(self.position_feed_forward(self.cross_attention_norm(target)))\n        print('target',target)\n        return target\n\nit finally returns Nan\nand I get this error\nTraceback (most recent call last):\n  File \"<stdin>\", line 6, in <module>\n  File \"/home/nikkokks/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/nikkokks/.local/lib/python3.8/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n    return func(*args, **kwargs)\n  File \"/home/nikkokks/Desktop/github/doctr/doctr/models/predictor/pytorch.py\", line 117, in forward\n    word_preds = self.reco_predictor([crop for page_crops in crops for crop in page_crops], **kwargs)\n  File \"/home/nikkokks/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/nikkokks/.local/lib/python3.8/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n    return func(*args, **kwargs)\n  File \"/home/nikkokks/Desktop/github/doctr/doctr/models/recognition/predictor/pytorch.py\", line 72, in forward\n    raw = [\n  File \"/home/nikkokks/Desktop/github/doctr/doctr/models/recognition/predictor/pytorch.py\", line 73, in <listcomp>\n    self.model(batch.to(device=_device), return_preds=True, **kwargs)[\"preds\"] for batch in processed_batches\n  File \"/home/nikkokks/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/nikkokks/Desktop/github/doctr/doctr/models/recognition/parseq/pytorch.py\", line 216, in forward\n    tgt_out = self.decode(tgt_in[:, :j], memory, tgt_mask[:j, :j], tgt_query=pos_queries[:, i:j],\n  File \"/home/nikkokks/Desktop/github/doctr/doctr/models/recognition/parseq/pytorch.py\", line 300, in decode\n    return self.decoder(tgt_query, tgt_emb, memory, tgt_query_mask)\n  File \"/home/nikkokks/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/nikkokks/Desktop/github/doctr/doctr/models/recognition/parseq/pytorch.py\", line 128, in forward\n    query = self.forward_stream(query, query_norm, content_norm, memory, query_mask)\n  File \"/home/nikkokks/Desktop/github/doctr/doctr/models/recognition/parseq/pytorch.py\", line 119, in forward_stream\n    target +=  self.attention_dropout(self.attention(normalized_target, kv_target, kv_target, mask=tgt_mask))\nRuntimeError: unsupported operation: more than one element of the written-to tensor refers to a single memory location. Please clone() the tensor before performing the operation.\n\nAny idea ?\n\nYes i think the problem are the masks i will check this for our MHA the mask values should be 1 for See and 0 for ignore (we change the zeros in the scaled dot product to -inf)",
                                    "url": "https://github.com/mindee/doctr/pull/1205#issuecomment-1571318467",
                                    "author": {
                                        "login": "felixdittrich92"
                                    }
                                },
                                {
                                    "createdAt": "2023-06-01T07:21:29Z",
                                    "bodyText": "target = target.clone() + ..  works fine\nAbout the masking for our MHA you can use:\ndef make_source_and_target_mask(\n        self, source: torch.Tensor, target: torch.Tensor\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        # borrowed and slightly modified from  https://github.com/wenwenyu/MASTER-pytorch\n        # NOTE: nn.TransformerDecoder takes the inverse from this implementation\n        # [True, True, True, ..., False, False, False] -> False is masked\n        target_pad_mask = (target != self.vocab_size + 2).unsqueeze(1).unsqueeze(1)  # (N, 1, 1, max_length)\n        target_length = target.size(1)\n        # sub mask filled diagonal with True = see and False = masked (max_length, max_length)\n        # NOTE: onnxruntime tril/triu works only with float currently (onnxruntime 1.11.1 - opset 14)\n        target_sub_mask = torch.tril(torch.ones((target_length, target_length), device=source.device), diagonal=0).to(\n            dtype=torch.bool\n        )\n        # source mask filled with ones (max_length, positional_encoded_seq_len)\n        source_mask = torch.ones((target_length, source.size(1)), dtype=torch.uint8, device=source.device)\n        # combine the two masks into one (N, 1, max_length, max_length)\n        target_mask = target_pad_mask & target_sub_mask\n        return source_mask, target_mask.int()\n\nThe difference is that we combine the tgt_mask and target_padding_mask  (in pytorch this is internally done) and our mask contains 0 and 1 like\ntensor([[1, 0, 0],\n        [1, 1, 0],\n        [1, 1, 1]], dtype=torch.int32)\n\nin our scaled_dot_product we transform the zeros to -inf to \"ignore\" it in the softmax activation to avoid that the transformer decoder can \"cheat\"\nthe pytorch MHA implementation expects the following:\ntensor([[0., -inf, -inf],\n        [0., 0., -inf],\n        [0., 0., 0.]])\n\nAbout the train and inference forward steps in our models take a look into this (copied from master implementation):\nI have added comments in this snippet to make clear where to put which code\n      if self.training and target is None:\n          raise ValueError(\"Need to provide labels during training\")\n\n      if target is not None:\n          # put everything related to training \n\n          # Compute target: tensor of gts and sequence lengths\n          _gt, _seq_len = self.build_target(target)\n          gt, seq_len = torch.from_numpy(_gt).to(dtype=torch.long), torch.tensor(_seq_len)\n          gt, seq_len = gt.to(x.device), seq_len.to(x.device)\n\n          # Compute source mask and target mask\n          source_mask, target_mask = self.make_source_and_target_mask(encoded, gt)\n          output = self.decoder(gt, encoded, source_mask, target_mask)\n          # Compute logits\n          logits = self.linear(output)\n      else:\n          # put every step for inference after the else\n          logits = self.decode(encoded)\n\n      if self.exportable:\n          out[\"logits\"] = logits\n          return out\n\n      if target is not None:\n          out[\"loss\"] = self.compute_loss(logits, gt, seq_len)\n\n      if return_model_output:\n          out[\"out_map\"] = logits\n\n      if return_preds:\n          out[\"preds\"] = self.postprocessor(logits)\n\n      return out\n\nIf it at the moment does not work with our MHA you can also follow the original implementation we can debug this later :)\nAbout your commits:\nPlease follow https://mindee.github.io/doctr/latest/contributing/contributing.html\nBefore you push your commits please run\nmake style\nIt makes it much easier for me to review things :)",
                                    "url": "https://github.com/mindee/doctr/pull/1205#issuecomment-1571496284",
                                    "author": {
                                        "login": "felixdittrich92"
                                    }
                                },
                                {
                                    "createdAt": "2023-06-01T09:02:30Z",
                                    "bodyText": "Btw. if i understand it correctly:\ndef decode(self, encoded: torch.Tensor) -> torch.Tensor:\n        \"\"\"Decode function for prediction\n\n        Args:\n            encoded: input tensor\n\n        Return:\n            A Tuple of torch.Tensor: predictions, logits\n        \"\"\"\n        b = encoded.size(0)\n\n\n        # Padding symbol + SOS at the beginning\n        ys = torch.full((b, self.max_length), self.vocab_size + 2, dtype=torch.long, device=encoded.device)  # pad\n        ys[:, 0] = self.vocab_size + 1  # sos\n\n        # Final dimension include EOS/SOS/PAD\n        for i in range(self.max_length - 1):\n            source_mask, target_mask = self.make_source_and_target_mask(encoded, ys)\n            output = self.decoder(ys, encoded, source_mask, target_mask)\n            logits = self.head(output)\n            prob = torch.softmax(logits, dim=-1)\n            next_token = torch.max(prob, dim=-1).indices\n            # update ys with the next token and ignore the first token (SOS)\n            ys[:, i + 1] = next_token[:, i]\n\n        # Shape (N, max_length, vocab_size + 1)\n        return logits\n\nshould do the same as decode_ar the only thing we would need to change is the mask that we only make the current position \"visible\" if this works we could extremly simplify the forward pass",
                                    "url": "https://github.com/mindee/doctr/pull/1205#issuecomment-1571643068",
                                    "author": {
                                        "login": "felixdittrich92"
                                    }
                                },
                                {
                                    "createdAt": "2023-06-01T09:20:10Z",
                                    "bodyText": "Seems that you don't have installed doctr corretly:\ninside the doctr folder run:\ninstall\npip3 install -e .[dev]\npre-commit install\nstyle\nmake style\nthan you can add and commit your changes",
                                    "url": "https://github.com/mindee/doctr/pull/1205#issuecomment-1571671123",
                                    "author": {
                                        "login": "felixT2K"
                                    }
                                },
                                {
                                    "createdAt": "2023-06-01T10:56:51Z",
                                    "bodyText": "first of all i am trying to succes training one epoch before getting the code requirements\nWith this new commit when I run the following command\npython3.8 ./references/recognition/train_pytorch.py parseq \n\nit gets\nTraceback (most recent call last):\n  File \"./references/recognition/train_pytorch.py\", line 475, in <module>\n    main(args)\n  File \"./references/recognition/train_pytorch.py\", line 386, in main\n    fit_one_epoch(model, train_loader, batch_transforms, optimizer, scheduler, mb, amp=args.amp)\n  File \"./references/recognition/train_pytorch.py\", line 141, in fit_one_epoch\n    train_loss.backward()\n  File \"/home/nikkokks/.local/lib/python3.8/site-packages/torch/_tensor.py\", line 487, in backward\n    torch.autograd.backward(\n  File \"/home/nikkokks/.local/lib/python3.8/site-packages/torch/autograd/__init__.py\", line 200, in backward\n    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.LongTensor [1, 66]] is at version 68; expected version 67 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!\n\n\nIf I understand well, It is caused by target",
                                    "url": "https://github.com/mindee/doctr/pull/1205#issuecomment-1571824252",
                                    "author": {
                                        "login": "nikokks"
                                    }
                                },
                                {
                                    "createdAt": "2023-06-01T10:59:13Z",
                                    "bodyText": "Hi to test the temporary implementation I try to load the model parseq, but the keys of state_dict are not the same now.\nthis is what I get\n\tMissing key(s) in state_dict: \"encoder.0.cls_token\", \"encoder.0.positions\", \"encoder.0.proj.weight\", \"encoder.0.proj.bias\", \"encoder.1.layer_norm_input.weight\", \"encoder.1.layer_norm_input.bias\", \"encoder.1.layer_norm_attention.weight\", \"encoder.1.layer_norm_attention.bias\", \"encoder.1.layer_norm_output.weight\", \"encoder.1.layer_norm_output.bias\", \"encoder.1.attention.0.linear_layers.0.weight\", \"encoder.1.attention.0.linear_layers.0.bias\", \"encoder.1.attention.0.linear_layers.1.weight\", \"encoder.1.attention.0.linear_layers.1.bias\", \"encoder.1.attention.0.linear_layers.2.weight\", \"encoder.1.attention.0.linear_layers.2.bias\", \"encoder.1.attention.0.output_linear.weight\", \"encoder.1.attention.0.output_linear.bias\", \"encoder.1.attention.1.linear_layers.0.weight\", \"encoder.1.attention.1.linear_layers.0.bias\", \"encoder.1.attention.1.linear_layers.1.weight\", \"encoder.1.attention.1.linear_layers.1.bias\", \"encoder.1.attention.1.linear_layers.2.weight\", \"encoder.1.attention.1.linear_layers.2.bias\", \"encoder.1.attention.1.output_linear.weight\", \"encoder.1.attention.1.output_linear.bias\", \"encoder.1.attention.2.linear_layers.0.weight\", \"encoder.1.attention.2.linear_layers.0.bias\", \"encoder.1.attention.2.linear_layers.1.weight\", \"encoder.1.attention.2.linear_layers.1.bias\", \"encoder.1.attention.2.linear_layers.2.weight\", \"encoder.1.attention.2.linear_layers.2.bias\", \"encoder.1.attention.2.output_linear.weight\", \"encoder.1.attention.2.output_linear.bias\", \"encoder.1.attention.3.linear_layers.0.weight\", \"encoder.1.attention.3.linear_layers.0.bias\", \"encoder.1.attention.3.linear_layers.1.weight\", \"encoder.1.attention.3.linear_layers.1.bias\", \"encoder.1.attention.3.linear_layers.2.weight\", \"encoder.1.attention.3.linear_layers.2.bias\", \"encoder.1.attention.3.output_linear.weight\", \"encoder.1.attention.3.output_linear.bias\", \"encoder.1.attention.4.linear_layers.0.weight\", \"encoder.1.attention.4.linear_layers.0.bias\", \"encoder.1.attention.4.linear_layers.1.weight\", \"encoder.1.attention.4.linear_layers.1.bias\", \"encoder.1.attention.4.linear_layers.2.weight\", \"encoder.1.attention.4.linear_layers.2.bias\", \"encoder.1.attention.4.output_linear.weight\", \"encoder.1.attention.4.output_linear.bias\", \"encoder.1.attention.5.linear_layers.0.weight\", \"encoder.1.attention.5.linear_layers.0.bias\", \"encoder.1.attention.5.linear_layers.1.weight\", \"encoder.1.attention.5.linear_layers.1.bias\", \"encoder.1.attention.5.linear_layers.2.weight\", \"encoder.1.attention.5.linear_layers.2.bias\", \"encoder.1.attention.5.output_linear.weight\", \"encoder.1.attention.5.output_linear.bias\", \"encoder.1.attention.6.linear_layers.0.weight\", \"encoder.1.attention.6.linear_layers.0.bias\", \"encoder.1.attention.6.linear_layers.1.weight\", \"encoder.1.attention.6.linear_layers.1.bias\", \"encoder.1.attention.6.linear_layers.2.weight\", \"encoder.1.attention.6.linear_layers.2.bias\", \"encoder.1.attention.6.output_linear.weight\", \"encoder.1.attention.6.output_linear.bias\", \"encoder.1.attention.7.linear_layers.0.weight\", \"encoder.1.attention.7.linear_layers.0.bias\", \"encoder.1.attention.7.linear_layers.1.weight\", \"encoder.1.attention.7.linear_layers.1.bias\", \"encoder.1.attention.7.linear_layers.2.weight\", \"encoder.1.attention.7.linear_layers.2.bias\", \"encoder.1.attention.7.output_linear.weight\", \"encoder.1.attention.7.output_linear.bias\", \"encoder.1.attention.8.linear_layers.0.weight\", \"encoder.1.attention.8.linear_layers.0.bias\", \"encoder.1.attention.8.linear_layers.1.weight\", \"encoder.1.attention.8.linear_layers.1.bias\", \"encoder.1.attention.8.linear_layers.2.weight\", \"encoder.1.attention.8.linear_layers.2.bias\", \"encoder.1.attention.8.output_linear.weight\", \"encoder.1.attention.8.output_linear.bias\", \"encoder.1.attention.9.linear_layers.0.weight\", \"encoder.1.attention.9.linear_layers.0.bias\", \"encoder.1.attention.9.linear_layers.1.weight\", \"encoder.1.attention.9.linear_layers.1.bias\", \"encoder.1.attention.9.linear_layers.2.weight\", \"encoder.1.attention.9.linear_layers.2.bias\", \"encoder.1.attention.9.output_linear.weight\", \"encoder.1.attention.9.output_linear.bias\", \"encoder.1.attention.10.linear_layers.0.weight\", \"encoder.1.attention.10.linear_layers.0.bias\", \"encoder.1.attention.10.linear_layers.1.weight\", \"encoder.1.attention.10.linear_layers.1.bias\", \"encoder.1.attention.10.linear_layers.2.weight\", \"encoder.1.attention.10.linear_layers.2.bias\", \"encoder.1.attention.10.output_linear.weight\", \"encoder.1.attention.10.output_linear.bias\", \"encoder.1.attention.11.linear_layers.0.weight\", \"encoder.1.attention.11.linear_layers.0.bias\", \"encoder.1.attention.11.linear_layers.1.weight\", \"encoder.1.attention.11.linear_layers.1.bias\", \"encoder.1.attention.11.linear_layers.2.weight\", \"encoder.1.attention.11.linear_layers.2.bias\", \"encoder.1.attention.11.output_linear.weight\", \"encoder.1.attention.11.output_linear.bias\", \"encoder.1.position_feed_forward.0.0.weight\", \"encoder.1.position_feed_forward.0.0.bias\", \"encoder.1.position_feed_forward.0.3.weight\", \"encoder.1.position_feed_forward.0.3.bias\", \"encoder.1.position_feed_forward.1.0.weight\", \"encoder.1.position_feed_forward.1.0.bias\", \"encoder.1.position_feed_forward.1.3.weight\", \"encoder.1.position_feed_forward.1.3.bias\", \"encoder.1.position_feed_forward.2.0.weight\", \"encoder.1.position_feed_forward.2.0.bias\", \"encoder.1.position_feed_forward.2.3.weight\", \"encoder.1.position_feed_forward.2.3.bias\", \"encoder.1.position_feed_forward.3.0.weight\", \"encoder.1.position_feed_forward.3.0.bias\", \"encoder.1.position_feed_forward.3.3.weight\", \"encoder.1.position_feed_forward.3.3.bias\", \"encoder.1.position_feed_forward.4.0.weight\", \"encoder.1.position_feed_forward.4.0.bias\", \"encoder.1.position_feed_forward.4.3.weight\", \"encoder.1.position_feed_forward.4.3.bias\", \"encoder.1.position_feed_forward.5.0.weight\", \"encoder.1.position_feed_forward.5.0.bias\", \"encoder.1.position_feed_forward.5.3.weight\", \"encoder.1.position_feed_forward.5.3.bias\", \"encoder.1.position_feed_forward.6.0.weight\", \"encoder.1.position_feed_forward.6.0.bias\", \"encoder.1.position_feed_forward.6.3.weight\", \"encoder.1.position_feed_forward.6.3.bias\", \"encoder.1.position_feed_forward.7.0.weight\", \"encoder.1.position_feed_forward.7.0.bias\", \"encoder.1.position_feed_forward.7.3.weight\", \"encoder.1.position_feed_forward.7.3.bias\", \"encoder.1.position_feed_forward.8.0.weight\", \"encoder.1.position_feed_forward.8.0.bias\", \"encoder.1.position_feed_forward.8.3.weight\", \"encoder.1.position_feed_forward.8.3.bias\", \"encoder.1.position_feed_forward.9.0.weight\", \"encoder.1.position_feed_forward.9.0.bias\", \"encoder.1.position_feed_forward.9.3.weight\", \"encoder.1.position_feed_forward.9.3.bias\", \"encoder.1.position_feed_forward.10.0.weight\", \"encoder.1.position_feed_forward.10.0.bias\", \"encoder.1.position_feed_forward.10.3.weight\", \"encoder.1.position_feed_forward.10.3.bias\", \"encoder.1.position_feed_forward.11.0.weight\", \"encoder.1.position_feed_forward.11.0.bias\", \"encoder.1.position_feed_forward.11.3.weight\", \"encoder.1.position_feed_forward.11.3.bias\".\n\t\n\t\n\t \n\tUnexpected key(s) in state_dict: \"encoder.pos_embed\", \"encoder.patch_embed.proj.weight\", \"encoder.patch_embed.proj.bias\", \"encoder.blocks.0.norm1.weight\", \"encoder.blocks.0.norm1.bias\", \"encoder.blocks.0.attn.qkv.weight\", \"encoder.blocks.0.attn.qkv.bias\", \"encoder.blocks.0.attn.proj.weight\", \"encoder.blocks.0.attn.proj.bias\", \"encoder.blocks.0.norm2.weight\", \"encoder.blocks.0.norm2.bias\", \"encoder.blocks.0.mlp.fc1.weight\", \"encoder.blocks.0.mlp.fc1.bias\", \"encoder.blocks.0.mlp.fc2.weight\", \"encoder.blocks.0.mlp.fc2.bias\", \"encoder.blocks.1.norm1.weight\", \"encoder.blocks.1.norm1.bias\", \"encoder.blocks.1.attn.qkv.weight\", \"encoder.blocks.1.attn.qkv.bias\", \"encoder.blocks.1.attn.proj.weight\", \"encoder.blocks.1.attn.proj.bias\", \"encoder.blocks.1.norm2.weight\", \"encoder.blocks.1.norm2.bias\", \"encoder.blocks.1.mlp.fc1.weight\", \"encoder.blocks.1.mlp.fc1.bias\", \"encoder.blocks.1.mlp.fc2.weight\", \"encoder.blocks.1.mlp.fc2.bias\", \"encoder.blocks.2.norm1.weight\", \"encoder.blocks.2.norm1.bias\", \"encoder.blocks.2.attn.qkv.weight\", \"encoder.blocks.2.attn.qkv.bias\", \"encoder.blocks.2.attn.proj.weight\", \"encoder.blocks.2.attn.proj.bias\", \"encoder.blocks.2.norm2.weight\", \"encoder.blocks.2.norm2.bias\", \"encoder.blocks.2.mlp.fc1.weight\", \"encoder.blocks.2.mlp.fc1.bias\", \"encoder.blocks.2.mlp.fc2.weight\", \"encoder.blocks.2.mlp.fc2.bias\", \"encoder.blocks.3.norm1.weight\", \"encoder.blocks.3.norm1.bias\", \"encoder.blocks.3.attn.qkv.weight\", \"encoder.blocks.3.attn.qkv.bias\", \"encoder.blocks.3.attn.proj.weight\", \"encoder.blocks.3.attn.proj.bias\", \"encoder.blocks.3.norm2.weight\", \"encoder.blocks.3.norm2.bias\", \"encoder.blocks.3.mlp.fc1.weight\", \"encoder.blocks.3.mlp.fc1.bias\", \"encoder.blocks.3.mlp.fc2.weight\", \"encoder.blocks.3.mlp.fc2.bias\", \"encoder.blocks.4.norm1.weight\", \"encoder.blocks.4.norm1.bias\", \"encoder.blocks.4.attn.qkv.weight\", \"encoder.blocks.4.attn.qkv.bias\", \"encoder.blocks.4.attn.proj.weight\", \"encoder.blocks.4.attn.proj.bias\", \"encoder.blocks.4.norm2.weight\", \"encoder.blocks.4.norm2.bias\", \"encoder.blocks.4.mlp.fc1.weight\", \"encoder.blocks.4.mlp.fc1.bias\", \"encoder.blocks.4.mlp.fc2.weight\", \"encoder.blocks.4.mlp.fc2.bias\", \"encoder.blocks.5.norm1.weight\", \"encoder.blocks.5.norm1.bias\", \"encoder.blocks.5.attn.qkv.weight\", \"encoder.blocks.5.attn.qkv.bias\", \"encoder.blocks.5.attn.proj.weight\", \"encoder.blocks.5.attn.proj.bias\", \"encoder.blocks.5.norm2.weight\", \"encoder.blocks.5.norm2.bias\", \"encoder.blocks.5.mlp.fc1.weight\", \"encoder.blocks.5.mlp.fc1.bias\", \"encoder.blocks.5.mlp.fc2.weight\", \"encoder.blocks.5.mlp.fc2.bias\", \"encoder.blocks.6.norm1.weight\", \"encoder.blocks.6.norm1.bias\", \"encoder.blocks.6.attn.qkv.weight\", \"encoder.blocks.6.attn.qkv.bias\", \"encoder.blocks.6.attn.proj.weight\", \"encoder.blocks.6.attn.proj.bias\", \"encoder.blocks.6.norm2.weight\", \"encoder.blocks.6.norm2.bias\", \"encoder.blocks.6.mlp.fc1.weight\", \"encoder.blocks.6.mlp.fc1.bias\", \"encoder.blocks.6.mlp.fc2.weight\", \"encoder.blocks.6.mlp.fc2.bias\", \"encoder.blocks.7.norm1.weight\", \"encoder.blocks.7.norm1.bias\", \"encoder.blocks.7.attn.qkv.weight\", \"encoder.blocks.7.attn.qkv.bias\", \"encoder.blocks.7.attn.proj.weight\", \"encoder.blocks.7.attn.proj.bias\", \"encoder.blocks.7.norm2.weight\", \"encoder.blocks.7.norm2.bias\", \"encoder.blocks.7.mlp.fc1.weight\", \"encoder.blocks.7.mlp.fc1.bias\", \"encoder.blocks.7.mlp.fc2.weight\", \"encoder.blocks.7.mlp.fc2.bias\", \"encoder.blocks.8.norm1.weight\", \"encoder.blocks.8.norm1.bias\", \"encoder.blocks.8.attn.qkv.weight\", \"encoder.blocks.8.attn.qkv.bias\", \"encoder.blocks.8.attn.proj.weight\", \"encoder.blocks.8.attn.proj.bias\", \"encoder.blocks.8.norm2.weight\", \"encoder.blocks.8.norm2.bias\", \"encoder.blocks.8.mlp.fc1.weight\", \"encoder.blocks.8.mlp.fc1.bias\", \"encoder.blocks.8.mlp.fc2.weight\", \"encoder.blocks.8.mlp.fc2.bias\", \"encoder.blocks.9.norm1.weight\", \"encoder.blocks.9.norm1.bias\", \"encoder.blocks.9.attn.qkv.weight\", \"encoder.blocks.9.attn.qkv.bias\", \"encoder.blocks.9.attn.proj.weight\", \"encoder.blocks.9.attn.proj.bias\", \"encoder.blocks.9.norm2.weight\", \"encoder.blocks.9.norm2.bias\", \"encoder.blocks.9.mlp.fc1.weight\", \"encoder.blocks.9.mlp.fc1.bias\", \"encoder.blocks.9.mlp.fc2.weight\", \"encoder.blocks.9.mlp.fc2.bias\", \"encoder.blocks.10.norm1.weight\", \"encoder.blocks.10.norm1.bias\", \"encoder.blocks.10.attn.qkv.weight\", \"encoder.blocks.10.attn.qkv.bias\", \"encoder.blocks.10.attn.proj.weight\", \"encoder.blocks.10.attn.proj.bias\", \"encoder.blocks.10.norm2.weight\", \"encoder.blocks.10.norm2.bias\", \"encoder.blocks.10.mlp.fc1.weight\", \"encoder.blocks.10.mlp.fc1.bias\", \"encoder.blocks.10.mlp.fc2.weight\", \"encoder.blocks.10.mlp.fc2.bias\", \"encoder.blocks.11.norm1.weight\", \"encoder.blocks.11.norm1.bias\", \"encoder.blocks.11.attn.qkv.weight\", \"encoder.blocks.11.attn.qkv.bias\", \"encoder.blocks.11.attn.proj.weight\", \"encoder.blocks.11.attn.proj.bias\", \"encoder.blocks.11.norm2.weight\", \"encoder.blocks.11.norm2.bias\", \"encoder.blocks.11.mlp.fc1.weight\", \"encoder.blocks.11.mlp.fc1.bias\", \"encoder.blocks.11.mlp.fc2.weight\", \"encoder.blocks.11.mlp.fc2.bias\", \"encoder.norm.weight\", \"encoder.norm.bias\".\n\t\n\t\n\t\n\ndo you know how to do ? This is due to the difference implementation and names of VitSTR Encoder\n\nFor debugging you can run: python references/recognition/train_pytorch.py parseq Or convert the actual state_dict but i would suggest the above way :) If the implementation is done we can do a dummy run to verify that all works well\n\nI would like to train the model finally =)",
                                    "url": "https://github.com/mindee/doctr/pull/1205#issuecomment-1571830499",
                                    "author": {
                                        "login": "nikokks"
                                    }
                                },
                                {
                                    "createdAt": "2023-06-01T11:00:12Z",
                                    "bodyText": "make_source_and_target_mask\n\nIs it the problem where I get stuck ?",
                                    "url": "https://github.com/mindee/doctr/pull/1205#issuecomment-1571832240",
                                    "author": {
                                        "login": "nikokks"
                                    }
                                },
                                {
                                    "createdAt": "2023-06-01T11:06:41Z",
                                    "bodyText": "make_source_and_target_mask\n\nIs it the problem where I get stuck ?\n\nDoes the decoder still compute Nan's than is the answer yes ! Otherwise i think there is a problem with the pos_queries (which has a shape of the traceback mentioned LongTensor)\nOn my branch it runs and can be trained but i haven't had the time for the refine iters and the permutations",
                                    "url": "https://github.com/mindee/doctr/pull/1205#issuecomment-1571842273",
                                    "author": {
                                        "login": "felixdittrich92"
                                    }
                                },
                                {
                                    "createdAt": "2023-06-01T11:10:13Z",
                                    "bodyText": "make_source_and_target_mask\n\nIs it the problem where I get stuck ?\n\nDoes the decoder still compute Nan's than is the answer yes ! Otherwise i think there is a problem with the pos_queries (which has a shape of the traceback mentioned LongTensor)\nOn my branch it runs and can be trained but i haven't had the time for the refine iters and the permutations\n\nFor the NaNs I found that init weights of self.pos_queries in init_ works with this line\nnn.init.trunc_normal_(self.pos_queries, std=0.02)",
                                    "url": "https://github.com/mindee/doctr/pull/1205#issuecomment-1571848286",
                                    "author": {
                                        "login": "nikokks"
                                    }
                                },
                                {
                                    "createdAt": "2023-06-01T13:48:18Z",
                                    "bodyText": "make_source_and_target_mask\n\nIs it the problem where I get stuck ?\n\nDoes the decoder still compute Nan's than is the answer yes ! Otherwise i think there is a problem with the pos_queries (which has a shape of the traceback mentioned LongTensor)\nOn my branch it runs and can be trained but i haven't had the time for the refine iters and the permutations\n\nYou were wright it is self.pos_queries !\nWhen you talk about It can be trained what do you mean ?\nCan you send me the commits to merge it with my code plz ?",
                                    "url": "https://github.com/mindee/doctr/pull/1205#issuecomment-1572088695",
                                    "author": {
                                        "login": "nikokks"
                                    }
                                },
                                {
                                    "createdAt": "2023-06-01T13:49:42Z",
                                    "bodyText": "More or less how much time does it cost to train others models you have trained ?\nI want to use my colab pro to train parseq .\nI think a SoTA model for Doctr would be good =)",
                                    "url": "https://github.com/mindee/doctr/pull/1205#issuecomment-1572090855",
                                    "author": {
                                        "login": "nikokks"
                                    }
                                },
                                {
                                    "createdAt": "2023-06-01T14:23:40Z",
                                    "bodyText": "make_source_and_target_mask\n\nIs it the problem where I get stuck ?\n\nDoes the decoder still compute Nan's than is the answer yes ! Otherwise i think there is a problem with the pos_queries (which has a shape of the traceback mentioned LongTensor)\nOn my branch it runs and can be trained but i haven't had the time for the refine iters and the permutations\n\nYou were wright it is self.pos_queries ! When you talk about It can be trained what do you mean ? Can you send me the commits to merge it with my code plz ?\n\ncode : https://github.com/felixdittrich92/doctr/tree/parseq-torch  (It runs but some steps from parseq are missing (refine_iters and the permutations and i will check some decoder parts again).. i need to spent more time to complete this)\nSuch a model implementation is not done in 1 or 2 days needs some debugging, architecture understanding, etc. \ud83d\ude05\nBut i would say we are on the right track so don't give up - you will learn a lot if we are done with it ;)",
                                    "url": "https://github.com/mindee/doctr/pull/1205#issuecomment-1572155520",
                                    "author": {
                                        "login": "felixdittrich92"
                                    }
                                },
                                {
                                    "createdAt": "2023-06-01T14:29:16Z",
                                    "bodyText": "More or less how much time does it cost to train others models you have trained ? I want to use my colab pro to train parseq . I think a SoTA model for Doctr would be good =)\n\nHardly to answer .. training from scratch needs lots of data (mindee's internal reco dataset contains >10M real data crops)\nOur MASTER and ViTSTR implemtations are also good (both will be really near on the parseq performance ..i think MASTER would outperform parseq if it is trained on real data) but still needs pretrained checkpoints .. this will take some time but Olivier is on it :)\nFinetuning on Colab yes ...but training from scratch will take days ...depending on the dataset size and quality the performance can differ much",
                                    "url": "https://github.com/mindee/doctr/pull/1205#issuecomment-1572165222",
                                    "author": {
                                        "login": "felixdittrich92"
                                    }
                                },
                                {
                                    "createdAt": "2023-06-02T13:37:26Z",
                                    "bodyText": "@nikokks the code in https://github.com/felixdittrich92/doctr/tree/parseq-torch works now also with our MHA implementation ... i need to rewrite some stuff ..a lot of cleaning up ..but yeah it is on the right way :)",
                                    "url": "https://github.com/mindee/doctr/pull/1205#issuecomment-1573748704",
                                    "author": {
                                        "login": "felixdittrich92"
                                    }
                                },
                                {
                                    "createdAt": "2023-06-02T14:43:57Z",
                                    "bodyText": "it was horrible to merge since 2 days haha",
                                    "url": "https://github.com/mindee/doctr/pull/1205#issuecomment-1573853749",
                                    "author": {
                                        "login": "nikokks"
                                    }
                                },
                                {
                                    "createdAt": "2023-06-02T14:56:07Z",
                                    "bodyText": "How do I permit to push on my nikokks doctr main ?\nI have this\ngit commit\ncheck python ast.........................................................Passed\ncheck yaml...........................................(no files to check)Skipped\ncheck toml...........................................(no files to check)Skipped\ncheck json...........................................(no files to check)Skipped\ncheck for added large files..............................................Passed\nfix end of files.........................................................Passed\ntrim trailing whitespace.................................................Passed\ndebug statements (python)................................................Passed\ncheck for merge conflicts................................................Passed\ndon't commit to branch...................................................Failed\n- hook id: no-commit-to-branch\n- exit code: 1\nblack....................................................................Passed\nisort....................................................................Passed\nruff.....................................................................Passed",
                                    "url": "https://github.com/mindee/doctr/pull/1205#issuecomment-1573870126",
                                    "author": {
                                        "login": "nikokks"
                                    }
                                },
                                {
                                    "createdAt": "2023-06-02T15:38:06Z",
                                    "bodyText": "Write SKIP=no-commit-to-branch git commit ..",
                                    "url": "https://github.com/mindee/doctr/pull/1205#issuecomment-1573940389",
                                    "author": {
                                        "login": "felixdittrich92"
                                    }
                                },
                                {
                                    "createdAt": "2023-06-05T12:23:20Z",
                                    "bodyText": "Hi @nikokks \ud83d\udc4b,\nI would suggest, that you revert your stuff and merge main...felixdittrich92:doctr:parseq-torch into your branch.\nIt is in a good shape to work with.\nPyTorch: needs some further debugging but both runs (train + inference)\nTensorFlow: train runs / inference broken currently\nI don't have much time this week so feel free to continue from this state :)",
                                    "url": "https://github.com/mindee/doctr/pull/1205#issuecomment-1576687571",
                                    "author": {
                                        "login": "felixdittrich92"
                                    }
                                },
                                {
                                    "createdAt": "2023-06-05T17:12:40Z",
                                    "bodyText": "Hi @nikokks \ud83d\udc4b,\nI would suggest, that you revert your stuff and merge main...felixdittrich92:doctr:parseq-torch into your branch. It is in a good shape to work with. PyTorch: needs some further debugging but both runs (train + inference) TensorFlow: train runs / inference broken currently\nI don't have much time this week so feel free to continue from this state :)\n\nHi,\nThis is what I was going to do :)\nI am busy these 2 next days. I will continue in the middle of the week.",
                                    "url": "https://github.com/mindee/doctr/pull/1205#issuecomment-1577172789",
                                    "author": {
                                        "login": "nikokks"
                                    }
                                },
                                {
                                    "createdAt": "2023-06-07T14:23:54Z",
                                    "bodyText": "TODO: exclude CLS token from features",
                                    "url": "https://github.com/mindee/doctr/pull/1205#issuecomment-1580933969",
                                    "author": {
                                        "login": "felixT2K"
                                    }
                                },
                                {
                                    "createdAt": "2023-06-08T09:01:06Z",
                                    "bodyText": "I have done a merge from your branch parseq-torch to my main.\nIs it ok for you ?\nThen I will complete the things todo :)",
                                    "url": "https://github.com/mindee/doctr/pull/1205#issuecomment-1582178073",
                                    "author": {
                                        "login": "nikokks"
                                    }
                                },
                                {
                                    "createdAt": "2023-06-08T09:12:27Z",
                                    "bodyText": "I have done a merge from your branch parseq-torch to my main. Is it ok for you ? Then I will complete the things todo :)\n\nYour PR contains still some older files see: https://github.com/mindee/doctr/pull/1205/files\nPlease update this firstly complete with my branch :)\nAfter this feel free to work on the to does\nOne thing in pytorch Line 319  after initializing the feature extractor write:\n# remove cls token\nfeatures = features[:, 1:, :]\n\nI have triggered a dummy run this morning i will share the result if it's done",
                                    "url": "https://github.com/mindee/doctr/pull/1205#issuecomment-1582194598",
                                    "author": {
                                        "login": "felixT2K"
                                    }
                                },
                                {
                                    "createdAt": "2023-06-08T09:54:44Z",
                                    "bodyText": "I have done a merge from your branch parseq-torch to my main. Is it ok for you ? Then I will complete the things todo :)\n\nYour PR contains still some older files see: https://github.com/mindee/doctr/pull/1205/files Please update this firstly complete with my branch :)\nAfter this feel free to work on the to does\nOne thing in pytorch Line 319 after initializing the feature extractor write:\n# remove cls token\nfeatures = features[:, 1:, :]\n\nI have triggered a dummy run this morning i will share the result if it's done\n\nI think I have done the changes.\nI do not know from which branch to compare.",
                                    "url": "https://github.com/mindee/doctr/pull/1205#issuecomment-1582249630",
                                    "author": {
                                        "login": "nikokks"
                                    }
                                },
                                {
                                    "createdAt": "2023-06-08T09:59:54Z",
                                    "bodyText": "compare with this: main...felixdittrich92:doctr:parseq-torch\nthe cls token remove is not included i will push this later into the branch",
                                    "url": "https://github.com/mindee/doctr/pull/1205#issuecomment-1582263129",
                                    "author": {
                                        "login": "felixT2K"
                                    }
                                },
                                {
                                    "createdAt": "2023-06-08T10:19:08Z",
                                    "bodyText": "Validation loss decreased inf --> 2.86128: saving state...\nEpoch 1/10 - Validation loss: 2.86128 (Exact: 0.21% | Partial: 0.40%)\ntrain works but converges really slow",
                                    "url": "https://github.com/mindee/doctr/pull/1205#issuecomment-1582312085",
                                    "author": {
                                        "login": "felixdittrich92"
                                    }
                                },
                                {
                                    "createdAt": "2023-06-08T11:10:51Z",
                                    "bodyText": "Validation loss decreased inf --> 2.86128: saving state... Epoch 1/10 - Validation loss: 2.86128 (Exact: 0.21% | Partial: 0.40%)\ntrain works but converges really slow\n\nSome ideas :\n\nLRscheduler with lr start high ?\nVit_s pretrained initialized with ckpt ?\nuse optimiser yogi (faster convergence) or https://www.researchgate.net/figure/Comparison-of-training-accuracy-of-SGD-Adam-AdaBound-Yogi-AdaBelief-SAdam-and_fig2_351134207",
                                    "url": "https://github.com/mindee/doctr/pull/1205#issuecomment-1582387969",
                                    "author": {
                                        "login": "nikokks"
                                    }
                                },
                                {
                                    "createdAt": "2023-06-08T11:47:24Z",
                                    "bodyText": "Validation loss decreased inf --> 2.86128: saving state... Epoch 1/10 - Validation loss: 2.86128 (Exact: 0.21% | Partial: 0.40%)\ntrain works but converges really slow\n\nI have done\ngit remote add upstream git@github.com:felixdittrich92/doctr.git\ngit fetch upstream\ngit checkout -b parseq-torch upstream/parseq-torch\ngit checkout main\ngit merge parseq-torch\ngit merge parseq-torch\n\nand it says me\nEverything up-to-date\n\nis my main ok ?",
                                    "url": "https://github.com/mindee/doctr/pull/1205#issuecomment-1582435834",
                                    "author": {
                                        "login": "nikokks"
                                    }
                                },
                                {
                                    "createdAt": "2023-06-08T12:06:29Z",
                                    "bodyText": "yeah looks right (there are only some files which i havent modified)",
                                    "url": "https://github.com/mindee/doctr/pull/1205#issuecomment-1582465059",
                                    "author": {
                                        "login": "felixT2K"
                                    }
                                },
                                {
                                    "createdAt": "2023-06-08T12:28:38Z",
                                    "bodyText": "okay now ? ( ^.^*)",
                                    "url": "https://github.com/mindee/doctr/pull/1205#issuecomment-1582494902",
                                    "author": {
                                        "login": "nikokks"
                                    }
                                },
                                {
                                    "createdAt": "2023-06-08T12:30:33Z",
                                    "bodyText": "Yep :) only a git and main file left to remove but you can remove it later ..now looks much better \ud83d\udc4d\ud83c\udffc\nThan i would say happy coding and debugging :D",
                                    "url": "https://github.com/mindee/doctr/pull/1205#issuecomment-1582497315",
                                    "author": {
                                        "login": "felixT2K"
                                    }
                                },
                                {
                                    "createdAt": "2023-06-08T12:46:42Z",
                                    "bodyText": "as you can see in this paper https://www.researchgate.net/figure/Comparison-of-training-accuracy-of-SGD-Adam-AdaBound-Yogi-AdaBelief-SAdam-and_fig2_351134207\nfasAdatBelief optimizer seems to allow faster convergence.\nAs I do not have the code I used chat gpt https://chat.openai.com/share/d13161ad-4054-422d-937f-e0d59f9a7206\nDo you think it could be better to use it ? It seems to be yes for me :)",
                                    "url": "https://github.com/mindee/doctr/pull/1205#issuecomment-1582519563",
                                    "author": {
                                        "login": "nikokks"
                                    }
                                },
                                {
                                    "createdAt": "2023-06-08T14:07:13Z",
                                    "bodyText": "Will you need the Refine Iters feature for model training?\nIf so, anyway, I'm trying the implementation tomorrow :)",
                                    "url": "https://github.com/mindee/doctr/pull/1205#issuecomment-1582642710",
                                    "author": {
                                        "login": "nikokks"
                                    }
                                },
                                {
                                    "createdAt": "2023-06-08T14:16:25Z",
                                    "bodyText": "Will you need the Refine Iters feature for model training? If so, anyway, I'm trying the implementation tomorrow :)\n\nNo the decode_predictions function is only called in inference\nI will try to find a way to train a little dummy for inference code verification ..\nBut the training looks good (tested with our vit model as backbone and the timm implementation both are while training really near)",
                                    "url": "https://github.com/mindee/doctr/pull/1205#issuecomment-1582659623",
                                    "author": {
                                        "login": "felixT2K"
                                    }
                                },
                                {
                                    "createdAt": "2023-06-08T15:09:02Z",
                                    "bodyText": "Will you need the Refine Iters feature for model training? If so, anyway, I'm trying the implementation tomorrow :)\n\nNo the decode_predictions function is only called in inference\nI will try to find a way to train a little dummy for inference code verification .. But the training looks good (tested with our vit model as backbone and the timm implementation both are while training really near)\n\nSuch good news !\nAre you going to wait for me to finish the Refine Iters so that you train the entire model? I would be very happy to have the parseq model available in the next weeks :D",
                                    "url": "https://github.com/mindee/doctr/pull/1205#issuecomment-1582773128",
                                    "author": {
                                        "login": "nikokks"
                                    }
                                },
                                {
                                    "createdAt": "2023-06-08T17:49:26Z",
                                    "bodyText": "@NicolasPlaye Do you mean fully pretrain the model on mindee's internal dataset ?",
                                    "url": "https://github.com/mindee/doctr/pull/1205#issuecomment-1583090357",
                                    "author": {
                                        "login": "felixdittrich92"
                                    }
                                },
                                {
                                    "createdAt": "2023-06-08T18:02:59Z",
                                    "bodyText": "@NicolasPlaye Do you mean fully pretrain the model on mindee's internal dataset ?\n\nTraining the model on the entire internal dataset yes.\nDo you think it is feasable ?",
                                    "url": "https://github.com/mindee/doctr/pull/1205#issuecomment-1583107003",
                                    "author": {
                                        "login": "nikokks"
                                    }
                                },
                                {
                                    "createdAt": "2023-06-08T18:14:11Z",
                                    "bodyText": "@NicolasPlaye Do you mean fully pretrain the model on mindee's internal dataset ?\n\nTraining the model on the entire internal dataset yes. Do you think it is feasable ?\n\nI can't decide this \ud83d\ude05 I am not part of the mindee team.\n@odulcy-mindee what do you think ? if the model works could we add it to the end of the list ? ^^\n(Before i want also to see that Vitstr performs well)",
                                    "url": "https://github.com/mindee/doctr/pull/1205#issuecomment-1583121683",
                                    "author": {
                                        "login": "felixdittrich92"
                                    }
                                },
                                {
                                    "createdAt": "2023-06-08T22:00:33Z",
                                    "bodyText": "So if I understand, parseq would be tra\u00eened in the year ?\nWhat do you think about a training on the benchmark datasets for the moment ?",
                                    "url": "https://github.com/mindee/doctr/pull/1205#issuecomment-1583446933",
                                    "author": {
                                        "login": "nikokks"
                                    }
                                },
                                {
                                    "createdAt": "2023-06-09T11:34:36Z",
                                    "bodyText": "hi\nthis code seems to work (for refine iters)\n        # TODO: fix refine iteration\n        # One refine iteration\n        # Update query mask\n        query_mask[\n            torch.triu(\n                torch.ones(self.max_length + 1, self.max_length + 1, dtype=torch.bool, device=features.device), 2\n            )\n        ] = 0\n\n        # Prepare target input for 1 refine iteration\n        sos = torch.full((features.size(0), 1), self.vocab_size + 1, dtype=torch.long, device=features.device)\n        ys = torch.cat([sos, logits[:, :-1].argmax(-1)], dim=1)\n\n        # Create padding mask for refined target input maskes all beyond EOS token\n        target_pad_mask = (\n            ((ys != self.vocab_size).int().cumsum(-1) > 0).unsqueeze(1).unsqueeze(1)\n        )  # (N, 1, 1, max_length)\n        mask = (target_pad_mask & query_mask[:, : ys.shape[1]]).int()\n        logits = self.head(self.decode(ys, features, mask, target_query=pos_queries))\n\nwhen I do an inference on an image, it gets outputs.\nI can't know if gets good predictions because the model is not trained\nI have done this\nif True:\n     from doctr.models import ocr_predictor\n     from doctr.io import DocumentFile\n     model = ocr_predictor(det_arch='db_resnet50', reco_arch='parseq', pretrained=True)\n     single_img_doc = DocumentFile.from_images(\"../1.png\")\n     result = model(single_img_doc)\n\nwould you be interested If I (or you) train the model parseq on CUTE80, ICDAR2013,ICDAR2015,IIIT5k,SVT and SVTP.\nHere are the benchmarks: https://paperswithcode.com/paper/scene-text-recognition-with-permuted",
                                    "url": "https://github.com/mindee/doctr/pull/1205#issuecomment-1584434317",
                                    "author": {
                                        "login": "nikokks"
                                    }
                                },
                                {
                                    "createdAt": "2023-06-09T11:50:30Z",
                                    "bodyText": "@nikokks\nYou can merge my branch again pyorch should be fine now.. i have fixed the refine iter already :)\nTensorflow left and some dummy tests \ud83d\udc4d\ud83c\udffc\nFeel free to grab the pytorch version and train it so i can spent a bit more time on TF and you can test that all works ;)\nBtw.: you mean for validation ? If you want to train it you would need to go with SynthText maybe. The mentioned datasets are only for validation (much to less data to train a model from scratch)",
                                    "url": "https://github.com/mindee/doctr/pull/1205#issuecomment-1584451397",
                                    "author": {
                                        "login": "felixT2K"
                                    }
                                },
                                {
                                    "createdAt": "2023-06-09T13:48:01Z",
                                    "bodyText": "is the merge ok on my main now ?",
                                    "url": "https://github.com/mindee/doctr/pull/1205#issuecomment-1584607124",
                                    "author": {
                                        "login": "nikokks"
                                    }
                                },
                                {
                                    "createdAt": "2023-06-09T14:27:05Z",
                                    "bodyText": "@nikokks yep looks correct",
                                    "url": "https://github.com/mindee/doctr/pull/1205#issuecomment-1584669961",
                                    "author": {
                                        "login": "felixdittrich92"
                                    }
                                },
                                {
                                    "createdAt": "2023-06-11T20:35:22Z",
                                    "bodyText": "Hey @nikokks \ud83d\udc4b,\nCould you please merge my last changes into this branch and afterwards rebase on doctr main ?",
                                    "url": "https://github.com/mindee/doctr/pull/1205#issuecomment-1586328996",
                                    "author": {
                                        "login": "felixdittrich92"
                                    }
                                },
                                {
                                    "createdAt": "2023-06-13T08:50:29Z",
                                    "bodyText": "is it ok now ? :)",
                                    "url": "https://github.com/mindee/doctr/pull/1205#issuecomment-1588842640",
                                    "author": {
                                        "login": "nikokks"
                                    }
                                },
                                {
                                    "createdAt": "2023-06-13T09:00:59Z",
                                    "bodyText": "@odulcy-mindee @charlesmindee @frgfm code is ready for review\n@nikokks could you please fix the style and mypy issues ? :)",
                                    "url": "https://github.com/mindee/doctr/pull/1205#issuecomment-1588859007",
                                    "author": {
                                        "login": "felixdittrich92"
                                    }
                                },
                                {
                                    "createdAt": "2023-06-13T09:15:11Z",
                                    "bodyText": "what\n\n@odulcy-mindee @charlesmindee @frgfm code is ready for review @nikokks could you please fix the style and mypy issues ? :)\n\nwhat are the commands ?",
                                    "url": "https://github.com/mindee/doctr/pull/1205#issuecomment-1588884296",
                                    "author": {
                                        "login": "nikokks"
                                    }
                                },
                                {
                                    "createdAt": "2023-06-13T09:18:52Z",
                                    "bodyText": "what\n\n@odulcy-mindee @charlesmindee @frgfm code is ready for review @nikokks could you please fix the style and mypy issues ? :)\n\nwhat are the commands ?\n\nFor style: make style\nFor quality: make quality this will list the mypy issues you need to fix them manually\nAnd in the tensorflow recognition onnx test (last test case in the file) please set the parseq test also same as above master and sar with the min ram check",
                                    "url": "https://github.com/mindee/doctr/pull/1205#issuecomment-1588890369",
                                    "author": {
                                        "login": "felixT2K"
                                    }
                                },
                                {
                                    "createdAt": "2023-06-13T09:27:30Z",
                                    "bodyText": "make quality\nisort . -c\nSkipped 43 files\nruff check .\nblack --check .\nAll done! \u2728 \ud83c\udf70 \u2728\n246 files would be left unchanged.\nmypy doctr/\ndoctr/__init__.py:3: error: Skipping analyzing \"doctr.version\": module is installed, but missing library stubs or py.typed marker  [import]\n    from .version import __version__  # noqa: F401\n    ^\ndoctr/__init__.py:3: note: See https://mypy.readthedocs.io/en/stable/running_mypy.html#missing-imports\ndoctr/models/recognition/parseq/tensorflow.py:206: error: No overload variant of \"__add__\" of \"list\" matches argument type \"int\"  [operator]\n            combined = tf.concat([sos_idx, perms + 1, eos_idx], axis=1)\n                                           ^~~~~~~~~\ndoctr/models/recognition/parseq/tensorflow.py:206: note: Possible overload variants:\ndoctr/models/recognition/parseq/tensorflow.py:206: note:     def __add__(self, List[Any], /) -> List[Any]\ndoctr/models/recognition/parseq/tensorflow.py:206: note:     def [_S] __add__(self, List[_S], /) -> List[Union[_S, Any]]\ndoctr/models/recognition/parseq/pytorch.py:183: error: Argument 1 to \"factorial\" has incompatible type \"Union[int, float]\"; expected \"SupportsIndex\"  [arg-type]\n            max_perms = math.factorial(max_num_chars) // 2\n                                       ^~~~~~~~~~~~~\ndoctr/models/recognition/parseq/pytorch.py:192: error: Argument 1 to \"range\" has incompatible type \"Union[int, float]\"; expected \"SupportsIndex\"  [arg-type]\n                perm_pool = torch.as_tensor(list(permutations(range(max_num_chars), max_num_chars)), device=seqlen.device)[\n                                                                    ^~~~~~~~~~~~~\ndoctr/models/recognition/parseq/pytorch.py:192: error: Argument 2 to \"permutations\" has incompatible type \"Union[int, float]\"; expected \"Optional[int]\"  [arg-type]\n                perm_pool = torch.as_tensor(list(permutations(range(max_num_chars), max_num_chars)), device=seqlen.device)[\n                                                                                    ^~~~~~~~~~~~~\ndoctr/models/recognition/parseq/pytorch.py:197: error: Incompatible types in assignment (expression has type \"Tensor\", variable has type \"List[Tensor]\")  [assignment]\n                perms = torch.stack(perms)\n                        ^~~~~~~~~~~~~~~~~~\ndoctr/models/recognition/parseq/pytorch.py:200: error: Incompatible types in assignment (expression has type \"Tensor\", variable has type \"List[Tensor]\")  [assignment]\n                    perms = torch.cat([perms, perm_pool[i]])\n                            ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\ndoctr/models/recognition/parseq/pytorch.py:200: error: List item 0 has incompatible type \"List[Tensor]\"; expected \"Tensor\"  [list-item]\n                    perms = torch.cat([perms, perm_pool[i]])\n                                       ^~~~~\ndoctr/models/recognition/parseq/pytorch.py:200: error: Invalid index type \"ndarray[Any, dtype[signedinteger[_64Bit]]]\" for \"Tensor\"; expected type\n\"Union[None, int, slice, Tensor, List[Any], Tuple[Any, ...]]\"  [index]\n                    perms = torch.cat([perms, perm_pool[i]])\n                                                        ^\ndoctr/models/recognition/parseq/pytorch.py:203: error: Argument 1 to \"randperm\" has incompatible type \"Union[int, float]\"; expected \"int\"  [arg-type]\n                    [torch.randperm(max_num_chars, device=seqlen.device) for _ in range(num_gen_perms - len(perms))]\n                                    ^~~~~~~~~~~~~\ndoctr/models/recognition/parseq/pytorch.py:205: error: Incompatible types in assignment (expression has type \"Tensor\", variable has type \"List[Tensor]\")  [assignment]\n                perms = torch.stack(perms)\n                        ^~~~~~~~~~~~~~~~~~\ndoctr/models/recognition/parseq/pytorch.py:207: error: \"List[Tensor]\" has no attribute \"flip\"  [attr-defined]\n            comp = perms.flip(-1)\n                   ^~~~~~~~~~\ndoctr/models/recognition/parseq/pytorch.py:208: error: Incompatible types in assignment (expression has type \"Tensor\", variable has type \"List[Tensor]\")  [assignment]\n            perms = torch.stack([perms, comp]).transpose(0, 1).reshape(-1, max_num_chars)\n                    ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\ndoctr/models/recognition/parseq/pytorch.py:208: error: List item 0 has incompatible type \"List[Tensor]\"; expected \"Tensor\"  [list-item]\n            perms = torch.stack([perms, comp]).transpose(0, 1).reshape(-1, max_num_chars)\n                                 ^~~~~\ndoctr/models/recognition/parseq/pytorch.py:208: error: Argument 2 to \"reshape\" of \"_TensorBase\" has incompatible type \"Union[int, float]\"; expected \"int\"  [arg-type]\n            perms = torch.stack([perms, comp]).transpose(0, 1).reshape(-1, max_num_chars)\n                                                                           ^~~~~~~~~~~~~\ndoctr/models/recognition/parseq/pytorch.py:210: error: \"List[Tensor]\" has no attribute \"device\"  [attr-defined]\n            sos_idx = torch.zeros(len(perms), 1, device=perms.device)\n                                                        ^~~~~~~~~~~~\ndoctr/models/recognition/parseq/pytorch.py:211: error: \"List[Tensor]\" has no attribute \"device\"  [attr-defined]\n            eos_idx = torch.full((len(perms), 1), max_num_chars + 1, device=perms.device)\n                                                                            ^~~~~~~~~~~~\ndoctr/models/recognition/parseq/pytorch.py:212: error: No overload variant of \"__add__\" of \"list\" matches argument type \"int\"  [operator]\n            combined = torch.cat([sos_idx, perms + 1, eos_idx], dim=1).int()\n                                           ^~~~~~~~~\ndoctr/models/recognition/parseq/pytorch.py:212: note: Possible overload variants:\ndoctr/models/recognition/parseq/pytorch.py:212: note:     def __add__(self, List[Tensor], /) -> List[Tensor]\ndoctr/models/recognition/parseq/pytorch.py:212: note:     def [_S] __add__(self, List[_S], /) -> List[Union[_S, Tensor]]\ndoctr/models/recognition/parseq/pytorch.py:289: error: Incompatible types in assignment (expression has type \"Tensor\", variable has type \"List[Any]\")  [assignment]\n            logits = torch.cat(logits, dim=1)  # (N, max_length, vocab_size + 1)\n                     ^~~~~~~~~~~~~~~~~~~~~~~~\ndoctr/models/recognition/parseq/pytorch.py:301: error: \"List[Any]\" has no attribute \"argmax\"  [attr-defined]\n            ys = torch.cat([sos, logits.argmax(-1)], dim=1)\n                                 ^~~~~~~~~~~~~\ndoctr/models/recognition/parseq/pytorch.py:309: error: Incompatible return value type (got \"List[Any]\", expected \"Tensor\")  [return-value]\n            return logits  # (N, max_length, vocab_size + 1)\n                   ^~~~~~\nFound 21 errors in 3 files (checked 154 source files)\nmake: *** [Makefile:7: quality] Error 1",
                                    "url": "https://github.com/mindee/doctr/pull/1205#issuecomment-1588904466",
                                    "author": {
                                        "login": "nikokks"
                                    }
                                },
                                {
                                    "createdAt": "2023-06-13T10:11:04Z",
                                    "bodyText": "@nikokks should be fixed on my branch",
                                    "url": "https://github.com/mindee/doctr/pull/1205#issuecomment-1588987561",
                                    "author": {
                                        "login": "felixT2K"
                                    }
                                },
                                {
                                    "createdAt": "2023-06-13T11:14:07Z",
                                    "bodyText": "@nikokks should be fixed on my branch\n\nOk for me for the last commit on my branch !! =)",
                                    "url": "https://github.com/mindee/doctr/pull/1205#issuecomment-1589090403",
                                    "author": {
                                        "login": "nikokks"
                                    }
                                },
                                {
                                    "createdAt": "2023-06-13T11:34:33Z",
                                    "bodyText": "Codecov Report\n\nMerging #1205 (02bf218) into main (3bd6b3d) will decrease coverage by 1.04%.\nThe diff coverage is 79.34%.\n\n@@            Coverage Diff             @@\n##             main    #1205      +/-   ##\n==========================================\n- Coverage   94.73%   93.69%   -1.04%     \n==========================================\n  Files         150      154       +4     \n  Lines        6458     6903     +445     \n==========================================\n+ Hits         6118     6468     +350     \n- Misses        340      435      +95     \n\n\n\nFlag\nCoverage \u0394\n\n\n\n\n\nunittests\n93.69% <79.34%> (-1.04%)\n\u2b07\ufe0f\n\n\n\nFlags with carried forward coverage won't be shown. Click here to find out more.\n\n\n\nImpacted Files\nCoverage \u0394\n\n\n\n\n\ndoctr/models/recognition/parseq/tensorflow.py\n76.85% <76.85%> (\u00f8)\n\n\n\ndoctr/models/recognition/parseq/pytorch.py\n78.36% <78.36%> (\u00f8)\n\n\n\ndoctr/models/modules/transformer/pytorch.py\n100.00% <100.00%> (\u00f8)\n\n\n\ndoctr/models/modules/transformer/tensorflow.py\n99.03% <100.00%> (\u00f8)\n\n\n\ndoctr/models/recognition/__init__.py\n100.00% <100.00%> (\u00f8)\n\n\n\ndoctr/models/recognition/parseq/__init__.py\n100.00% <100.00%> (\u00f8)\n\n\n\ndoctr/models/recognition/parseq/base.py\n100.00% <100.00%> (\u00f8)\n\n\n\ndoctr/models/recognition/vitstr/base.py\n100.00% <100.00%> (\u00f8)\n\n\n\ndoctr/models/recognition/vitstr/pytorch.py\n100.00% <100.00%> (\u00f8)\n\n\n\ndoctr/models/recognition/vitstr/tensorflow.py\n97.61% <100.00%> (\u00f8)\n\n\n\n... and 1 more",
                                    "url": "https://github.com/mindee/doctr/pull/1205#issuecomment-1589122517",
                                    "author": {
                                        "login": "codecov"
                                    }
                                },
                                {
                                    "createdAt": "2023-06-13T11:48:04Z",
                                    "bodyText": "Thanks @nikokks \ud83d\udc4d now it's fine lets wait for a final review :)",
                                    "url": "https://github.com/mindee/doctr/pull/1205#issuecomment-1589144679",
                                    "author": {
                                        "login": "felixdittrich92"
                                    }
                                },
                                {
                                    "createdAt": "2023-06-14T08:42:23Z",
                                    "bodyText": "Thank you @nikokks for the PR and @felixdittrich92 for this review, I'll have a look at it today !",
                                    "url": "https://github.com/mindee/doctr/pull/1205#issuecomment-1590747344",
                                    "author": {
                                        "login": "odulcy-mindee"
                                    }
                                },
                                {
                                    "createdAt": "2023-06-14T19:44:35Z",
                                    "bodyText": "Thank you for your contribution ! Really great work! Code seems fine to me. I just have 2 questions, you can merge after that.\n@felixdittrich92 a new model to add on the training list haha\n\n\ud83d\ude48 Yes but i would keep it as the last model on the list to train (I still try to debug some things but it's really hard these models with a transformer encoder as backbone needs a ton of data \ud83e\udd72\ud83d\ude05)",
                                    "url": "https://github.com/mindee/doctr/pull/1205#issuecomment-1591875754",
                                    "author": {
                                        "login": "felixdittrich92"
                                    }
                                },
                                {
                                    "createdAt": "2023-06-14T19:55:51Z",
                                    "bodyText": "I will do some tests if i am done i will merge it :)",
                                    "url": "https://github.com/mindee/doctr/pull/1205#issuecomment-1591887489",
                                    "author": {
                                        "login": "felixdittrich92"
                                    }
                                }
                            ]
                        },
                        "reviews": {
                            "edges": [
                                {
                                    "node": {
                                        "state": "CHANGES_REQUESTED",
                                        "bodyText": "Hi @NicolasPlaye \ud83d\udc4b,\nthanks a lot for opening the PR and working on this \ud83d\udc4d\nFirst a few general comments:\n\nthe sequence building is done in the base class of a model (ref.: https://github.com/mindee/doctr/blob/main/doctr/models/recognition/vitstr/base.py) No need to add parseq/utils.py\nin the classifiaction models are our backbones so parseq should be removed\n\nSome suggestions how we can go further:\nTo add the PARSeq model you only need to add 2 files:\n\ndoctr/models/recognition/parseq/base.py (should be the same as vitstr only rename the class name)\ndoctr/models/recognition/parseq/pytorch.py (very similar to https://github.com/mindee/doctr/blob/main/doctr/models/recognition/vitstr/pytorch.py)\nin this file you need to add the decoder part of parseq for the encoder we can use our vit_s model (from classification)\n\nI would say let's start with this the other stuff is afterwards are only minor changes :)",
                                        "comments": {
                                            "edges": []
                                        }
                                    }
                                },
                                {
                                    "node": {
                                        "state": "COMMENTED",
                                        "bodyText": "",
                                        "comments": {
                                            "edges": [
                                                {
                                                    "node": {
                                                        "bodyText": "remove",
                                                        "author": {
                                                            "login": "felixdittrich92"
                                                        },
                                                        "url": "https://github.com/mindee/doctr/pull/1205#discussion_r1210085758",
                                                        "originalCommit": {
                                                            "abbreviatedOid": "abf70cb",
                                                            "authoredDate": "2023-05-30T07:58:15Z"
                                                        }
                                                    }
                                                },
                                                {
                                                    "node": {
                                                        "bodyText": "remove",
                                                        "author": {
                                                            "login": "felixdittrich92"
                                                        },
                                                        "url": "https://github.com/mindee/doctr/pull/1205#discussion_r1210086061",
                                                        "originalCommit": {
                                                            "abbreviatedOid": "abf70cb",
                                                            "authoredDate": "2023-05-30T07:58:15Z"
                                                        }
                                                    }
                                                },
                                                {
                                                    "node": {
                                                        "bodyText": "remove file -> CrossEntropySystem forward logic needs to be implemented in parseq/pytorch.py",
                                                        "author": {
                                                            "login": "felixdittrich92"
                                                        },
                                                        "url": "https://github.com/mindee/doctr/pull/1205#discussion_r1210087937",
                                                        "originalCommit": {
                                                            "abbreviatedOid": "abf70cb",
                                                            "authoredDate": "2023-05-30T07:58:15Z"
                                                        }
                                                    }
                                                },
                                                {
                                                    "node": {
                                                        "bodyText": "Decoder needs to be implemented in parseq/pytorch.py the encoder can be removed this is our feature_extractor with the vit_s backbone ref: template with todos",
                                                        "author": {
                                                            "login": "felixdittrich92"
                                                        },
                                                        "url": "https://github.com/mindee/doctr/pull/1205#discussion_r1210089948",
                                                        "originalCommit": {
                                                            "abbreviatedOid": "abf70cb",
                                                            "authoredDate": "2023-05-30T07:58:15Z"
                                                        }
                                                    }
                                                },
                                                {
                                                    "node": {
                                                        "bodyText": "tokenization is done in doctr no need for this file :)",
                                                        "author": {
                                                            "login": "felixdittrich92"
                                                        },
                                                        "url": "https://github.com/mindee/doctr/pull/1205#discussion_r1210090941",
                                                        "originalCommit": {
                                                            "abbreviatedOid": "abf70cb",
                                                            "authoredDate": "2023-05-30T07:58:15Z"
                                                        }
                                                    }
                                                },
                                                {
                                                    "node": {
                                                        "bodyText": "file can be removed we use our vit_s model as backbone ref.: template with todos",
                                                        "author": {
                                                            "login": "felixdittrich92"
                                                        },
                                                        "url": "https://github.com/mindee/doctr/pull/1205#discussion_r1210092180",
                                                        "originalCommit": {
                                                            "abbreviatedOid": "abf70cb",
                                                            "authoredDate": "2023-05-30T07:58:15Z"
                                                        }
                                                    }
                                                },
                                                {
                                                    "node": {
                                                        "bodyText": "remove",
                                                        "author": {
                                                            "login": "felixdittrich92"
                                                        },
                                                        "url": "https://github.com/mindee/doctr/pull/1205#discussion_r1210092634",
                                                        "originalCommit": {
                                                            "abbreviatedOid": "abf70cb",
                                                            "authoredDate": "2023-05-30T07:58:15Z"
                                                        }
                                                    }
                                                },
                                                {
                                                    "node": {
                                                        "bodyText": "ref.: template with todos",
                                                        "author": {
                                                            "login": "felixdittrich92"
                                                        },
                                                        "url": "https://github.com/mindee/doctr/pull/1205#discussion_r1210092944",
                                                        "originalCommit": {
                                                            "abbreviatedOid": "abf70cb",
                                                            "authoredDate": "2023-05-30T07:58:15Z"
                                                        }
                                                    }
                                                },
                                                {
                                                    "node": {
                                                        "bodyText": "that's fine \ud83d\udc4d",
                                                        "author": {
                                                            "login": "felixdittrich92"
                                                        },
                                                        "url": "https://github.com/mindee/doctr/pull/1205#discussion_r1210093573",
                                                        "originalCommit": {
                                                            "abbreviatedOid": "abf70cb",
                                                            "authoredDate": "2023-05-30T07:58:15Z"
                                                        }
                                                    }
                                                }
                                            ]
                                        }
                                    }
                                },
                                {
                                    "node": {
                                        "state": "COMMENTED",
                                        "bodyText": "",
                                        "comments": {
                                            "edges": [
                                                {
                                                    "node": {
                                                        "bodyText": "Also to remove no need to touch anything in the classification folder :D",
                                                        "author": {
                                                            "login": "felixdittrich92"
                                                        },
                                                        "url": "https://github.com/mindee/doctr/pull/1205#discussion_r1210360554",
                                                        "originalCommit": {
                                                            "abbreviatedOid": "91f76c3",
                                                            "authoredDate": "2023-05-30T14:18:06Z"
                                                        }
                                                    }
                                                }
                                            ]
                                        }
                                    }
                                },
                                {
                                    "node": {
                                        "state": "COMMENTED",
                                        "bodyText": "",
                                        "comments": {
                                            "edges": [
                                                {
                                                    "node": {
                                                        "bodyText": "remove the unused imports than this file is fine \ud83d\udc4d\norder is not related at this point :)\nchange to: self._embedding = list(vocab) + [\"<eos>\", \"<sos>\", \"<pad>\"]",
                                                        "author": {
                                                            "login": "felixdittrich92"
                                                        },
                                                        "url": "https://github.com/mindee/doctr/pull/1205#discussion_r1210372728",
                                                        "originalCommit": {
                                                            "abbreviatedOid": "91f76c3",
                                                            "authoredDate": "2023-05-30T14:18:06Z"
                                                        }
                                                    }
                                                }
                                            ]
                                        }
                                    }
                                },
                                {
                                    "node": {
                                        "state": "COMMENTED",
                                        "bodyText": "",
                                        "comments": {
                                            "edges": [
                                                {
                                                    "node": {
                                                        "bodyText": "should be vit_s from our classifiaction models instead of parseq_model :)",
                                                        "author": {
                                                            "login": "felixdittrich92"
                                                        },
                                                        "url": "https://github.com/mindee/doctr/pull/1205#discussion_r1210376045",
                                                        "originalCommit": {
                                                            "abbreviatedOid": "91f76c3",
                                                            "authoredDate": "2023-05-30T14:18:06Z"
                                                        }
                                                    }
                                                }
                                            ]
                                        }
                                    }
                                },
                                {
                                    "node": {
                                        "state": "COMMENTED",
                                        "bodyText": "",
                                        "comments": {
                                            "edges": [
                                                {
                                                    "node": {
                                                        "bodyText": "what do you mean about that ?",
                                                        "author": {
                                                            "login": "nikokks"
                                                        },
                                                        "url": "https://github.com/mindee/doctr/pull/1205#discussion_r1211748958",
                                                        "originalCommit": {
                                                            "abbreviatedOid": "abf70cb",
                                                            "authoredDate": "2023-05-30T07:58:15Z"
                                                        }
                                                    }
                                                }
                                            ]
                                        }
                                    }
                                },
                                {
                                    "node": {
                                        "state": "COMMENTED",
                                        "bodyText": "",
                                        "comments": {
                                            "edges": [
                                                {
                                                    "node": {
                                                        "bodyText": "revert this change",
                                                        "author": {
                                                            "login": "felixT2K"
                                                        },
                                                        "url": "https://github.com/mindee/doctr/pull/1205#discussion_r1222947620",
                                                        "originalCommit": {
                                                            "abbreviatedOid": "03d9a87",
                                                            "authoredDate": "2023-06-08T11:54:58Z"
                                                        }
                                                    }
                                                },
                                                {
                                                    "node": {
                                                        "bodyText": "revert this change",
                                                        "author": {
                                                            "login": "felixT2K"
                                                        },
                                                        "url": "https://github.com/mindee/doctr/pull/1205#discussion_r1222948043",
                                                        "originalCommit": {
                                                            "abbreviatedOid": "03d9a87",
                                                            "authoredDate": "2023-06-08T11:54:58Z"
                                                        }
                                                    }
                                                },
                                                {
                                                    "node": {
                                                        "bodyText": "revert this line",
                                                        "author": {
                                                            "login": "felixT2K"
                                                        },
                                                        "url": "https://github.com/mindee/doctr/pull/1205#discussion_r1222948386",
                                                        "originalCommit": {
                                                            "abbreviatedOid": "03d9a87",
                                                            "authoredDate": "2023-06-08T11:54:58Z"
                                                        }
                                                    }
                                                },
                                                {
                                                    "node": {
                                                        "bodyText": "revert this file",
                                                        "author": {
                                                            "login": "felixT2K"
                                                        },
                                                        "url": "https://github.com/mindee/doctr/pull/1205#discussion_r1222948682",
                                                        "originalCommit": {
                                                            "abbreviatedOid": "03d9a87",
                                                            "authoredDate": "2023-06-08T11:54:58Z"
                                                        }
                                                    }
                                                },
                                                {
                                                    "node": {
                                                        "bodyText": "revert this file",
                                                        "author": {
                                                            "login": "felixT2K"
                                                        },
                                                        "url": "https://github.com/mindee/doctr/pull/1205#discussion_r1222948910",
                                                        "originalCommit": {
                                                            "abbreviatedOid": "03d9a87",
                                                            "authoredDate": "2023-06-08T11:54:58Z"
                                                        }
                                                    }
                                                },
                                                {
                                                    "node": {
                                                        "bodyText": "remove this file",
                                                        "author": {
                                                            "login": "felixT2K"
                                                        },
                                                        "url": "https://github.com/mindee/doctr/pull/1205#discussion_r1222949094",
                                                        "originalCommit": {
                                                            "abbreviatedOid": "03d9a87",
                                                            "authoredDate": "2023-06-08T11:54:58Z"
                                                        }
                                                    }
                                                },
                                                {
                                                    "node": {
                                                        "bodyText": "remove this file",
                                                        "author": {
                                                            "login": "felixT2K"
                                                        },
                                                        "url": "https://github.com/mindee/doctr/pull/1205#discussion_r1222949243",
                                                        "originalCommit": {
                                                            "abbreviatedOid": "03d9a87",
                                                            "authoredDate": "2023-06-08T11:54:58Z"
                                                        }
                                                    }
                                                },
                                                {
                                                    "node": {
                                                        "bodyText": "revert this file",
                                                        "author": {
                                                            "login": "felixT2K"
                                                        },
                                                        "url": "https://github.com/mindee/doctr/pull/1205#discussion_r1222949393",
                                                        "originalCommit": {
                                                            "abbreviatedOid": "03d9a87",
                                                            "authoredDate": "2023-06-08T11:54:58Z"
                                                        }
                                                    }
                                                },
                                                {
                                                    "node": {
                                                        "bodyText": "revert this file",
                                                        "author": {
                                                            "login": "felixT2K"
                                                        },
                                                        "url": "https://github.com/mindee/doctr/pull/1205#discussion_r1222949572",
                                                        "originalCommit": {
                                                            "abbreviatedOid": "03d9a87",
                                                            "authoredDate": "2023-06-08T11:54:58Z"
                                                        }
                                                    }
                                                },
                                                {
                                                    "node": {
                                                        "bodyText": "revert this file",
                                                        "author": {
                                                            "login": "felixT2K"
                                                        },
                                                        "url": "https://github.com/mindee/doctr/pull/1205#discussion_r1222949857",
                                                        "originalCommit": {
                                                            "abbreviatedOid": "03d9a87",
                                                            "authoredDate": "2023-06-08T11:54:58Z"
                                                        }
                                                    }
                                                }
                                            ]
                                        }
                                    }
                                },
                                {
                                    "node": {
                                        "state": "CHANGES_REQUESTED",
                                        "bodyText": "@nikokks After removeing these 2 lines please merge my changes again into your branch :)\nThis should it be for the last final reviews ;)",
                                        "comments": {
                                            "edges": [
                                                {
                                                    "node": {
                                                        "bodyText": "remove this change",
                                                        "author": {
                                                            "login": "felixdittrich92"
                                                        },
                                                        "url": "https://github.com/mindee/doctr/pull/1205#discussion_r1226475734",
                                                        "originalCommit": {
                                                            "abbreviatedOid": "b2406a6",
                                                            "authoredDate": "2023-06-12T07:02:47Z"
                                                        }
                                                    }
                                                },
                                                {
                                                    "node": {
                                                        "bodyText": "remove this change",
                                                        "author": {
                                                            "login": "felixdittrich92"
                                                        },
                                                        "url": "https://github.com/mindee/doctr/pull/1205#discussion_r1226475949",
                                                        "originalCommit": {
                                                            "abbreviatedOid": "b2406a6",
                                                            "authoredDate": "2023-06-12T07:02:47Z"
                                                        }
                                                    }
                                                }
                                            ]
                                        }
                                    }
                                },
                                {
                                    "node": {
                                        "state": "COMMENTED",
                                        "bodyText": "",
                                        "comments": {
                                            "edges": [
                                                {
                                                    "node": {
                                                        "bodyText": "please remove this line (duplicate)",
                                                        "author": {
                                                            "login": "felixT2K"
                                                        },
                                                        "url": "https://github.com/mindee/doctr/pull/1205#discussion_r1227765642",
                                                        "originalCommit": {
                                                            "abbreviatedOid": "c9ac969",
                                                            "authoredDate": "2023-06-13T08:45:54Z"
                                                        }
                                                    }
                                                }
                                            ]
                                        }
                                    }
                                },
                                {
                                    "node": {
                                        "state": "COMMENTED",
                                        "bodyText": "",
                                        "comments": {
                                            "edges": [
                                                {
                                                    "node": {
                                                        "bodyText": "remove this empty line please",
                                                        "author": {
                                                            "login": "felixT2K"
                                                        },
                                                        "url": "https://github.com/mindee/doctr/pull/1205#discussion_r1227772674",
                                                        "originalCommit": {
                                                            "abbreviatedOid": "22a910e",
                                                            "authoredDate": "2023-06-13T08:49:57Z"
                                                        }
                                                    }
                                                }
                                            ]
                                        }
                                    }
                                },
                                {
                                    "node": {
                                        "state": "COMMENTED",
                                        "bodyText": "",
                                        "comments": {
                                            "edges": [
                                                {
                                                    "node": {
                                                        "bodyText": "@nikokks",
                                                        "author": {
                                                            "login": "felixT2K"
                                                        },
                                                        "url": "https://github.com/mindee/doctr/pull/1205#discussion_r1227777259",
                                                        "originalCommit": {
                                                            "abbreviatedOid": "22a910e",
                                                            "authoredDate": "2023-06-13T08:49:57Z"
                                                        }
                                                    }
                                                }
                                            ]
                                        }
                                    }
                                },
                                {
                                    "node": {
                                        "state": "COMMENTED",
                                        "bodyText": "",
                                        "comments": {
                                            "edges": [
                                                {
                                                    "node": {
                                                        "bodyText": "and run make style please",
                                                        "author": {
                                                            "login": "felixT2K"
                                                        },
                                                        "url": "https://github.com/mindee/doctr/pull/1205#discussion_r1227778162",
                                                        "originalCommit": {
                                                            "abbreviatedOid": "22a910e",
                                                            "authoredDate": "2023-06-13T08:49:57Z"
                                                        }
                                                    }
                                                }
                                            ]
                                        }
                                    }
                                },
                                {
                                    "node": {
                                        "state": "COMMENTED",
                                        "bodyText": "",
                                        "comments": {
                                            "edges": [
                                                {
                                                    "node": {
                                                        "bodyText": "after you have installed doctr dev with pip3 install -e .[dev]",
                                                        "author": {
                                                            "login": "felixT2K"
                                                        },
                                                        "url": "https://github.com/mindee/doctr/pull/1205#discussion_r1227778959",
                                                        "originalCommit": {
                                                            "abbreviatedOid": "22a910e",
                                                            "authoredDate": "2023-06-13T08:49:57Z"
                                                        }
                                                    }
                                                }
                                            ]
                                        }
                                    }
                                },
                                {
                                    "node": {
                                        "state": "COMMENTED",
                                        "bodyText": "",
                                        "comments": {
                                            "edges": [
                                                {
                                                    "node": {
                                                        "bodyText": "@nikokks still to remove this line please xD",
                                                        "author": {
                                                            "login": "felixdittrich92"
                                                        },
                                                        "url": "https://github.com/mindee/doctr/pull/1205#discussion_r1227963175",
                                                        "originalCommit": {
                                                            "abbreviatedOid": "22a910e",
                                                            "authoredDate": "2023-06-13T08:49:57Z"
                                                        }
                                                    }
                                                }
                                            ]
                                        }
                                    }
                                },
                                {
                                    "node": {
                                        "state": "COMMENTED",
                                        "bodyText": "",
                                        "comments": {
                                            "edges": [
                                                {
                                                    "node": {
                                                        "bodyText": "@nikokks  please revert this change :)",
                                                        "author": {
                                                            "login": "felixdittrich92"
                                                        },
                                                        "url": "https://github.com/mindee/doctr/pull/1205#discussion_r1227964668",
                                                        "originalCommit": {
                                                            "abbreviatedOid": "cdc1c9e",
                                                            "authoredDate": "2023-06-13T11:13:21Z"
                                                        }
                                                    }
                                                }
                                            ]
                                        }
                                    }
                                },
                                {
                                    "node": {
                                        "state": "COMMENTED",
                                        "bodyText": "Thank you for your contribution ! Really great work! Code seems fine to me.\nI just have 2 questions, you can merge after that.\n@felixdittrich92 a new model to add on the training list haha",
                                        "comments": {
                                            "edges": [
                                                {
                                                    "node": {
                                                        "bodyText": "Why are there clone calls here ?",
                                                        "author": {
                                                            "login": "odulcy-mindee"
                                                        },
                                                        "url": "https://github.com/mindee/doctr/pull/1205#discussion_r1230059362",
                                                        "originalCommit": {
                                                            "abbreviatedOid": "513fa29",
                                                            "authoredDate": "2023-06-13T11:46:02Z"
                                                        }
                                                    }
                                                },
                                                {
                                                    "node": {
                                                        "bodyText": "So we're using 0 and 1 for mask and authors used float(-inf) and 0 respectively, I'm a correct ?",
                                                        "author": {
                                                            "login": "odulcy-mindee"
                                                        },
                                                        "url": "https://github.com/mindee/doctr/pull/1205#discussion_r1230078747",
                                                        "originalCommit": {
                                                            "abbreviatedOid": "513fa29",
                                                            "authoredDate": "2023-06-13T11:46:02Z"
                                                        }
                                                    }
                                                }
                                            ]
                                        }
                                    }
                                },
                                {
                                    "node": {
                                        "state": "COMMENTED",
                                        "bodyText": "",
                                        "comments": {
                                            "edges": [
                                                {
                                                    "node": {
                                                        "bodyText": "Yep for our MHA implementation 0 is masked (transformer decoder can't \"see\" it we replace it inside scaled dot product to -inf this masking is needed for Transformer decoder otherwise the model would be able to \"cheat\") and 1 is visible the softmax activation does the rest :)\nTo overcome the question why we don't set it directly to -inf\nShort answer: this would raise problems on ONNX exporting :)",
                                                        "author": {
                                                            "login": "felixdittrich92"
                                                        },
                                                        "url": "https://github.com/mindee/doctr/pull/1205#discussion_r1230092922",
                                                        "originalCommit": {
                                                            "abbreviatedOid": "513fa29",
                                                            "authoredDate": "2023-06-13T11:46:02Z"
                                                        }
                                                    }
                                                }
                                            ]
                                        }
                                    }
                                },
                                {
                                    "node": {
                                        "state": "COMMENTED",
                                        "bodyText": "",
                                        "comments": {
                                            "edges": [
                                                {
                                                    "node": {
                                                        "bodyText": "Pytorch does not allow overriding inplace because it would raise problems in CUDA :)\nThe other Option would be to use 2 variables but i personally like the clone()\nway -> minimizing to code a few lines \ud83d\ude05",
                                                        "author": {
                                                            "login": "felixdittrich92"
                                                        },
                                                        "url": "https://github.com/mindee/doctr/pull/1205#discussion_r1230096878",
                                                        "originalCommit": {
                                                            "abbreviatedOid": "513fa29",
                                                            "authoredDate": "2023-06-13T11:46:02Z"
                                                        }
                                                    }
                                                }
                                            ]
                                        }
                                    }
                                },
                                {
                                    "node": {
                                        "state": "DISMISSED",
                                        "bodyText": "Okey, thanks for the answers !",
                                        "comments": {
                                            "edges": []
                                        }
                                    }
                                },
                                {
                                    "node": {
                                        "state": "COMMENTED",
                                        "bodyText": "",
                                        "comments": {
                                            "edges": [
                                                {
                                                    "node": {
                                                        "bodyText": "You can do an only one clone before line 97 and remove the other clones. I have tried before it worked. To verify to be sure. With that it should speed up processing.",
                                                        "author": {
                                                            "login": "nikokks"
                                                        },
                                                        "url": "https://github.com/mindee/doctr/pull/1205#discussion_r1230107453",
                                                        "originalCommit": {
                                                            "abbreviatedOid": "513fa29",
                                                            "authoredDate": "2023-06-13T11:46:02Z"
                                                        }
                                                    }
                                                }
                                            ]
                                        }
                                    }
                                },
                                {
                                    "node": {
                                        "state": "APPROVED",
                                        "bodyText": "",
                                        "comments": {
                                            "edges": []
                                        }
                                    }
                                },
                                {
                                    "node": {
                                        "state": "COMMENTED",
                                        "bodyText": "Hello, I'm the original author of PARSeq. @felixdittrich92 asked me to review your implementation. I'll add my comments here even though the PR is already closed.\nOverall, it looks correct except for the masking + training loop. Good job and thanks for this initiative. :)",
                                        "comments": {
                                            "edges": [
                                                {
                                                    "node": {
                                                        "bodyText": "Why does target determine whether the inference mode is AR (target is None) or NAR (target is not None)?",
                                                        "author": {
                                                            "login": "baudm"
                                                        },
                                                        "url": "https://github.com/mindee/doctr/pull/1205#discussion_r1232619579",
                                                        "originalCommit": {
                                                            "abbreviatedOid": "02bf218",
                                                            "authoredDate": "2023-06-15T10:27:43Z"
                                                        }
                                                    }
                                                },
                                                {
                                                    "node": {
                                                        "bodyText": "target_mask, as generated by generate_permutations_attention_masks(), cannot be ANDed with the padding_mask. The mask generated from a permutation is shared across all sequences (shape: (max_len, max_len)), while the padding_mask varies for each sequence (shape: (N, max_len)). If you want to AND both masks, you have to tile the target_mask for each sequence such that it becomes (N, max_len, max_len), and tile padding_mask for each character output position, i.e. reshape to (N, 1, max_len) first then tile such that it becomes the same shape.\nPersonally, at least for PyTorch, it would be better to use the padding_mask and target_mask separately since this is handled automatically by the native MHA implementation.",
                                                        "author": {
                                                            "login": "baudm"
                                                        },
                                                        "url": "https://github.com/mindee/doctr/pull/1205#discussion_r1232623059",
                                                        "originalCommit": {
                                                            "abbreviatedOid": "02bf218",
                                                            "authoredDate": "2023-06-15T10:27:43Z"
                                                        }
                                                    }
                                                },
                                                {
                                                    "node": {
                                                        "bodyText": "This should be inside the loop where logits is computed. The model loss is the mean cross-entropy loss across all permutations. There's also a small detail about the computation of losses for [E] (where it is replaced by [P] after the forward and backward permutations). Please check the reference implementation:\nhttps://github.com/baudm/parseq/blob/bc8d95cda4666d32fa53daf2ea97ff712b71e7c7/strhub/models/parseq/system.py#L242-L256",
                                                        "author": {
                                                            "login": "baudm"
                                                        },
                                                        "url": "https://github.com/mindee/doctr/pull/1205#discussion_r1232627965",
                                                        "originalCommit": {
                                                            "abbreviatedOid": "02bf218",
                                                            "authoredDate": "2023-06-15T10:27:43Z"
                                                        }
                                                    }
                                                },
                                                {
                                                    "node": {
                                                        "bodyText": "The refinement process can be done regardless of the initial decoding scheme (AR or NAR). I suggest moving this to a separate method so it can be used by either AR or NAR decoding.",
                                                        "author": {
                                                            "login": "baudm"
                                                        },
                                                        "url": "https://github.com/mindee/doctr/pull/1205#discussion_r1232636515",
                                                        "originalCommit": {
                                                            "abbreviatedOid": "02bf218",
                                                            "authoredDate": "2023-06-15T10:27:43Z"
                                                        }
                                                    }
                                                }
                                            ]
                                        }
                                    }
                                },
                                {
                                    "node": {
                                        "state": "COMMENTED",
                                        "bodyText": "",
                                        "comments": {
                                            "edges": [
                                                {
                                                    "node": {
                                                        "bodyText": "Hey \ud83d\udc4b we have already changed it in https://github.com/felixdittrich92/doctr/tree/parseq-fixes (was more for F\u00fcrther debugging) :)",
                                                        "author": {
                                                            "login": "felixdittrich92"
                                                        },
                                                        "url": "https://github.com/mindee/doctr/pull/1205#discussion_r1233015212",
                                                        "originalCommit": {
                                                            "abbreviatedOid": "02bf218",
                                                            "authoredDate": "2023-06-15T10:27:43Z"
                                                        }
                                                    }
                                                }
                                            ]
                                        }
                                    }
                                },
                                {
                                    "node": {
                                        "state": "COMMENTED",
                                        "bodyText": "",
                                        "comments": {
                                            "edges": [
                                                {
                                                    "node": {
                                                        "bodyText": "Alright got it thanks \ud83d\udc4d",
                                                        "author": {
                                                            "login": "felixdittrich92"
                                                        },
                                                        "url": "https://github.com/mindee/doctr/pull/1205#discussion_r1233015290",
                                                        "originalCommit": {
                                                            "abbreviatedOid": "02bf218",
                                                            "authoredDate": "2023-06-15T10:27:43Z"
                                                        }
                                                    }
                                                }
                                            ]
                                        }
                                    }
                                },
                                {
                                    "node": {
                                        "state": "COMMENTED",
                                        "bodyText": "",
                                        "comments": {
                                            "edges": [
                                                {
                                                    "node": {
                                                        "bodyText": "\ud83d\udc4d",
                                                        "author": {
                                                            "login": "felixdittrich92"
                                                        },
                                                        "url": "https://github.com/mindee/doctr/pull/1205#discussion_r1233015343",
                                                        "originalCommit": {
                                                            "abbreviatedOid": "02bf218",
                                                            "authoredDate": "2023-06-15T10:27:43Z"
                                                        }
                                                    }
                                                }
                                            ]
                                        }
                                    }
                                },
                                {
                                    "node": {
                                        "state": "COMMENTED",
                                        "bodyText": "",
                                        "comments": {
                                            "edges": [
                                                {
                                                    "node": {
                                                        "bodyText": "@baudm but as you can see we use our own MHA implementation (it is a bit slower as the native implementation of course) but in the past we have had some trouble with the native implementation especially with onnx and it makes it much easier for us to port it to Tensorflow :)",
                                                        "author": {
                                                            "login": "felixdittrich92"
                                                        },
                                                        "url": "https://github.com/mindee/doctr/pull/1205#discussion_r1233022484",
                                                        "originalCommit": {
                                                            "abbreviatedOid": "02bf218",
                                                            "authoredDate": "2023-06-15T10:27:43Z"
                                                        }
                                                    }
                                                }
                                            ]
                                        }
                                    }
                                },
                                {
                                    "node": {
                                        "state": "COMMENTED",
                                        "bodyText": "",
                                        "comments": {
                                            "edges": [
                                                {
                                                    "node": {
                                                        "bodyText": "Pseudocode (not tested):\ntiled_target_mask = target_mask.unsqueeze(0).repeat(gt.shape[0], 1, 1)  # (N, max_len, max_len)\n # tile padding mask for each character output position\ntiled_padding_mask = padding_mask.reshape(gt.shape[0], 1, -1).repeat(1, tiled_target_mask.shape[1], 1)  # (N, max_len, max_len)\n# reshape to (N, max_len, max_len) to match the shape of tiled_target_mask\ntiled_target_mask = tiled_target_mask.reshape(gt.shape[0], 1, -1)\n# combine target padding mask and query mask\nmask = (tiled_target_mask & tiled_padding_mask).int()\n\n@nikokks\n@baudm correct me if i am wrong :)",
                                                        "author": {
                                                            "login": "felixdittrich92"
                                                        },
                                                        "url": "https://github.com/mindee/doctr/pull/1205#discussion_r1233023764",
                                                        "originalCommit": {
                                                            "abbreviatedOid": "02bf218",
                                                            "authoredDate": "2023-06-15T10:27:43Z"
                                                        }
                                                    }
                                                }
                                            ]
                                        }
                                    }
                                },
                                {
                                    "node": {
                                        "state": "COMMENTED",
                                        "bodyText": "",
                                        "comments": {
                                            "edges": [
                                                {
                                                    "node": {
                                                        "bodyText": "@nikokks this should be already handled by our ce_mask but i will check it next week",
                                                        "author": {
                                                            "login": "felixdittrich92"
                                                        },
                                                        "url": "https://github.com/mindee/doctr/pull/1205#discussion_r1233024116",
                                                        "originalCommit": {
                                                            "abbreviatedOid": "02bf218",
                                                            "authoredDate": "2023-06-15T10:27:43Z"
                                                        }
                                                    }
                                                }
                                            ]
                                        }
                                    }
                                },
                                {
                                    "node": {
                                        "state": "COMMENTED",
                                        "bodyText": "",
                                        "comments": {
                                            "edges": [
                                                {
                                                    "node": {
                                                        "bodyText": "@nikokks in this case we need to remove the padding inside generate_permutations function (See function return)",
                                                        "author": {
                                                            "login": "felixdittrich92"
                                                        },
                                                        "url": "https://github.com/mindee/doctr/pull/1205#discussion_r1233024520",
                                                        "originalCommit": {
                                                            "abbreviatedOid": "02bf218",
                                                            "authoredDate": "2023-06-15T10:27:43Z"
                                                        }
                                                    }
                                                }
                                            ]
                                        }
                                    }
                                },
                                {
                                    "node": {
                                        "state": "COMMENTED",
                                        "bodyText": "",
                                        "comments": {
                                            "edges": [
                                                {
                                                    "node": {
                                                        "bodyText": "@baudm Does it have any impact if we pad the permutation list to max_length with the eos char to ensure size unified attention masks ?",
                                                        "author": {
                                                            "login": "felixdittrich92"
                                                        },
                                                        "url": "https://github.com/mindee/doctr/pull/1205#discussion_r1233303519",
                                                        "originalCommit": {
                                                            "abbreviatedOid": "02bf218",
                                                            "authoredDate": "2023-06-15T10:27:43Z"
                                                        }
                                                    }
                                                }
                                            ]
                                        }
                                    }
                                },
                                {
                                    "node": {
                                        "state": "COMMENTED",
                                        "bodyText": "",
                                        "comments": {
                                            "edges": [
                                                {
                                                    "node": {
                                                        "bodyText": "Still stucking \ud83d\ude05\nAnother problem is how can we pad it to self.max_length\n# Create padding mask for target input\n# [True, True, True, ..., False, False, False] -> False is masked\npadding_mask = ~(((gt == self.vocab_size + 2) | (gt == self.vocab_size)).int().cumsum(-1) > 0)\n\ntorch.set_printoptions(profile=\"full\")\nif self.training:\n    # Generate permutations for the target sequences\n    tgt_perms = self.generate_permutations(seq_len)\n    print(f\"Permutations: {tgt_perms}\")\n\n    loss = 0\n    for perm in tgt_perms:\n        print(f\"Permutation: {perm}\")\n        # Generate attention mask for the permutation\n        _, target_mask = self.generate_permutations_attention_masks(perm)\n        key_padding_mask_expanded = padding_mask[:, :target_mask.shape[-1]].view(features.shape[0], 1, 1, target_mask.shape[-1]).expand(-1, 1, -1, -1)\n        print(f\"key_padding_mask_expanded shape: {key_padding_mask_expanded.shape}\")\n        print(f\"key_padding_mask_expanded: \\n{key_padding_mask_expanded}\")\n        target_mask = target_mask.view(1, 1, target_mask.shape[-1], target_mask.shape[-1]).expand(features.shape[0], 1, -1, -1)\n        print(f\"target_mask shape: {target_mask.shape}\")\n        print(f\"target_mask: \\n{target_mask}\")\n        mask = (key_padding_mask_expanded.bool() & target_mask.bool()).int()\n        print(f\"mask shape: {mask.shape}\")\n        print(f\"mask: \\n{mask}\")\n\n        logits = self.head(self.decode(gt[:, :target_mask.shape[-1]], features, mask))  # (N, max_length, vocab_size + 1)\n        print(f\"logits shape: {logits.shape}\")\n\nPermutations: tensor([[0, 1, 2, 3, 4, 5, 6, 7],\n        [0, 7, 6, 5, 4, 3, 2, 1],\n        [0, 3, 6, 2, 5, 1, 4, 7],\n        [0, 4, 1, 5, 2, 6, 3, 7],\n        [0, 1, 6, 3, 5, 2, 4, 7],\n        [0, 4, 2, 5, 3, 6, 1, 7]], device='cuda:0', dtype=torch.int32)\nPermutation: tensor([0, 1, 2, 3, 4, 5, 6, 7], device='cuda:0', dtype=torch.int32)\nkey_padding_mask_expanded shape: torch.Size([3, 1, 1, 7])\nkey_padding_mask_expanded: \ntensor([[[[ True,  True,  True,  True,  True,  True, False]]],\n\n\n        [[[ True,  True,  True, False, False, False, False]]],\n\n\n        [[[ True,  True,  True, False, False, False, False]]]],\n       device='cuda:0')\ntarget_mask shape: torch.Size([3, 1, 7, 7])\ntarget_mask: \ntensor([[[[1, 0, 0, 0, 0, 0, 0],\n          [1, 1, 0, 0, 0, 0, 0],\n          [1, 1, 1, 0, 0, 0, 0],\n          [1, 1, 1, 1, 0, 0, 0],\n          [1, 1, 1, 1, 1, 0, 0],\n          [1, 1, 1, 1, 1, 1, 0],\n          [1, 1, 1, 1, 1, 1, 1]]],\n\n\n        [[[1, 0, 0, 0, 0, 0, 0],\n          [1, 1, 0, 0, 0, 0, 0],\n          [1, 1, 1, 0, 0, 0, 0],\n          [1, 1, 1, 1, 0, 0, 0],\n          [1, 1, 1, 1, 1, 0, 0],\n          [1, 1, 1, 1, 1, 1, 0],\n          [1, 1, 1, 1, 1, 1, 1]]],\n\n\n        [[[1, 0, 0, 0, 0, 0, 0],\n          [1, 1, 0, 0, 0, 0, 0],\n          [1, 1, 1, 0, 0, 0, 0],\n          [1, 1, 1, 1, 0, 0, 0],\n          [1, 1, 1, 1, 1, 0, 0],\n          [1, 1, 1, 1, 1, 1, 0],\n          [1, 1, 1, 1, 1, 1, 1]]]], device='cuda:0', dtype=torch.int32)\nmask shape: torch.Size([3, 1, 7, 7])\nmask: \ntensor([[[[1, 0, 0, 0, 0, 0, 0],\n          [1, 1, 0, 0, 0, 0, 0],\n          [1, 1, 1, 0, 0, 0, 0],\n          [1, 1, 1, 1, 0, 0, 0],\n          [1, 1, 1, 1, 1, 0, 0],\n          [1, 1, 1, 1, 1, 1, 0],\n          [1, 1, 1, 1, 1, 1, 0]]],\n\n\n        [[[1, 0, 0, 0, 0, 0, 0],\n          [1, 1, 0, 0, 0, 0, 0],\n          [1, 1, 1, 0, 0, 0, 0],\n          [1, 1, 1, 0, 0, 0, 0],\n          [1, 1, 1, 0, 0, 0, 0],\n          [1, 1, 1, 0, 0, 0, 0],\n          [1, 1, 1, 0, 0, 0, 0]]],\n\n\n        [[[1, 0, 0, 0, 0, 0, 0],\n          [1, 1, 0, 0, 0, 0, 0],\n          [1, 1, 1, 0, 0, 0, 0],\n          [1, 1, 1, 0, 0, 0, 0],\n          [1, 1, 1, 0, 0, 0, 0],\n          [1, 1, 1, 0, 0, 0, 0],\n          [1, 1, 1, 0, 0, 0, 0]]]], device='cuda:0', dtype=torch.int32)\nlogits shape: torch.Size([3, 7, 127])\nPermutation: tensor([0, 7, 6, 5, 4, 3, 2, 1], device='cuda:0', dtype=torch.int32)\nkey_padding_mask_expanded shape: torch.Size([3, 1, 1, 7])\nkey_padding_mask_expanded: \ntensor([[[[ True,  True,  True,  True,  True,  True, False]]],\n\n\n        [[[ True,  True,  True, False, False, False, False]]],\n\n\n        [[[ True,  True,  True, False, False, False, False]]]],\n       device='cuda:0')\ntarget_mask shape: torch.Size([3, 1, 7, 7])\ntarget_mask: \ntensor([[[[1, 0, 1, 1, 1, 1, 1],\n          [1, 0, 0, 1, 1, 1, 1],\n          [1, 0, 0, 0, 1, 1, 1],\n          [1, 0, 0, 0, 0, 1, 1],\n          [1, 0, 0, 0, 0, 0, 1],\n          [1, 0, 0, 0, 0, 0, 0],\n          [1, 0, 0, 0, 0, 0, 0]]],\n\n\n        [[[1, 0, 1, 1, 1, 1, 1],\n          [1, 0, 0, 1, 1, 1, 1],\n          [1, 0, 0, 0, 1, 1, 1],\n          [1, 0, 0, 0, 0, 1, 1],\n          [1, 0, 0, 0, 0, 0, 1],\n          [1, 0, 0, 0, 0, 0, 0],\n          [1, 0, 0, 0, 0, 0, 0]]],\n\n\n        [[[1, 0, 1, 1, 1, 1, 1],\n          [1, 0, 0, 1, 1, 1, 1],\n          [1, 0, 0, 0, 1, 1, 1],\n          [1, 0, 0, 0, 0, 1, 1],\n          [1, 0, 0, 0, 0, 0, 1],\n          [1, 0, 0, 0, 0, 0, 0],\n          [1, 0, 0, 0, 0, 0, 0]]]], device='cuda:0', dtype=torch.int32)\nmask shape: torch.Size([3, 1, 7, 7])\nmask: \ntensor([[[[1, 0, 1, 1, 1, 1, 0],\n          [1, 0, 0, 1, 1, 1, 0],\n          [1, 0, 0, 0, 1, 1, 0],\n          [1, 0, 0, 0, 0, 1, 0],\n          [1, 0, 0, 0, 0, 0, 0],\n          [1, 0, 0, 0, 0, 0, 0],\n          [1, 0, 0, 0, 0, 0, 0]]],\n\n\n        [[[1, 0, 1, 0, 0, 0, 0],\n          [1, 0, 0, 0, 0, 0, 0],\n          [1, 0, 0, 0, 0, 0, 0],\n          [1, 0, 0, 0, 0, 0, 0],\n          [1, 0, 0, 0, 0, 0, 0],\n          [1, 0, 0, 0, 0, 0, 0],\n          [1, 0, 0, 0, 0, 0, 0]]],\n\n\n        [[[1, 0, 1, 0, 0, 0, 0],\n          [1, 0, 0, 0, 0, 0, 0],\n          [1, 0, 0, 0, 0, 0, 0],\n          [1, 0, 0, 0, 0, 0, 0],\n          [1, 0, 0, 0, 0, 0, 0],\n          [1, 0, 0, 0, 0, 0, 0],\n          [1, 0, 0, 0, 0, 0, 0]]]], device='cuda:0', dtype=torch.int32)\nlogits shape: torch.Size([3, 7, 127])\nPermutation: tensor([0, 3, 6, 2, 5, 1, 4, 7], device='cuda:0', dtype=torch.int32)\nkey_padding_mask_expanded shape: torch.Size([3, 1, 1, 7])\nkey_padding_mask_expanded: \ntensor([[[[ True,  True,  True,  True,  True,  True, False]]],\n\n\n        [[[ True,  True,  True, False, False, False, False]]],\n\n\n        [[[ True,  True,  True, False, False, False, False]]]],\n       device='cuda:0')\ntarget_mask shape: torch.Size([3, 1, 7, 7])\ntarget_mask: \ntensor([[[[1, 0, 1, 1, 0, 1, 1],\n          [1, 0, 0, 1, 0, 0, 1],\n          [1, 0, 0, 0, 0, 0, 0],\n          [1, 1, 1, 1, 0, 1, 1],\n          [1, 0, 1, 1, 0, 0, 1],\n          [1, 0, 0, 1, 0, 0, 0],\n          [1, 1, 1, 1, 1, 1, 1]]],\n\n\n        [[[1, 0, 1, 1, 0, 1, 1],\n          [1, 0, 0, 1, 0, 0, 1],\n          [1, 0, 0, 0, 0, 0, 0],\n          [1, 1, 1, 1, 0, 1, 1],\n          [1, 0, 1, 1, 0, 0, 1],\n          [1, 0, 0, 1, 0, 0, 0],\n          [1, 1, 1, 1, 1, 1, 1]]],\n\n\n        [[[1, 0, 1, 1, 0, 1, 1],\n          [1, 0, 0, 1, 0, 0, 1],\n          [1, 0, 0, 0, 0, 0, 0],\n          [1, 1, 1, 1, 0, 1, 1],\n          [1, 0, 1, 1, 0, 0, 1],\n          [1, 0, 0, 1, 0, 0, 0],\n          [1, 1, 1, 1, 1, 1, 1]]]], device='cuda:0', dtype=torch.int32)\nmask shape: torch.Size([3, 1, 7, 7])\nmask: \ntensor([[[[1, 0, 1, 1, 0, 1, 0],\n          [1, 0, 0, 1, 0, 0, 0],\n          [1, 0, 0, 0, 0, 0, 0],\n          [1, 1, 1, 1, 0, 1, 0],\n          [1, 0, 1, 1, 0, 0, 0],\n          [1, 0, 0, 1, 0, 0, 0],\n          [1, 1, 1, 1, 1, 1, 0]]],\n\n\n        [[[1, 0, 1, 0, 0, 0, 0],\n          [1, 0, 0, 0, 0, 0, 0],\n          [1, 0, 0, 0, 0, 0, 0],\n          [1, 1, 1, 0, 0, 0, 0],\n          [1, 0, 1, 0, 0, 0, 0],\n          [1, 0, 0, 0, 0, 0, 0],\n          [1, 1, 1, 0, 0, 0, 0]]],\n\n\n        [[[1, 0, 1, 0, 0, 0, 0],\n          [1, 0, 0, 0, 0, 0, 0],\n          [1, 0, 0, 0, 0, 0, 0],\n          [1, 1, 1, 0, 0, 0, 0],\n          [1, 0, 1, 0, 0, 0, 0],\n          [1, 0, 0, 0, 0, 0, 0],\n          [1, 1, 1, 0, 0, 0, 0]]]], device='cuda:0', dtype=torch.int32)\nlogits shape: torch.Size([3, 7, 127])\nPermutation: tensor([0, 4, 1, 5, 2, 6, 3, 7], device='cuda:0', dtype=torch.int32)\nkey_padding_mask_expanded shape: torch.Size([3, 1, 1, 7])\nkey_padding_mask_expanded: \ntensor([[[[ True,  True,  True,  True,  True,  True, False]]],\n\n\n        [[[ True,  True,  True, False, False, False, False]]],\n\n\n        [[[ True,  True,  True, False, False, False, False]]]],\n       device='cuda:0')\ntarget_mask shape: torch.Size([3, 1, 7, 7])\ntarget_mask: \ntensor([[[[1, 0, 0, 0, 1, 0, 0],\n          [1, 1, 0, 0, 1, 1, 0],\n          [1, 1, 1, 0, 1, 1, 1],\n          [1, 0, 0, 0, 0, 0, 0],\n          [1, 1, 0, 0, 1, 0, 0],\n          [1, 1, 1, 0, 1, 1, 0],\n          [1, 1, 1, 1, 1, 1, 1]]],\n\n\n        [[[1, 0, 0, 0, 1, 0, 0],\n          [1, 1, 0, 0, 1, 1, 0],\n          [1, 1, 1, 0, 1, 1, 1],\n          [1, 0, 0, 0, 0, 0, 0],\n          [1, 1, 0, 0, 1, 0, 0],\n          [1, 1, 1, 0, 1, 1, 0],\n          [1, 1, 1, 1, 1, 1, 1]]],\n\n\n        [[[1, 0, 0, 0, 1, 0, 0],\n          [1, 1, 0, 0, 1, 1, 0],\n          [1, 1, 1, 0, 1, 1, 1],\n          [1, 0, 0, 0, 0, 0, 0],\n          [1, 1, 0, 0, 1, 0, 0],\n          [1, 1, 1, 0, 1, 1, 0],\n          [1, 1, 1, 1, 1, 1, 1]]]], device='cuda:0', dtype=torch.int32)\nmask shape: torch.Size([3, 1, 7, 7])\nmask: \ntensor([[[[1, 0, 0, 0, 1, 0, 0],\n          [1, 1, 0, 0, 1, 1, 0],\n          [1, 1, 1, 0, 1, 1, 0],\n          [1, 0, 0, 0, 0, 0, 0],\n          [1, 1, 0, 0, 1, 0, 0],\n          [1, 1, 1, 0, 1, 1, 0],\n          [1, 1, 1, 1, 1, 1, 0]]],\n\n\n        [[[1, 0, 0, 0, 0, 0, 0],\n          [1, 1, 0, 0, 0, 0, 0],\n          [1, 1, 1, 0, 0, 0, 0],\n          [1, 0, 0, 0, 0, 0, 0],\n          [1, 1, 0, 0, 0, 0, 0],\n          [1, 1, 1, 0, 0, 0, 0],\n          [1, 1, 1, 0, 0, 0, 0]]],\n\n\n        [[[1, 0, 0, 0, 0, 0, 0],\n          [1, 1, 0, 0, 0, 0, 0],\n          [1, 1, 1, 0, 0, 0, 0],\n          [1, 0, 0, 0, 0, 0, 0],\n          [1, 1, 0, 0, 0, 0, 0],\n          [1, 1, 1, 0, 0, 0, 0],\n          [1, 1, 1, 0, 0, 0, 0]]]], device='cuda:0', dtype=torch.int32)\nlogits shape: torch.Size([3, 7, 127])\nPermutation: tensor([0, 1, 6, 3, 5, 2, 4, 7], device='cuda:0', dtype=torch.int32)\nkey_padding_mask_expanded shape: torch.Size([3, 1, 1, 7])\nkey_padding_mask_expanded: \ntensor([[[[ True,  True,  True,  True,  True,  True, False]]],\n\n\n        [[[ True,  True,  True, False, False, False, False]]],\n\n\n        [[[ True,  True,  True, False, False, False, False]]]],\n       device='cuda:0')\ntarget_mask shape: torch.Size([3, 1, 7, 7])\ntarget_mask: \ntensor([[[[1, 0, 0, 0, 0, 0, 0],\n          [1, 1, 0, 1, 0, 1, 1],\n          [1, 1, 0, 0, 0, 0, 1],\n          [1, 1, 1, 1, 0, 1, 1],\n          [1, 1, 0, 1, 0, 0, 1],\n          [1, 1, 0, 0, 0, 0, 0],\n          [1, 1, 1, 1, 1, 1, 1]]],\n\n\n        [[[1, 0, 0, 0, 0, 0, 0],\n          [1, 1, 0, 1, 0, 1, 1],\n          [1, 1, 0, 0, 0, 0, 1],\n          [1, 1, 1, 1, 0, 1, 1],\n          [1, 1, 0, 1, 0, 0, 1],\n          [1, 1, 0, 0, 0, 0, 0],\n          [1, 1, 1, 1, 1, 1, 1]]],\n\n\n        [[[1, 0, 0, 0, 0, 0, 0],\n          [1, 1, 0, 1, 0, 1, 1],\n          [1, 1, 0, 0, 0, 0, 1],\n          [1, 1, 1, 1, 0, 1, 1],\n          [1, 1, 0, 1, 0, 0, 1],\n          [1, 1, 0, 0, 0, 0, 0],\n          [1, 1, 1, 1, 1, 1, 1]]]], device='cuda:0', dtype=torch.int32)\nmask shape: torch.Size([3, 1, 7, 7])\nmask: \ntensor([[[[1, 0, 0, 0, 0, 0, 0],\n          [1, 1, 0, 1, 0, 1, 0],\n          [1, 1, 0, 0, 0, 0, 0],\n          [1, 1, 1, 1, 0, 1, 0],\n          [1, 1, 0, 1, 0, 0, 0],\n          [1, 1, 0, 0, 0, 0, 0],\n          [1, 1, 1, 1, 1, 1, 0]]],\n\n\n        [[[1, 0, 0, 0, 0, 0, 0],\n          [1, 1, 0, 0, 0, 0, 0],\n          [1, 1, 0, 0, 0, 0, 0],\n          [1, 1, 1, 0, 0, 0, 0],\n          [1, 1, 0, 0, 0, 0, 0],\n          [1, 1, 0, 0, 0, 0, 0],\n          [1, 1, 1, 0, 0, 0, 0]]],\n\n\n        [[[1, 0, 0, 0, 0, 0, 0],\n          [1, 1, 0, 0, 0, 0, 0],\n          [1, 1, 0, 0, 0, 0, 0],\n          [1, 1, 1, 0, 0, 0, 0],\n          [1, 1, 0, 0, 0, 0, 0],\n          [1, 1, 0, 0, 0, 0, 0],\n          [1, 1, 1, 0, 0, 0, 0]]]], device='cuda:0', dtype=torch.int32)\nlogits shape: torch.Size([3, 7, 127])\nPermutation: tensor([0, 4, 2, 5, 3, 6, 1, 7], device='cuda:0', dtype=torch.int32)\nkey_padding_mask_expanded shape: torch.Size([3, 1, 1, 7])\nkey_padding_mask_expanded: \ntensor([[[[ True,  True,  True,  True,  True,  True, False]]],\n\n\n        [[[ True,  True,  True, False, False, False, False]]],\n\n\n        [[[ True,  True,  True, False, False, False, False]]]],\n       device='cuda:0')\ntarget_mask shape: torch.Size([3, 1, 7, 7])\ntarget_mask: \ntensor([[[[1, 0, 1, 1, 1, 1, 1],\n          [1, 0, 0, 0, 1, 0, 0],\n          [1, 0, 1, 0, 1, 1, 0],\n          [1, 0, 0, 0, 0, 0, 0],\n          [1, 0, 1, 0, 1, 0, 0],\n          [1, 0, 1, 1, 1, 1, 0],\n          [1, 1, 1, 1, 1, 1, 1]]],\n\n\n        [[[1, 0, 1, 1, 1, 1, 1],\n          [1, 0, 0, 0, 1, 0, 0],\n          [1, 0, 1, 0, 1, 1, 0],\n          [1, 0, 0, 0, 0, 0, 0],\n          [1, 0, 1, 0, 1, 0, 0],\n          [1, 0, 1, 1, 1, 1, 0],\n          [1, 1, 1, 1, 1, 1, 1]]],\n\n\n        [[[1, 0, 1, 1, 1, 1, 1],\n          [1, 0, 0, 0, 1, 0, 0],\n          [1, 0, 1, 0, 1, 1, 0],\n          [1, 0, 0, 0, 0, 0, 0],\n          [1, 0, 1, 0, 1, 0, 0],\n          [1, 0, 1, 1, 1, 1, 0],\n          [1, 1, 1, 1, 1, 1, 1]]]], device='cuda:0', dtype=torch.int32)\nmask shape: torch.Size([3, 1, 7, 7])\nmask: \ntensor([[[[1, 0, 1, 1, 1, 1, 0],\n          [1, 0, 0, 0, 1, 0, 0],\n          [1, 0, 1, 0, 1, 1, 0],\n          [1, 0, 0, 0, 0, 0, 0],\n          [1, 0, 1, 0, 1, 0, 0],\n          [1, 0, 1, 1, 1, 1, 0],\n          [1, 1, 1, 1, 1, 1, 0]]],\n\n\n        [[[1, 0, 1, 0, 0, 0, 0],\n          [1, 0, 0, 0, 0, 0, 0],\n          [1, 0, 1, 0, 0, 0, 0],\n          [1, 0, 0, 0, 0, 0, 0],\n          [1, 0, 1, 0, 0, 0, 0],\n          [1, 0, 1, 0, 0, 0, 0],\n          [1, 1, 1, 0, 0, 0, 0]]],\n\n\n        [[[1, 0, 1, 0, 0, 0, 0],\n          [1, 0, 0, 0, 0, 0, 0],\n          [1, 0, 1, 0, 0, 0, 0],\n          [1, 0, 0, 0, 0, 0, 0],\n          [1, 0, 1, 0, 0, 0, 0],\n          [1, 0, 1, 0, 0, 0, 0],\n          [1, 1, 1, 0, 0, 0, 0]]]], device='cuda:0', dtype=torch.int32)\nlogits shape: torch.Size([3, 7, 127])",
                                                        "author": {
                                                            "login": "felixdittrich92"
                                                        },
                                                        "url": "https://github.com/mindee/doctr/pull/1205#discussion_r1234164527",
                                                        "originalCommit": {
                                                            "abbreviatedOid": "02bf218",
                                                            "authoredDate": "2023-06-15T10:27:43Z"
                                                        }
                                                    }
                                                }
                                            ]
                                        }
                                    }
                                }
                            ]
                        }
                    },
                    "textMatches": [
                        {
                            "property": "comments.body"
                        }
                    ]
                }
            ],
            "pageInfo": {
                "endCursor": "Y3Vyc29yOjI=",
                "hasNextPage": false,
                "hasPreviousPage": false,
                "startCursor": "Y3Vyc29yOjE="
            },
            "issueCount": 2
        }
    }
}